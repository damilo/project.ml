{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Train a Quadcopter How to Fly\n",
    "\n",
    "Design an agent to fly a quadcopter, and then train it using a reinforcement learning algorithm of your choice! \n",
    "\n",
    "Try to apply the techniques you have learnt, but also feel free to come up with innovative ideas and test them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "Take a look at the files in the directory to better understand the structure of the project. \n",
    "\n",
    "- `task.py`: Define your task (environment) in this file.\n",
    "- `agents/`: Folder containing reinforcement learning agents.\n",
    "    - `policy_search.py`: A sample agent has been provided here.\n",
    "    - `agent.py`: Develop your agent here.\n",
    "- `physics_sim.py`: This file contains the simulator for the quadcopter.  **DO NOT MODIFY THIS FILE**.\n",
    "\n",
    "For this project, you will define your own task in `task.py`.  Although we have provided a example task to get you started, you are encouraged to change it.  Later in this notebook, you will learn more about how to amend this file.\n",
    "\n",
    "You will also design a reinforcement learning agent in `agent.py` to complete your chosen task.  \n",
    "\n",
    "You are welcome to create any additional files to help you to organize your code.  For instance, you may find it useful to define a `model.py` file defining any needed neural network architectures.\n",
    "\n",
    "## Controlling the Quadcopter\n",
    "\n",
    "We provide a sample agent in the code cell below to show you how to use the sim to control the quadcopter.  This agent is even simpler than the sample agent that you'll examine (in `agents/policy_search.py`) later in this notebook!\n",
    "\n",
    "The agent controls the quadcopter by setting the revolutions per second on each of its four rotors.  The provided agent in the `Basic_Agent` class below always selects a random action for each of the four rotors.  These four speeds are returned by the `act` method as a list of four floating-point numbers.  \n",
    "\n",
    "For this project, the agent that you will implement in `agents/agent.py` will have a far more intelligent method for selecting actions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Basic_Agent ():\n",
    "    def __init__(self, task):\n",
    "        self.task = task\n",
    "    \n",
    "    def act (self):\n",
    "        new_thrust = random.gauss(450., 25.)\n",
    "        return [new_thrust + random.gauss(0., 1.) for x in range(4)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code cell below to have the agent select actions to control the quadcopter.  \n",
    "\n",
    "Feel free to change the provided values of `runtime`, `init_pose`, `init_velocities`, and `init_angle_velocities` below to change the starting conditions of the quadcopter.\n",
    "\n",
    "The `labels` list below annotates statistics that are saved while running the simulation.  All of this information is saved in a text file `data.txt` and stored in the dictionary `results`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "from task import Task\n",
    "\n",
    "# Modify the values below to give the quadcopter a different starting position.\n",
    "runtime = 5.                                     # time limit of the episode\n",
    "init_pose = np.array([0., 0., 0., 0., 0., 0.])  # initial pose # [x, y, z, phi, theta, psi]\n",
    "init_velocities = np.array([0., 0., 0.])         # initial velocities # x_velocity, y_velocity, z_velocity\n",
    "init_angle_velocities = np.array([0., 0., 0.])   # initial angle velocities # phi_velocity, theta_velocity, psi_velocity\n",
    "file_output = 'data.txt'                         # file name for saved results\n",
    "\n",
    "# Setup\n",
    "task = Task(init_pose, init_velocities, init_angle_velocities, runtime)\n",
    "agent = Basic_Agent(task)\n",
    "done = False\n",
    "labels = ['time', 'x', 'y', 'z', 'phi', 'theta', 'psi', 'x_velocity',\n",
    "          'y_velocity', 'z_velocity', 'phi_velocity', 'theta_velocity',\n",
    "          'psi_velocity', 'rotor_speed1', 'rotor_speed2', 'rotor_speed3', 'rotor_speed4']\n",
    "results = {x : [] for x in labels}\n",
    "\n",
    "# Run the simulation, and save the results.\n",
    "with open(file_output, 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(labels)\n",
    "    \n",
    "    # reset environment\n",
    "    initial_state = task.reset ()\n",
    "    print ('initial state: {} {} {}'.format (initial_state[:6], initial_state[6:12], initial_state[12:]))\n",
    "    \n",
    "    while True:\n",
    "        rotor_speeds = agent.act()\n",
    "        #rotor_speeds = [400, 400, 400, 400]\n",
    "        _, _, done = task.step(rotor_speeds)\n",
    "        to_write = [task.sim.time] + list(task.sim.pose) + list(task.sim.v) + list(task.sim.angular_v) + list(rotor_speeds)\n",
    "        for ii in range(len(labels)):\n",
    "            results[labels[ii]].append(to_write[ii])\n",
    "        writer.writerow(to_write)\n",
    "        \n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code cell below to visualize how the position of the quadcopter evolved during the simulation. Next, you can plot the Euler angles (the rotation of the quadcopter over the $x$-, $y$-, and $z$-axes),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig, axs = plt.subplots (1, 2)\n",
    "\n",
    "axs[0].plot(results['time'], results['x'], label='x')\n",
    "axs[0].plot(results['time'], results['y'], label='y')\n",
    "axs[0].plot(results['time'], results['z'], label='z')\n",
    "axs[0].legend(loc='upper left')\n",
    "axs[0].set (title='position')\n",
    "\n",
    "axs[1].plot(results['time'], results['phi'], label='phi')\n",
    "axs[1].plot(results['time'], results['theta'], label='theta')\n",
    "axs[1].plot(results['time'], results['psi'], label='psi')\n",
    "axs[1].legend(loc='upper left')\n",
    "axs[1].set (title='Euler angles')\n",
    "\n",
    "_ = plt.ylim()\n",
    "fig.set_size_inches ((14., 6.), forward=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code cell visualizes the velocity of the quadcopter. before plotting the velocities (in radians per second) corresponding to each of the Euler angles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots (1, 2)\n",
    "\n",
    "axs[0].plot(results['time'], results['x_velocity'], label='x_hat')\n",
    "axs[0].plot(results['time'], results['y_velocity'], label='y_hat')\n",
    "axs[0].plot(results['time'], results['z_velocity'], label='z_hat')\n",
    "axs[0].legend(loc='upper left')\n",
    "axs[0].set (title='velocity')\n",
    "\n",
    "axs[1].plot(results['time'], results['phi_velocity'], label='phi_velocity')\n",
    "axs[1].plot(results['time'], results['theta_velocity'], label='theta_velocity')\n",
    "axs[1].plot(results['time'], results['psi_velocity'], label='psi_velocity')\n",
    "axs[1].legend(loc='upper left')\n",
    "axs[1].set (title='velocity in Euler angles')\n",
    "\n",
    "_ = plt.ylim()\n",
    "fig.set_size_inches ((14., 6.), forward=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can use the code cell below to print the agent's choice of actions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(results['time'], results['rotor_speed1'], label='Rotor 1 revolutions / second')\n",
    "plt.plot(results['time'], results['rotor_speed2'], label='Rotor 2 revolutions / second')\n",
    "plt.plot(results['time'], results['rotor_speed3'], label='Rotor 3 revolutions / second')\n",
    "plt.plot(results['time'], results['rotor_speed4'], label='Rotor 4 revolutions / second')\n",
    "plt.legend()\n",
    "_ = plt.ylim()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When specifying a task, you will derive the environment state from the simulator.  Run the code cell below to print the values of the following variables at the end of the simulation:\n",
    "- `task.sim.pose` (the position of the quadcopter in ($x,y,z$) dimensions and the Euler angles),\n",
    "- `task.sim.v` (the velocity of the quadcopter in ($x,y,z$) dimensions), and\n",
    "- `task.sim.angular_v` (radians/second for each of the three Euler angles)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the pose, velocity, and angular velocity of the quadcopter at the end of the episode\n",
    "print(task.sim.pose)\n",
    "print(task.sim.v)\n",
    "print(task.sim.angular_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the sample task in `task.py`, we use the 6-dimensional pose of the quadcopter to construct the state of the environment at each timestep.  However, when amending the task for your purposes, you are welcome to expand the size of the state vector by including the velocity information.  You can use any combination of the pose, velocity, and angular velocity - feel free to tinker here, and construct the state to suit your task.\n",
    "\n",
    "## The Task\n",
    "\n",
    "A sample task has been provided for you in `task.py`.  Open this file in a new window now. \n",
    "\n",
    "The `__init__()` method is used to initialize several variables that are needed to specify the task.  \n",
    "- The simulator is initialized as an instance of the `PhysicsSim` class (from `physics_sim.py`).  \n",
    "- Inspired by the methodology in the original DDPG paper, we make use of action repeats.  For each timestep of the agent, we step the simulation `action_repeats` timesteps.  If you are not familiar with action repeats, please read the **Results** section in [the DDPG paper](https://arxiv.org/abs/1509.02971).\n",
    "- We set the number of elements in the state vector.  For the sample task, we only work with the 6-dimensional pose information.  To set the size of the state (`state_size`), we must take action repeats into account.  \n",
    "- The environment will always have a 4-dimensional action space, with one entry for each rotor (`action_size=4`). You can set the minimum (`action_low`) and maximum (`action_high`) values of each entry here.\n",
    "- The sample task in this provided file is for the agent to reach a target position.  We specify that target position as a variable.\n",
    "\n",
    "The `reset()` method resets the simulator.  The agent should call this method every time the episode ends.  You can see an example of this in the code cell below.\n",
    "\n",
    "The `step()` method is perhaps the most important.  It accepts the agent's choice of action `rotor_speeds`, which is used to prepare the next state to pass on to the agent.  Then, the reward is computed from `get_reward()`.  The episode is considered done if the time limit has been exceeded, or the quadcopter has travelled outside of the bounds of the simulation.\n",
    "\n",
    "In the next section, you will learn how to test the performance of an agent on this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Agent\n",
    "\n",
    "The sample agent given in `agents/policy_search.py` uses a very simplistic linear policy to directly compute the action vector as a dot product of the state vector and a matrix of weights. Then, it randomly perturbs the parameters by adding some Gaussian noise, to produce a different policy. Based on the average reward obtained in each episode (`score`), it keeps track of the best set of parameters found so far, how the score is changing, and accordingly tweaks a scaling factor to widen or tighten the noise.\n",
    "\n",
    "Run the code cell below to see how the agent performs on the sample task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "from agents.policy_search import PolicySearch_Agent\n",
    "from task import Task\n",
    "\n",
    "num_episodes = 1000\n",
    "target_pos = np.array([0., 0., 10.])\n",
    "task = Task(target_pos=target_pos)\n",
    "agent = PolicySearch_Agent(task) \n",
    "\n",
    "for i_episode in range(1, num_episodes+1):\n",
    "    state = agent.reset_episode() # start a new episode\n",
    "    while True:\n",
    "        action = agent.act(state) \n",
    "        next_state, reward, done = task.step(action)\n",
    "        agent.step(reward, done)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            print(\"\\rEpisode = {:4d}, score = {:7.3f} (best = {:7.3f}), noise_scale = {}\".format(\n",
    "                i_episode, agent.score, agent.best_score, agent.noise_scale), end=\"\")  # [debug]\n",
    "            break\n",
    "    sys.stdout.flush ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This agent should perform very poorly on this task.  And that's where you come in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Define the Task, Design the Agent, and Train Your Agent!\n",
    "\n",
    "Amend `task.py` to specify a task of your choosing.  If you're unsure what kind of task to specify, you may like to teach your quadcopter to takeoff, hover in place, land softly, or reach a target pose.  \n",
    "\n",
    "After specifying your task, use the sample agent in `agents/policy_search.py` as a template to define your own agent in `agents/agent.py`.  You can borrow whatever you need from the sample agent, including ideas on how you might modularize your code (using helper methods like `act()`, `learn()`, `reset_episode()`, etc.).\n",
    "\n",
    "Note that it is **highly unlikely** that the first agent and task that you specify will learn well.  You will likely have to tweak various hyperparameters and the reward function for your task until you arrive at reasonably good behavior.\n",
    "\n",
    "As you develop your agent, it's important to keep an eye on how it's performing. Use the code above as inspiration to build in a mechanism to log/save the total rewards obtained in each episode to file.  If the episode rewards are gradually increasing, this is an indication that your agent is learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My Environment\n",
    "The environment is derived by the given class 'Task'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from task import Task\n",
    "import numpy as np\n",
    "\n",
    "class MyTask (Task):\n",
    "    \n",
    "    \n",
    "    def __init__ (self, init_pose, init_velocities, init_angle_velocities, runtime=5.):\n",
    "        #Task.__init__ (self)\n",
    "        super ().__init__ (init_pose, init_velocities, init_angle_velocities, runtime)\n",
    "        \n",
    "        # Goal\n",
    "        self.target_pos = np.array ([0., 0., 10.])\n",
    "        \n",
    "        # other\n",
    "        self.__old_v = self.sim.v\n",
    "    \n",
    "    \n",
    "    # reward function (overwrite)\n",
    "    def get_reward (self):\n",
    "        # 20% time stayed 'alive'\n",
    "        # 50% difference in position\n",
    "        # 30% change of velocity per step\n",
    "        reward = 0.20*(self.sim.time/self.sim.runtime) - \\\n",
    "            0.50*abs (self.sim.pose[2] - self.target_pos[2]) - \\\n",
    "            0.30*abs(self.sim.v[2]-self.__old_v[2])\n",
    "        return reward\n",
    "    \n",
    "    # step function (overwrite)\n",
    "    def step (self, rotor_speeds):\n",
    "        self.__old_v = self.sim.v\n",
    "        return (super ().step (rotor_speeds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My Agent\n",
    "The agent is defined in <a href=\".\\agents\\agent.py\">agent.py</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "### Prepare environment\n",
    "import os\n",
    "import numpy as np\n",
    "import random as rn\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "# clear everything known of past instances (\"useful to avoid clutter from old models / layers\")\n",
    "K.clear_session ()\n",
    "\n",
    "# How can I obtain reproducible results using Keras during development?\n",
    "# source: https://keras.io/getting-started/faq/#how-can-i-obtain-reproducible-results-using-keras-during-development\n",
    "random_state = 42\n",
    "\n",
    "# for certain hash-based operations\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "# for starting Numpy generated random numbers in a well-defined initial state\n",
    "np.random.seed (random_state)\n",
    "# for starting core Python generated random numbers in a well-defined state\n",
    "rn.seed (random_state)\n",
    "# force TensorFlow to use single thread\n",
    "session_conf = tf.ConfigProto (intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "# random number generation in the TensorFlow backend have a well-defined initial state\n",
    "tf.set_random_seed (random_state)\n",
    "sess = tf.Session (graph=tf.get_default_graph (), config=session_conf)\n",
    "K.set_session (sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original state size w/ action repeat: 18\n",
      "original state size w/o 'action repeat': 6\n",
      "example of a original sample:\n",
      "[0.         0.         0.00318131 0.         0.         0.\n",
      " 0.         0.         0.01272409 0.         0.         0.\n",
      " 0.         0.         0.02862411 0.         0.         0.        ] -14.976535243418478 False\n",
      "[0.         0.         0.95385392]\n",
      "my sample feature vector: <1, z, v_z, z*v_z>\n",
      "[1.         0.00318131 0.95385392 0.0030345 ]\n"
     ]
    }
   ],
   "source": [
    "## Environmet\n",
    "import random as rn\n",
    "\n",
    "runtime = 20.\n",
    "init_pose = np.array ([0., 0., 0., 0., 0., 0.])\n",
    "init_velocities = np.array ([0., 0., 0.])\n",
    "init_angle_velocities = np.array ([0., 0., 0.])\n",
    "\n",
    "env = MyTask (init_pose, init_velocities, init_angle_velocities, runtime)\n",
    "\n",
    "print ('original state size w/ action repeat:', env.state_size)\n",
    "print ('original state size w/o \\'action repeat\\':', env.sim.pose.shape[0])\n",
    "print ('example of a original sample:')\n",
    "env.reset ()\n",
    "rotor_speeds = np.zeros (4)\n",
    "random_speed = rn.randint (0, 900)\n",
    "for i in range (4):\n",
    "    rotor_speeds[i] = random_speed\n",
    "next_state, reward, done = env.step (rotor_speeds)\n",
    "print (next_state, reward, done)\n",
    "print (env.sim.v)\n",
    "# new feature vector, consisting of <1, z, v_z, z*v_z>\n",
    "print ('my sample feature vector: <1, z, v_z, z*v_z>')\n",
    "state_vec = np.array ([1., next_state[2], env.sim.v[2], next_state[2] * env.sim.v[2]])\n",
    "print (state_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rl hyperparameters:\n",
      " epsilon: max = 1.0, min = 0.01, decay = 5e-05\n",
      " gamma: value = 0.9\n",
      "nn hyperparameters:\n",
      " learning rate: value = 0.001\n",
      "nn architecture:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "hidden_1 (Dense)             (None, 64)                320       \n",
      "_________________________________________________________________\n",
      "hidden_2 (Dense)             (None, 48)                3120      \n",
      "_________________________________________________________________\n",
      "hidden_3 (Dense)             (None, 48)                2352      \n",
      "_________________________________________________________________\n",
      "hidden_4 (Dense)             (None, 24)                1176      \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 15)                375       \n",
      "=================================================================\n",
      "Total params: 7,343\n",
      "Trainable params: 7,343\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "target nn parameters:\n",
      " update target every learn cycle: 1000\n",
      " tau: value = 0.125\n"
     ]
    }
   ],
   "source": [
    "## TODO: Train your agent here.\n",
    "from agents.agent import MLPQNAgent\n",
    "# state size = 4, shape =(none, 4)\n",
    "# action size = 13\n",
    "in_shape = np.zeros (4)\n",
    "agent = MLPQNAgent (in_shape.shape, 15, init_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:     1 total reward:     -29.99 avg. training loss:     0.0000 explore P: 0.99000\n",
      "episode:     2 total reward:    -148.49 avg. training loss:     0.0000 explore P: 0.99000\n",
      "episode:     3 total reward:   -3111.49 avg. training loss:     0.0000 explore P: 0.99000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daniel\\OneDrive\\projects\\udacity.mlnd\\mlnd\\projects\\RL-Quadcopter-2\\physics_sim.py:114: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  J = V / n * D\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:     4 total reward:  -40063.45 avg. training loss:    43.6693 explore P: 0.99064\n",
      "episode:     5 total reward:    -163.96 avg. training loss:  8310.9346 explore P: 0.99010\n",
      "episode:     6 total reward:  -18886.33 avg. training loss:   161.7681 explore P: 0.97894\n",
      "episode:     7 total reward:     -74.87 avg. training loss:   136.9950 explore P: 0.97870\n",
      "episode:     8 total reward:    -363.29 avg. training loss:   103.4511 explore P: 0.97749\n",
      "episode:     9 total reward:     -59.90 avg. training loss:   157.6302 explore P: 0.97730\n",
      "episode:    10 total reward:  -73603.37 avg. training loss:    74.4485 explore P: 0.96213\n",
      "episode:    11 total reward:    -208.11 avg. training loss:   179.8806 explore P: 0.96147\n",
      "episode:    12 total reward:    -104.56 avg. training loss:    78.6643 explore P: 0.96113\n",
      "episode:    13 total reward:   -1495.48 avg. training loss:   125.2793 explore P: 0.95639\n",
      ">>> target estimator network updated (1000)\n",
      "episode:    14 total reward:  -50272.84 avg. training loss:   108.3124 explore P: 0.94314\n",
      "episode:    15 total reward:  -67813.48 avg. training loss:  1028.1292 explore P: 0.92773\n",
      "episode:    16 total reward:  -47641.51 avg. training loss:   603.3235 explore P: 0.91416\n",
      "episode:    17 total reward:    -149.10 avg. training loss:   577.5347 explore P: 0.91371\n",
      "episode:    18 total reward:    -104.58 avg. training loss:   880.4924 explore P: 0.91339\n",
      ">>> target estimator network updated (2000)\n",
      "episode:    19 total reward:  -58795.48 avg. training loss:   486.4023 explore P: 0.89847\n",
      "episode:    20 total reward:  -41702.74 avg. training loss:   202.0167 explore P: 0.88380\n",
      "episode:    21 total reward:     -29.99 avg. training loss:   420.4222 explore P: 0.88372\n",
      "episode:    22 total reward:    -264.73 avg. training loss:   347.0296 explore P: 0.88293\n",
      "episode:    23 total reward:     -44.97 avg. training loss:  1005.1175 explore P: 0.88280\n",
      "episode:    24 total reward:    -480.73 avg. training loss:   242.8233 explore P: 0.88084\n",
      "episode:    25 total reward:     -29.99 avg. training loss:    51.7384 explore P: 0.88075\n",
      "episode:    26 total reward:  -45702.74 avg. training loss:   149.8553 explore P: 0.86976\n",
      "episode:    27 total reward:    -456.41 avg. training loss:  1923.6701 explore P: 0.86654\n",
      "episode:    28 total reward:     -44.95 avg. training loss:  4795.2214 explore P: 0.86641\n",
      "episode:    29 total reward:     -44.97 avg. training loss:   587.4869 explore P: 0.86629\n",
      "episode:    30 total reward:     -44.97 avg. training loss:  5592.1827 explore P: 0.86616\n",
      "episode:    31 total reward:     -44.97 avg. training loss:  4303.3717 explore P: 0.86603\n",
      ">>> target estimator network updated (3000)\n",
      "episode:    32 total reward:  -50274.68 avg. training loss:   840.4707 explore P: 0.85527\n",
      "episode:    33 total reward:  -41588.82 avg. training loss:   804.7982 explore P: 0.84590\n",
      "episode:    34 total reward:    -457.98 avg. training loss:   609.5406 explore P: 0.84456\n",
      "episode:    35 total reward:    -715.54 avg. training loss:   540.1242 explore P: 0.84198\n",
      "episode:    36 total reward:     -29.98 avg. training loss:   139.9433 explore P: 0.84189\n",
      "episode:    37 total reward:    -351.24 avg. training loss:   695.9041 explore P: 0.83978\n",
      "episode:    38 total reward:    -531.86 avg. training loss:   591.8619 explore P: 0.83808\n",
      "episode:    39 total reward:     -59.92 avg. training loss:  1025.3147 explore P: 0.83791\n",
      "episode:    40 total reward:     -29.99 avg. training loss:  2299.0754 explore P: 0.83783\n",
      "episode:    41 total reward:     -44.97 avg. training loss:    79.9520 explore P: 0.83770\n",
      "episode:    42 total reward:    -178.71 avg. training loss:   437.7018 explore P: 0.83721\n",
      "episode:    43 total reward:    -392.60 avg. training loss:   443.0200 explore P: 0.83498\n",
      "episode:    44 total reward:  -52249.07 avg. training loss:   580.5138 explore P: 0.82355\n",
      ">>> target estimator network updated (4000)\n",
      "episode:    45 total reward:   -2803.28 avg. training loss:   494.2489 explore P: 0.81900\n",
      "episode:    46 total reward:  -32586.06 avg. training loss:   448.3288 explore P: 0.80912\n",
      "episode:    47 total reward:     -44.97 avg. training loss:    85.6505 explore P: 0.80900\n",
      "episode:    48 total reward:    -319.84 avg. training loss:  1389.9810 explore P: 0.80812\n",
      "episode:    49 total reward:   -3312.41 avg. training loss:   718.5495 explore P: 0.80354\n",
      "episode:    50 total reward:    -619.26 avg. training loss:   664.0312 explore P: 0.80081\n",
      "episode:    51 total reward:    -220.92 avg. training loss:   161.2581 explore P: 0.80021\n",
      "episode:    52 total reward:  -19996.03 avg. training loss:   541.7524 explore P: 0.79075\n",
      "episode:    53 total reward:    -149.34 avg. training loss:   702.9338 explore P: 0.79036\n",
      "episode:    54 total reward:    -519.35 avg. training loss:   380.2266 explore P: 0.78864\n",
      "episode:    55 total reward:     -29.98 avg. training loss:   971.2423 explore P: 0.78857\n",
      "episode:    56 total reward:    -133.72 avg. training loss:   122.8513 explore P: 0.78822\n",
      "episode:    57 total reward:     -44.96 avg. training loss:  1486.9577 explore P: 0.78810\n",
      "episode:    58 total reward:    -382.99 avg. training loss:   560.8073 explore P: 0.78701\n",
      "episode:    59 total reward:    -822.91 avg. training loss:   564.2437 explore P: 0.78406\n",
      ">>> target estimator network updated (5000)\n",
      "episode:    60 total reward:   -2628.61 avg. training loss:   503.2058 explore P: 0.78028\n",
      "episode:    61 total reward:    -446.70 avg. training loss:   500.2501 explore P: 0.77870\n",
      "episode:    62 total reward:    -341.65 avg. training loss:   770.5379 explore P: 0.77778\n",
      "episode:    63 total reward:    -119.26 avg. training loss:   224.0506 explore P: 0.77747\n",
      "episode:    64 total reward:    -178.30 avg. training loss:   310.5203 explore P: 0.77701\n",
      "episode:    65 total reward:   -4261.35 avg. training loss:   603.1412 explore P: 0.77246\n",
      "episode:    66 total reward:    -177.70 avg. training loss:   395.6202 explore P: 0.77201\n",
      "episode:    67 total reward:    -119.20 avg. training loss:   471.8221 explore P: 0.77170\n",
      "episode:    68 total reward:    -513.82 avg. training loss:   339.9282 explore P: 0.77033\n",
      "episode:    69 total reward:    -337.88 avg. training loss:   596.7572 explore P: 0.76938\n",
      "episode:    70 total reward:   -4765.34 avg. training loss:   500.6725 explore P: 0.76216\n",
      "episode:    71 total reward:    -566.16 avg. training loss:   696.0518 explore P: 0.76017\n",
      "episode:    72 total reward:    -119.66 avg. training loss:  1646.0926 explore P: 0.75987\n",
      "episode:    73 total reward:     -44.97 avg. training loss:    24.5885 explore P: 0.75976\n",
      "episode:    74 total reward:   -2099.92 avg. training loss:   541.2133 explore P: 0.75584\n",
      "episode:    75 total reward:    -571.71 avg. training loss:   591.0376 explore P: 0.75353\n",
      "episode:    76 total reward:    -546.39 avg. training loss:   444.2147 explore P: 0.75167\n",
      "episode:    77 total reward:     -29.99 avg. training loss:    41.3211 explore P: 0.75160\n",
      "episode:    78 total reward:    -236.15 avg. training loss:   696.0957 explore P: 0.75100\n",
      "episode:    79 total reward:     -44.96 avg. training loss:   133.7370 explore P: 0.75089\n",
      "episode:    80 total reward:    -624.69 avg. training loss:   486.0484 explore P: 0.74760\n",
      "episode:    81 total reward:    -206.74 avg. training loss:   302.3028 explore P: 0.74709\n",
      "episode:    82 total reward:    -621.64 avg. training loss:   503.7311 explore P: 0.74510\n",
      ">>> target estimator network updated (6000)\n",
      "episode:    83 total reward:   -5109.07 avg. training loss:   370.4027 explore P: 0.74074\n",
      "episode:    84 total reward:   -1440.38 avg. training loss:   419.6870 explore P: 0.73702\n",
      "episode:    85 total reward:   -1250.16 avg. training loss:   387.2014 explore P: 0.73401\n",
      "episode:    86 total reward:     -29.99 avg. training loss:   946.8078 explore P: 0.73394\n",
      "episode:    87 total reward:    -249.74 avg. training loss:   698.8965 explore P: 0.73332\n",
      "episode:    88 total reward:    -163.16 avg. training loss:   263.5535 explore P: 0.73292\n",
      "episode:    89 total reward:     -29.99 avg. training loss:  5973.4029 explore P: 0.73285\n",
      "episode:    90 total reward:    -134.25 avg. training loss:   500.0050 explore P: 0.73253\n",
      "episode:    91 total reward:     -59.91 avg. training loss:  1248.2528 explore P: 0.73238\n",
      "episode:    92 total reward:  -60942.36 avg. training loss:   626.2276 explore P: 0.72045\n",
      "episode:    93 total reward:     -29.99 avg. training loss:    57.0444 explore P: 0.72038\n",
      "episode:    94 total reward:     -44.94 avg. training loss:    81.1450 explore P: 0.72028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:    95 total reward:   -6543.90 avg. training loss:   387.4424 explore P: 0.71451\n",
      ">>> target estimator network updated (7000)\n",
      "episode:    96 total reward:  -45522.62 avg. training loss:   378.1848 explore P: 0.70597\n",
      "episode:    97 total reward:   -1219.66 avg. training loss:   351.6574 explore P: 0.70291\n",
      "episode:    98 total reward:     -29.99 avg. training loss:    16.7287 explore P: 0.70284\n",
      "episode:    99 total reward:     -29.99 avg. training loss:  1171.4358 explore P: 0.70277\n",
      "episode:   100 total reward:     -59.90 avg. training loss:   219.0483 explore P: 0.70264\n",
      "episode:   101 total reward:    -192.49 avg. training loss:   153.3907 explore P: 0.70219\n",
      "episode:   102 total reward:     -59.91 avg. training loss:   526.2268 explore P: 0.70205\n",
      "episode:   103 total reward:   -3435.58 avg. training loss:   335.4877 explore P: 0.69715\n",
      "episode:   104 total reward:     -59.89 avg. training loss:   302.5870 explore P: 0.69701\n",
      "episode:   105 total reward:   -1608.03 avg. training loss:   267.0716 explore P: 0.69379\n",
      "episode:   106 total reward:     -89.62 avg. training loss:   115.6597 explore P: 0.69359\n",
      "episode:   107 total reward:  -50748.42 avg. training loss:   289.1248 explore P: 0.68395\n",
      ">>> target estimator network updated (8000)\n",
      "episode:   108 total reward:  -48440.18 avg. training loss:   318.0441 explore P: 0.67282\n",
      "episode:   109 total reward:  -50358.45 avg. training loss:   285.5141 explore P: 0.66331\n",
      "episode:   110 total reward:    -249.34 avg. training loss:    33.9685 explore P: 0.66276\n",
      "episode:   111 total reward:    -148.47 avg. training loss:   212.0856 explore P: 0.66243\n",
      "episode:   112 total reward:    -572.51 avg. training loss:   224.7074 explore P: 0.66093\n",
      "episode:   113 total reward:  -10298.56 avg. training loss:   317.0485 explore P: 0.65310\n",
      "episode:   114 total reward:  -43168.25 avg. training loss:   216.0789 explore P: 0.64502\n",
      "episode:   115 total reward:    -427.29 avg. training loss:   263.3012 explore P: 0.64353\n",
      "episode:   116 total reward:    -281.94 avg. training loss:   274.1399 explore P: 0.64293\n",
      ">>> target estimator network updated (9000)\n",
      "episode:   117 total reward:  -46080.35 avg. training loss:   180.4849 explore P: 0.63547\n",
      "episode:   118 total reward:  -46225.63 avg. training loss:   220.2589 explore P: 0.62727\n",
      "episode:   119 total reward:   -9435.39 avg. training loss:   131.8895 explore P: 0.62229\n",
      "episode:   120 total reward:  -19508.75 avg. training loss:   166.8477 explore P: 0.61626\n",
      ">>> target estimator network updated (10000)\n",
      "episode:   121 total reward:  -64728.02 avg. training loss:   185.1078 explore P: 0.60637\n",
      "episode:   122 total reward:   -7180.73 avg. training loss:   187.7939 explore P: 0.60253\n",
      "episode:   123 total reward:     -29.99 avg. training loss:  1729.3577 explore P: 0.60247\n",
      "episode:   124 total reward:     -74.83 avg. training loss:     9.3228 explore P: 0.60232\n",
      "episode:   125 total reward:  -12277.00 avg. training loss:   151.4715 explore P: 0.59758\n",
      "episode:   126 total reward:     -59.89 avg. training loss:    51.7436 explore P: 0.59746\n",
      "episode:   127 total reward:    -405.56 avg. training loss:   201.1529 explore P: 0.59561\n",
      "episode:   128 total reward:    -434.35 avg. training loss:   135.6546 explore P: 0.59391\n",
      "episode:   129 total reward:  -49540.83 avg. training loss:   158.6135 explore P: 0.58427\n",
      "episode:   130 total reward:    -417.68 avg. training loss:   389.5440 explore P: 0.58298\n",
      "episode:   131 total reward:    -192.61 avg. training loss:   362.7878 explore P: 0.58261\n",
      ">>> target estimator network updated (11000)\n",
      "episode:   132 total reward:   -2016.23 avg. training loss:   209.8883 explore P: 0.57936\n",
      "episode:   133 total reward:     -74.81 avg. training loss:    26.3009 explore P: 0.57921\n",
      "episode:   134 total reward:    -490.74 avg. training loss:   115.0442 explore P: 0.57637\n",
      "episode:   135 total reward:    -529.30 avg. training loss:   172.3942 explore P: 0.57519\n",
      "episode:   136 total reward:    -394.05 avg. training loss:   181.9041 explore P: 0.57440\n",
      "episode:   137 total reward:  -57777.19 avg. training loss:   133.5872 explore P: 0.56508\n",
      "episode:   138 total reward:    -148.34 avg. training loss:   131.5412 explore P: 0.56480\n",
      "episode:   139 total reward:   -6225.89 avg. training loss:    85.6800 explore P: 0.56123\n",
      "episode:   140 total reward:   -7961.59 avg. training loss:   129.3292 explore P: 0.55766\n",
      "episode:   141 total reward:    -486.06 avg. training loss:   139.6079 explore P: 0.55668\n",
      "episode:   142 total reward:    -104.61 avg. training loss:   275.8952 explore P: 0.55648\n",
      "episode:   143 total reward:   -1956.75 avg. training loss:    80.8791 explore P: 0.55346\n",
      ">>> target estimator network updated (12000)\n",
      "episode:   144 total reward:   -4202.22 avg. training loss:   167.9798 explore P: 0.55056\n",
      "episode:   145 total reward:    -134.33 avg. training loss:   680.3613 explore P: 0.55032\n",
      "episode:   146 total reward:    -193.48 avg. training loss:   186.1097 explore P: 0.54997\n",
      "episode:   147 total reward:    -489.19 avg. training loss:   142.5582 explore P: 0.54899\n",
      "episode:   148 total reward:   -8510.15 avg. training loss:   134.6508 explore P: 0.54510\n",
      "episode:   149 total reward:  -14210.53 avg. training loss:   140.9123 explore P: 0.54004\n",
      "episode:   150 total reward:  -41628.81 avg. training loss:    95.2983 explore P: 0.53348\n",
      "episode:   151 total reward:    -522.85 avg. training loss:   127.4127 explore P: 0.53158\n",
      "episode:   152 total reward:    -298.30 avg. training loss:   102.1771 explore P: 0.53105\n",
      "episode:   153 total reward:     -29.99 avg. training loss:     8.9720 explore P: 0.53100\n",
      "episode:   154 total reward:    -178.41 avg. training loss:   113.6318 explore P: 0.53069\n",
      ">>> target estimator network updated (13000)\n",
      "episode:   155 total reward:  -58077.82 avg. training loss:   273.7276 explore P: 0.52314\n",
      "episode:   156 total reward:    -501.84 avg. training loss:   216.9495 explore P: 0.52120\n",
      "episode:   157 total reward:    -219.81 avg. training loss:    85.4034 explore P: 0.52081\n",
      "episode:   158 total reward:    -288.51 avg. training loss:    84.2449 explore P: 0.52030\n",
      "episode:   159 total reward:    -250.64 avg. training loss:    56.9429 explore P: 0.51987\n",
      "episode:   160 total reward:  -43806.20 avg. training loss:   175.9260 explore P: 0.51437\n",
      "episode:   161 total reward:    -457.60 avg. training loss:   146.3827 explore P: 0.51291\n",
      "episode:   162 total reward:     -44.96 avg. training loss:     3.5718 explore P: 0.51283\n",
      "episode:   163 total reward:   -2145.09 avg. training loss:   182.5776 explore P: 0.51005\n",
      "episode:   164 total reward:   -3873.08 avg. training loss:   181.1373 explore P: 0.50696\n",
      "episode:   165 total reward:    -480.84 avg. training loss:   227.4214 explore P: 0.50512\n",
      "episode:   166 total reward:   -4407.87 avg. training loss:   165.3579 explore P: 0.50182\n",
      ">>> target estimator network updated (14000)\n",
      "episode:   167 total reward:    -401.03 avg. training loss:    61.6685 explore P: 0.50110\n",
      "episode:   168 total reward:  -13777.14 avg. training loss:   158.9550 explore P: 0.49753\n",
      "episode:   169 total reward:     -44.97 avg. training loss:    20.1729 explore P: 0.49746\n",
      "episode:   170 total reward:     -29.99 avg. training loss:   732.7874 explore P: 0.49741\n",
      "episode:   171 total reward:  -16373.43 avg. training loss:   133.9717 explore P: 0.49220\n",
      "episode:   172 total reward:    -133.78 avg. training loss:    14.3676 explore P: 0.49198\n",
      "episode:   173 total reward:    -273.00 avg. training loss:   118.6224 explore P: 0.49152\n",
      "episode:   174 total reward:    -467.28 avg. training loss:   235.8890 explore P: 0.49059\n",
      "episode:   175 total reward:    -885.96 avg. training loss:    73.4927 explore P: 0.48876\n",
      "episode:   176 total reward:  -74036.12 avg. training loss:   133.0549 explore P: 0.48086\n",
      ">>> target estimator network updated (15000)\n",
      "episode:   177 total reward:  -14997.01 avg. training loss:   140.7596 explore P: 0.47657\n",
      "episode:   178 total reward:     -44.96 avg. training loss:   532.1109 explore P: 0.47650\n",
      "episode:   179 total reward:     -29.98 avg. training loss:   565.4380 explore P: 0.47645\n",
      "episode:   180 total reward:     -29.98 avg. training loss:   297.0089 explore P: 0.47641\n",
      "episode:   181 total reward:  -48265.58 avg. training loss:    84.2043 explore P: 0.47008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:   182 total reward:    -483.31 avg. training loss:    89.3895 explore P: 0.46891\n",
      "episode:   183 total reward:    -134.33 avg. training loss:   315.5741 explore P: 0.46870\n",
      "episode:   184 total reward:    -264.87 avg. training loss:    49.1654 explore P: 0.46829\n",
      "episode:   185 total reward:    -454.91 avg. training loss:    87.8472 explore P: 0.46664\n",
      "episode:   186 total reward:  -18464.50 avg. training loss:   132.6835 explore P: 0.46154\n",
      "episode:   187 total reward:    -309.22 avg. training loss:    99.8893 explore P: 0.46106\n",
      "episode:   188 total reward:     -89.53 avg. training loss:     9.6093 explore P: 0.46093\n",
      "episode:   189 total reward:     -29.99 avg. training loss:     2.4897 explore P: 0.46088\n",
      "episode:   190 total reward:     -29.99 avg. training loss:  1181.9857 explore P: 0.46084\n",
      "episode:   191 total reward:    -193.17 avg. training loss:    30.3565 explore P: 0.46054\n",
      "episode:   192 total reward:   -1077.49 avg. training loss:   219.3226 explore P: 0.45834\n",
      "episode:   193 total reward:     -44.97 avg. training loss:   576.8029 explore P: 0.45827\n",
      "episode:   194 total reward:    -423.28 avg. training loss:    68.5843 explore P: 0.45684\n",
      "episode:   195 total reward:    -149.14 avg. training loss:    13.4403 explore P: 0.45662\n",
      ">>> target estimator network updated (16000)\n",
      "episode:   196 total reward:    -772.57 avg. training loss:   168.4739 explore P: 0.45448\n",
      "episode:   197 total reward:    -652.69 avg. training loss:   134.3881 explore P: 0.45277\n",
      "episode:   198 total reward:   -1126.39 avg. training loss:   204.4732 explore P: 0.45078\n",
      "episode:   199 total reward:    -207.75 avg. training loss:    28.7115 explore P: 0.45048\n",
      "episode:   200 total reward:    -339.45 avg. training loss:    81.1869 explore P: 0.44933\n",
      "episode:   201 total reward:    -626.33 avg. training loss:    84.9539 explore P: 0.44780\n",
      "episode:   202 total reward:    -286.50 avg. training loss:   194.4363 explore P: 0.44736\n",
      "episode:   203 total reward:    -133.98 avg. training loss:   102.6252 explore P: 0.44716\n",
      "episode:   204 total reward:    -916.58 avg. training loss:    33.2578 explore P: 0.44522\n",
      "episode:   205 total reward:    -501.17 avg. training loss:   103.5684 explore P: 0.44353\n",
      "episode:   206 total reward:    -413.87 avg. training loss:    79.5570 explore P: 0.44275\n",
      "episode:   207 total reward:    -366.66 avg. training loss:    31.9638 explore P: 0.44158\n",
      "episode:   208 total reward:    -503.18 avg. training loss:    36.6805 explore P: 0.44078\n",
      "episode:   209 total reward:    -492.91 avg. training loss:    45.9868 explore P: 0.43936\n",
      "episode:   210 total reward:  -17883.87 avg. training loss:    96.1490 explore P: 0.43569\n",
      ">>> target estimator network updated (17000)\n",
      "episode:   211 total reward:  -10647.45 avg. training loss:   115.7900 explore P: 0.43179\n",
      "episode:   212 total reward:    -207.91 avg. training loss:    27.9015 explore P: 0.43149\n",
      "episode:   213 total reward:   -9408.65 avg. training loss:   171.5135 explore P: 0.42799\n",
      "episode:   214 total reward:  -28925.42 avg. training loss:    73.7292 explore P: 0.42352\n",
      "episode:   215 total reward:     -59.91 avg. training loss:    29.0725 explore P: 0.42344\n",
      "episode:   216 total reward:     -29.99 avg. training loss:    11.1530 explore P: 0.42340\n",
      "episode:   217 total reward:    -104.69 avg. training loss:    10.3600 explore P: 0.42325\n",
      "episode:   218 total reward:   -3370.13 avg. training loss:   157.3465 explore P: 0.42064\n",
      "episode:   219 total reward:    -442.14 avg. training loss:   126.0168 explore P: 0.41947\n",
      "episode:   220 total reward:    -458.99 avg. training loss:   124.6772 explore P: 0.41810\n",
      "episode:   221 total reward:     -29.99 avg. training loss:    16.4402 explore P: 0.41806\n",
      "episode:   222 total reward:     -29.99 avg. training loss:    10.7701 explore P: 0.41802\n",
      "episode:   223 total reward:    -104.64 avg. training loss:   189.4895 explore P: 0.41787\n",
      "episode:   224 total reward:    -133.20 avg. training loss:   102.6389 explore P: 0.41769\n",
      "episode:   225 total reward:   -4584.81 avg. training loss:    71.9789 explore P: 0.41535\n",
      "episode:   226 total reward:    -119.40 avg. training loss:    27.3054 explore P: 0.41519\n",
      "episode:   227 total reward:     -44.95 avg. training loss:     8.9538 explore P: 0.41513\n",
      "episode:   228 total reward:     -29.99 avg. training loss:    67.1211 explore P: 0.41509\n",
      "episode:   229 total reward:    -164.08 avg. training loss:   206.9777 explore P: 0.41487\n",
      "episode:   230 total reward:    -177.63 avg. training loss:    52.8184 explore P: 0.41462\n",
      ">>> target estimator network updated (18000)\n",
      "episode:   231 total reward:  -13554.48 avg. training loss:   121.7420 explore P: 0.40994\n",
      "episode:   232 total reward:    -299.96 avg. training loss:   108.7940 explore P: 0.40952\n",
      "episode:   233 total reward:    -387.71 avg. training loss:    54.5975 explore P: 0.40894\n",
      "episode:   234 total reward:     -29.98 avg. training loss:   236.4627 explore P: 0.40890\n",
      "episode:   235 total reward:     -44.97 avg. training loss:   151.3943 explore P: 0.40884\n",
      "episode:   236 total reward:    -179.48 avg. training loss:     6.2946 explore P: 0.40860\n",
      "episode:   237 total reward:    -179.26 avg. training loss:   106.2131 explore P: 0.40836\n",
      "episode:   238 total reward:     -59.89 avg. training loss:    42.2589 explore P: 0.40828\n",
      "episode:   239 total reward:     -29.99 avg. training loss:   430.7714 explore P: 0.40824\n",
      "episode:   240 total reward:     -29.98 avg. training loss:     5.5641 explore P: 0.40820\n",
      "episode:   241 total reward:    -163.49 avg. training loss:    17.8469 explore P: 0.40798\n",
      "episode:   242 total reward:    -475.14 avg. training loss:    34.3259 explore P: 0.40725\n",
      "episode:   243 total reward:     -29.99 avg. training loss:   160.5151 explore P: 0.40721\n",
      "episode:   244 total reward:    -236.78 avg. training loss:    26.5043 explore P: 0.40689\n",
      "episode:   245 total reward:     -29.99 avg. training loss:     1.6657 explore P: 0.40685\n",
      "episode:   246 total reward:     -44.95 avg. training loss:   434.3528 explore P: 0.40679\n",
      "episode:   247 total reward:     -89.74 avg. training loss:     5.9482 explore P: 0.40667\n",
      "episode:   248 total reward:     -29.99 avg. training loss:   118.2640 explore P: 0.40663\n",
      "episode:   249 total reward:     -89.67 avg. training loss:    22.4364 explore P: 0.40651\n",
      "episode:   250 total reward:     -59.90 avg. training loss:    33.2543 explore P: 0.40643\n",
      "episode:   251 total reward:    -104.52 avg. training loss:    21.7963 explore P: 0.40629\n",
      "episode:   252 total reward:     -44.97 avg. training loss:     5.1799 explore P: 0.40623\n",
      "episode:   253 total reward:     -89.71 avg. training loss:     5.2663 explore P: 0.40612\n",
      "episode:   254 total reward:     -29.99 avg. training loss:     8.1420 explore P: 0.40608\n",
      "episode:   255 total reward:   -3011.62 avg. training loss:   163.5811 explore P: 0.40367\n",
      "episode:   256 total reward:     -29.99 avg. training loss:     2.6683 explore P: 0.40363\n",
      "episode:   257 total reward:     -29.99 avg. training loss:   909.9735 explore P: 0.40359\n",
      "episode:   258 total reward:    -270.45 avg. training loss:   114.7364 explore P: 0.40321\n",
      "episode:   259 total reward:    -423.26 avg. training loss:   140.4955 explore P: 0.40188\n",
      "episode:   260 total reward:    -207.81 avg. training loss:    77.9824 explore P: 0.40161\n",
      "episode:   261 total reward:    -992.31 avg. training loss:    58.3750 explore P: 0.39926\n",
      "episode:   262 total reward:     -44.96 avg. training loss:   194.6213 explore P: 0.39920\n",
      "episode:   263 total reward:     -29.99 avg. training loss:     5.3421 explore P: 0.39917\n",
      "episode:   264 total reward:    -287.97 avg. training loss:   128.1425 explore P: 0.39878\n",
      "episode:   265 total reward:     -29.99 avg. training loss:     5.5290 explore P: 0.39874\n",
      "episode:   266 total reward:   -5576.34 avg. training loss:   104.9091 explore P: 0.39489\n",
      "episode:   267 total reward:    -134.25 avg. training loss:    45.6298 explore P: 0.39472\n",
      "episode:   268 total reward:    -371.36 avg. training loss:   136.5282 explore P: 0.39422\n",
      "episode:   269 total reward:     -29.98 avg. training loss:     2.9739 explore P: 0.39418\n",
      "episode:   270 total reward:     -29.98 avg. training loss:  1561.1431 explore P: 0.39414\n",
      "episode:   271 total reward:     -44.97 avg. training loss:    72.5101 explore P: 0.39408\n",
      "episode:   272 total reward:     -29.98 avg. training loss:   662.3830 explore P: 0.39404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:   273 total reward:     -59.88 avg. training loss:   396.5286 explore P: 0.39397\n",
      "episode:   274 total reward:    -235.87 avg. training loss:   191.4509 explore P: 0.39366\n",
      "episode:   275 total reward:    -450.00 avg. training loss:   146.7738 explore P: 0.39303\n",
      "episode:   276 total reward:     -59.82 avg. training loss:   115.3594 explore P: 0.39295\n",
      ">>> target estimator network updated (19000)\n",
      "episode:   277 total reward:   -3666.42 avg. training loss:   109.7602 explore P: 0.39074\n",
      "episode:   278 total reward:    -119.50 avg. training loss:    23.6663 explore P: 0.39058\n",
      "episode:   279 total reward:   -2824.40 avg. training loss:    76.6652 explore P: 0.38878\n",
      "episode:   280 total reward:     -29.99 avg. training loss:    32.5206 explore P: 0.38874\n",
      "episode:   281 total reward:     -59.91 avg. training loss:   281.8889 explore P: 0.38867\n",
      "episode:   282 total reward:     -74.84 avg. training loss:     7.4412 explore P: 0.38857\n",
      "episode:   283 total reward:     -29.99 avg. training loss:     7.4370 explore P: 0.38853\n",
      "episode:   284 total reward:     -89.77 avg. training loss:    11.2220 explore P: 0.38842\n",
      "episode:   285 total reward:    -134.06 avg. training loss:   158.9455 explore P: 0.38825\n",
      "episode:   286 total reward:     -29.99 avg. training loss:     2.3397 explore P: 0.38821\n",
      "episode:   287 total reward:    -912.99 avg. training loss:   116.3667 explore P: 0.38663\n",
      "episode:   288 total reward:    -262.25 avg. training loss:    42.2847 explore P: 0.38629\n",
      "episode:   289 total reward:    -265.89 avg. training loss:    39.5046 explore P: 0.38595\n",
      "episode:   290 total reward:   -2568.07 avg. training loss:   146.3991 explore P: 0.38389\n",
      "episode:   291 total reward:    -223.50 avg. training loss:    34.7372 explore P: 0.38361\n",
      "episode:   292 total reward:  -30126.13 avg. training loss:   101.7212 explore P: 0.37928\n",
      "episode:   293 total reward:   -7867.95 avg. training loss:   107.1625 explore P: 0.37477\n",
      ">>> target estimator network updated (20000)\n",
      "episode:   294 total reward:   -2260.67 avg. training loss:    94.9440 explore P: 0.37306\n",
      "episode:   295 total reward:   -1855.51 avg. training loss:    73.5204 explore P: 0.37085\n",
      "episode:   296 total reward:    -327.62 avg. training loss:   138.0488 explore P: 0.37041\n",
      "episode:   297 total reward:    -255.44 avg. training loss:   103.5524 explore P: 0.37009\n",
      "episode:   298 total reward:     -59.90 avg. training loss:    40.8890 explore P: 0.37002\n",
      "episode:   299 total reward:     -29.98 avg. training loss:   381.4382 explore P: 0.36998\n",
      "episode:   300 total reward:    -148.77 avg. training loss:    10.2494 explore P: 0.36980\n",
      "episode:   301 total reward:  -38112.57 avg. training loss:   111.4156 explore P: 0.36498\n",
      "episode:   302 total reward:  -45301.93 avg. training loss:   112.8719 explore P: 0.35912\n",
      "episode:   303 total reward:    -569.75 avg. training loss:    90.5965 explore P: 0.35833\n",
      ">>> target estimator network updated (21000)\n",
      "episode:   304 total reward:  -58816.75 avg. training loss:    84.3064 explore P: 0.35258\n",
      "episode:   305 total reward:  -40657.50 avg. training loss:    74.1812 explore P: 0.34692\n",
      "episode:   306 total reward:  -26914.98 avg. training loss:    85.7711 explore P: 0.34342\n",
      "episode:   307 total reward:    -566.32 avg. training loss:    79.9841 explore P: 0.34272\n",
      "episode:   308 total reward:   -7904.04 avg. training loss:    81.6024 explore P: 0.34028\n",
      ">>> target estimator network updated (22000)\n",
      "episode:   309 total reward:    -713.36 avg. training loss:   102.4364 explore P: 0.33883\n",
      "episode:   310 total reward:     -74.82 avg. training loss:     3.3100 explore P: 0.33875\n",
      "episode:   311 total reward:     -29.99 avg. training loss:     2.2461 explore P: 0.33872\n",
      "episode:   312 total reward:     -29.99 avg. training loss:    27.7401 explore P: 0.33869\n",
      "episode:   313 total reward:  -12080.67 avg. training loss:    52.8009 explore P: 0.33628\n",
      "episode:   314 total reward:    -119.51 avg. training loss:     6.5403 explore P: 0.33615\n",
      "episode:   315 total reward:  -12128.71 avg. training loss:    72.0867 explore P: 0.33323\n",
      "episode:   316 total reward:  -16832.29 avg. training loss:    78.4841 explore P: 0.32979\n",
      "episode:   317 total reward:    -356.23 avg. training loss:    12.4212 explore P: 0.32939\n",
      "episode:   318 total reward:    -427.71 avg. training loss:    48.3321 explore P: 0.32891\n",
      "episode:   319 total reward:  -12462.27 avg. training loss:   104.2540 explore P: 0.32653\n",
      "episode:   320 total reward:     -59.91 avg. training loss:    27.9549 explore P: 0.32646\n",
      ">>> target estimator network updated (23000)\n",
      "episode:   321 total reward:  -15658.83 avg. training loss:    71.0517 explore P: 0.32281\n",
      "episode:   322 total reward:    -148.96 avg. training loss:    43.7658 explore P: 0.32266\n",
      "episode:   323 total reward:     -74.79 avg. training loss:     8.6618 explore P: 0.32258\n",
      "episode:   324 total reward:    -119.42 avg. training loss:    28.3002 explore P: 0.32245\n",
      "episode:   325 total reward:    -375.55 avg. training loss:    19.8583 explore P: 0.32202\n",
      "episode:   326 total reward:   -2314.66 avg. training loss:    55.4174 explore P: 0.32032\n",
      "episode:   327 total reward:     -89.75 avg. training loss:     5.8048 explore P: 0.32023\n",
      "episode:   328 total reward:    -134.31 avg. training loss:    70.7201 explore P: 0.32009\n",
      "episode:   329 total reward:     -29.99 avg. training loss:     6.5661 explore P: 0.32006\n",
      "episode:   330 total reward:  -22034.54 avg. training loss:    39.4742 explore P: 0.31719\n",
      "episode:   331 total reward:  -62300.61 avg. training loss:    65.6848 explore P: 0.31308\n",
      "episode:   332 total reward:    -558.06 avg. training loss:    27.4970 explore P: 0.31233\n",
      "episode:   333 total reward:   -4368.30 avg. training loss:    37.1474 explore P: 0.31041\n",
      ">>> target estimator network updated (24000)\n",
      "episode:   334 total reward:  -42705.33 avg. training loss:    71.6014 explore P: 0.30701\n",
      "episode:   335 total reward:   -1613.45 avg. training loss:   170.7976 explore P: 0.30544\n",
      "episode:   336 total reward:  -17506.81 avg. training loss:    82.9830 explore P: 0.30283\n",
      "episode:   337 total reward:    -104.66 avg. training loss:   184.5193 explore P: 0.30273\n",
      "episode:   338 total reward:    -492.06 avg. training loss:   201.4684 explore P: 0.30209\n",
      "episode:   339 total reward:   -1141.20 avg. training loss:   103.2402 explore P: 0.30046\n",
      "episode:   340 total reward:    -149.24 avg. training loss:    36.2235 explore P: 0.30031\n",
      "episode:   341 total reward:    -499.99 avg. training loss:   277.8263 explore P: 0.29973\n",
      "episode:   342 total reward:    -651.46 avg. training loss:   132.7989 explore P: 0.29873\n",
      "episode:   343 total reward:    -119.54 avg. training loss:   100.3087 explore P: 0.29862\n",
      "episode:   344 total reward:     -59.90 avg. training loss:    34.6837 explore P: 0.29856\n",
      "episode:   345 total reward:    -264.24 avg. training loss:   127.4077 explore P: 0.29830\n",
      "episode:   346 total reward:     -89.75 avg. training loss:   159.0181 explore P: 0.29821\n",
      "episode:   347 total reward:     -59.89 avg. training loss:   167.4730 explore P: 0.29816\n",
      "episode:   348 total reward:    -104.71 avg. training loss:    72.6443 explore P: 0.29806\n",
      "episode:   349 total reward:   -3281.10 avg. training loss:   112.8359 explore P: 0.29636\n",
      "episode:   350 total reward:    -364.53 avg. training loss:   151.4386 explore P: 0.29598\n",
      "episode:   351 total reward:    -497.37 avg. training loss:    71.3748 explore P: 0.29545\n",
      ">>> target estimator network updated (25000)\n",
      "episode:   352 total reward:   -6098.19 avg. training loss:    60.2193 explore P: 0.29259\n",
      "episode:   353 total reward:    -312.39 avg. training loss:    51.7346 explore P: 0.29228\n",
      "episode:   354 total reward:   -2182.59 avg. training loss:    58.6824 explore P: 0.29051\n",
      "episode:   355 total reward:  -10831.88 avg. training loss:    43.6601 explore P: 0.28845\n",
      "episode:   356 total reward:    -191.32 avg. training loss:    48.8685 explore P: 0.28827\n",
      "episode:   357 total reward:     -44.97 avg. training loss:     1.7114 explore P: 0.28823\n",
      "episode:   358 total reward:  -79776.74 avg. training loss:    45.4956 explore P: 0.28369\n",
      "episode:   359 total reward:    -310.83 avg. training loss:    96.9372 explore P: 0.28339\n",
      "episode:   360 total reward:  -23515.99 avg. training loss:   306.4470 explore P: 0.28027\n",
      "episode:   361 total reward:    -344.93 avg. training loss:   174.3524 explore P: 0.27994\n",
      "episode:   362 total reward:     -59.95 avg. training loss:    41.0721 explore P: 0.27989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:   363 total reward:     -59.87 avg. training loss:     9.4697 explore P: 0.27983\n",
      ">>> target estimator network updated (26000)\n",
      "episode:   364 total reward:  -43782.34 avg. training loss:   116.8805 explore P: 0.27594\n",
      "episode:   365 total reward:  -10982.35 avg. training loss:    50.0785 explore P: 0.27378\n",
      "episode:   366 total reward:  -10405.38 avg. training loss:    60.0867 explore P: 0.27181\n",
      "episode:   367 total reward:  -20213.47 avg. training loss:   104.3209 explore P: 0.26930\n",
      ">>> target estimator network updated (27000)\n",
      "episode:   368 total reward:  -13309.96 avg. training loss:   109.5894 explore P: 0.26639\n",
      "episode:   369 total reward:  -13737.38 avg. training loss:   106.9653 explore P: 0.26444\n",
      "episode:   370 total reward:   -3742.04 avg. training loss:    58.7764 explore P: 0.26292\n",
      "episode:   371 total reward:     -29.98 avg. training loss:     3.0205 explore P: 0.26289\n",
      "episode:   372 total reward:     -44.97 avg. training loss:    28.3607 explore P: 0.26285\n",
      "episode:   373 total reward:   -5221.94 avg. training loss:   112.2857 explore P: 0.26114\n",
      "episode:   374 total reward:   -6855.54 avg. training loss:   125.7235 explore P: 0.25966\n",
      "episode:   375 total reward:   -4740.08 avg. training loss:    98.1106 explore P: 0.25818\n",
      "episode:   376 total reward:  -10152.38 avg. training loss:   102.8020 explore P: 0.25612\n",
      "episode:   377 total reward:   -9528.98 avg. training loss:   116.0624 explore P: 0.25423\n",
      ">>> target estimator network updated (28000)\n",
      "episode:   378 total reward:  -23102.55 avg. training loss:   118.5694 explore P: 0.25164\n",
      "episode:   379 total reward:    -133.98 avg. training loss:    28.0256 explore P: 0.25153\n",
      "episode:   380 total reward:     -29.99 avg. training loss:    12.4718 explore P: 0.25151\n",
      "episode:   381 total reward:     -29.99 avg. training loss:     4.4403 explore P: 0.25148\n",
      "episode:   382 total reward:     -29.99 avg. training loss:     3.2090 explore P: 0.25146\n",
      "episode:   383 total reward:  -12827.99 avg. training loss:    36.8437 explore P: 0.24968\n",
      "episode:   384 total reward:    -258.61 avg. training loss:    47.9520 explore P: 0.24946\n",
      "episode:   385 total reward:   -2832.57 avg. training loss:    71.4109 explore P: 0.24823\n",
      "episode:   386 total reward:    -329.47 avg. training loss:    21.7441 explore P: 0.24796\n",
      "episode:   387 total reward:    -450.98 avg. training loss:    68.8772 explore P: 0.24711\n",
      "episode:   388 total reward:  -10154.68 avg. training loss:    83.8587 explore P: 0.24458\n",
      "episode:   389 total reward:    -318.58 avg. training loss:    25.0336 explore P: 0.24432\n",
      ">>> target estimator network updated (29000)\n",
      "episode:   390 total reward:  -75659.56 avg. training loss:    40.3564 explore P: 0.24045\n",
      "episode:   391 total reward:    -466.11 avg. training loss:    46.4148 explore P: 0.24006\n",
      "episode:   392 total reward:   -3319.68 avg. training loss:    61.5222 explore P: 0.23880\n",
      "episode:   393 total reward:    -422.45 avg. training loss:    32.7020 explore P: 0.23847\n",
      "episode:   394 total reward:   -9977.57 avg. training loss:    30.2036 explore P: 0.23681\n",
      "episode:   395 total reward:  -28073.39 avg. training loss:    75.3038 explore P: 0.23405\n",
      "episode:   396 total reward:     -29.98 avg. training loss:    23.7765 explore P: 0.23402\n",
      "episode:   397 total reward:     -29.98 avg. training loss:    18.9194 explore P: 0.23400\n",
      "episode:   398 total reward:     -29.98 avg. training loss:    95.3066 explore P: 0.23398\n",
      "episode:   399 total reward:    -148.61 avg. training loss:   111.3834 explore P: 0.23387\n",
      "episode:   400 total reward:     -29.99 avg. training loss:     5.0071 explore P: 0.23385\n",
      ">>> target estimator network updated (30000)\n",
      "episode:   401 total reward:  -31253.00 avg. training loss:    58.2919 explore P: 0.23059\n",
      "episode:   402 total reward:    -689.46 avg. training loss:    35.7896 explore P: 0.23002\n",
      "episode:   403 total reward:    -573.42 avg. training loss:    55.4952 explore P: 0.22917\n",
      "episode:   404 total reward:    -423.34 avg. training loss:    59.0950 explore P: 0.22881\n",
      "episode:   405 total reward:    -177.52 avg. training loss:    41.3723 explore P: 0.22868\n",
      "episode:   406 total reward:   -1103.91 avg. training loss:    34.7519 explore P: 0.22773\n",
      "episode:   407 total reward:    -119.54 avg. training loss:    11.7915 explore P: 0.22764\n",
      "episode:   408 total reward:    -932.42 avg. training loss:    50.8513 explore P: 0.22687\n",
      "episode:   409 total reward:  -12687.95 avg. training loss:    42.7760 explore P: 0.22524\n",
      "episode:   410 total reward:    -119.54 avg. training loss:     8.5508 explore P: 0.22515\n",
      "episode:   411 total reward:    -207.50 avg. training loss:    31.5214 explore P: 0.22500\n",
      "episode:   412 total reward:    -104.47 avg. training loss:    21.5847 explore P: 0.22493\n",
      "episode:   413 total reward:     -29.99 avg. training loss:    14.0622 explore P: 0.22491\n",
      "episode:   414 total reward:     -29.99 avg. training loss:     7.1227 explore P: 0.22489\n",
      "episode:   415 total reward:     -29.99 avg. training loss:     3.9118 explore P: 0.22486\n",
      "episode:   416 total reward:  -10175.36 avg. training loss:    41.0704 explore P: 0.22313\n",
      "episode:   417 total reward:     -89.67 avg. training loss:    28.6318 explore P: 0.22307\n",
      "episode:   418 total reward:   -1546.79 avg. training loss:    36.1948 explore P: 0.22213\n",
      "episode:   419 total reward:    -572.31 avg. training loss:    88.8747 explore P: 0.22131\n",
      ">>> target estimator network updated (31000)\n",
      "episode:   420 total reward:  -47129.80 avg. training loss:    52.9436 explore P: 0.21836\n",
      "episode:   421 total reward:   -4018.87 avg. training loss:    54.9345 explore P: 0.21686\n",
      "episode:   422 total reward:   -1453.88 avg. training loss:   110.9277 explore P: 0.21603\n",
      "episode:   423 total reward:    -398.10 avg. training loss:    23.8007 explore P: 0.21544\n",
      "episode:   424 total reward:   -1472.23 avg. training loss:    37.1239 explore P: 0.21455\n",
      "episode:   425 total reward:  -28256.11 avg. training loss:    42.0354 explore P: 0.21241\n",
      "episode:   426 total reward:    -652.55 avg. training loss:    64.3532 explore P: 0.21185\n",
      "episode:   427 total reward:  -11999.75 avg. training loss:    38.3224 explore P: 0.21005\n",
      ">>> target estimator network updated (32000)\n",
      "episode:   428 total reward:  -39651.71 avg. training loss:    37.3170 explore P: 0.20737\n",
      "episode:   429 total reward:     -89.70 avg. training loss:    13.6826 explore P: 0.20732\n",
      "episode:   430 total reward:     -89.77 avg. training loss:     9.0548 explore P: 0.20726\n",
      "episode:   431 total reward:     -89.78 avg. training loss:     1.6967 explore P: 0.20720\n",
      "episode:   432 total reward:    -494.00 avg. training loss:    65.9199 explore P: 0.20685\n",
      "episode:   433 total reward:    -104.62 avg. training loss:    21.6404 explore P: 0.20678\n",
      "episode:   434 total reward:    -724.09 avg. training loss:    29.3227 explore P: 0.20600\n",
      "episode:   435 total reward:   -1003.25 avg. training loss:    31.7545 explore P: 0.20516\n",
      "episode:   436 total reward:     -29.99 avg. training loss:     1.7512 explore P: 0.20514\n",
      "episode:   437 total reward:     -29.99 avg. training loss:   102.8297 explore P: 0.20512\n",
      "episode:   438 total reward:     -29.99 avg. training loss:   185.6950 explore P: 0.20510\n",
      "episode:   439 total reward:     -29.99 avg. training loss:    11.9106 explore P: 0.20508\n",
      "episode:   440 total reward:     -29.99 avg. training loss:     5.6701 explore P: 0.20506\n",
      "episode:   441 total reward:     -29.99 avg. training loss:     2.3156 explore P: 0.20504\n",
      "episode:   442 total reward:     -29.99 avg. training loss:     4.1712 explore P: 0.20502\n",
      "episode:   443 total reward:     -29.99 avg. training loss:    53.2135 explore P: 0.20500\n",
      "episode:   444 total reward:    -514.96 avg. training loss:    38.1331 explore P: 0.20457\n",
      "episode:   445 total reward:   -1082.02 avg. training loss:    19.8568 explore P: 0.20355\n",
      "episode:   446 total reward:    -633.30 avg. training loss:    49.1012 explore P: 0.20291\n",
      "episode:   447 total reward:    -208.65 avg. training loss:    17.0877 explore P: 0.20277\n",
      "episode:   448 total reward:   -2201.31 avg. training loss:    36.9691 explore P: 0.20160\n",
      "episode:   449 total reward:    -727.32 avg. training loss:    39.8143 explore P: 0.20085\n",
      ">>> target estimator network updated (33000)\n",
      "episode:   450 total reward:    -727.28 avg. training loss:    45.2497 explore P: 0.20009\n",
      "episode:   451 total reward:    -220.43 avg. training loss:    24.3768 explore P: 0.19995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:   452 total reward:   -1193.02 avg. training loss:    40.8122 explore P: 0.19896\n",
      "episode:   453 total reward:    -458.95 avg. training loss:    17.8909 explore P: 0.19829\n",
      "episode:   454 total reward:    -391.75 avg. training loss:    44.2197 explore P: 0.19769\n",
      "episode:   455 total reward:    -134.32 avg. training loss:    31.0293 explore P: 0.19761\n",
      "episode:   456 total reward:   -2591.89 avg. training loss:    41.3636 explore P: 0.19671\n",
      "episode:   457 total reward:  -25586.76 avg. training loss:    23.9412 explore P: 0.19457\n",
      "episode:   458 total reward:    -436.91 avg. training loss:    44.6226 explore P: 0.19389\n",
      "episode:   459 total reward:    -119.54 avg. training loss:    18.9677 explore P: 0.19382\n",
      "episode:   460 total reward:    -305.82 avg. training loss:    22.8137 explore P: 0.19363\n",
      "episode:   461 total reward:    -516.28 avg. training loss:    36.1714 explore P: 0.19309\n",
      "episode:   462 total reward:     -29.99 avg. training loss:    19.0332 explore P: 0.19307\n",
      "episode:   463 total reward:    -739.52 avg. training loss:    47.9798 explore P: 0.19222\n",
      "episode:   464 total reward:    -148.79 avg. training loss:    55.5968 explore P: 0.19213\n",
      "episode:   465 total reward:    -390.65 avg. training loss:    55.9689 explore P: 0.19188\n",
      "episode:   466 total reward:    -463.90 avg. training loss:    17.1246 explore P: 0.19155\n",
      "episode:   467 total reward:    -521.16 avg. training loss:    19.2269 explore P: 0.19122\n",
      ">>> target estimator network updated (34000)\n",
      "episode:   468 total reward:    -445.02 avg. training loss:    22.5606 explore P: 0.19068\n",
      "episode:   469 total reward:  -50187.81 avg. training loss:    39.0746 explore P: 0.18795\n",
      "episode:   470 total reward:     -74.84 avg. training loss:     2.1209 explore P: 0.18791\n",
      "episode:   471 total reward:    -221.96 avg. training loss:    56.2222 explore P: 0.18777\n",
      "episode:   472 total reward:    -771.86 avg. training loss:    39.0917 explore P: 0.18703\n",
      "episode:   473 total reward:    -404.07 avg. training loss:    21.1488 explore P: 0.18641\n",
      "episode:   474 total reward:    -725.45 avg. training loss:    31.3816 explore P: 0.18589\n",
      "episode:   475 total reward:    -484.35 avg. training loss:    44.5539 explore P: 0.18554\n",
      "episode:   476 total reward:    -134.50 avg. training loss:    36.0309 explore P: 0.18546\n",
      "episode:   477 total reward:  -63725.71 avg. training loss:    36.7031 explore P: 0.18279\n",
      ">>> target estimator network updated (35000)\n",
      "episode:   478 total reward:  -13382.08 avg. training loss:    30.8773 explore P: 0.18126\n",
      "episode:   479 total reward:    -206.43 avg. training loss:    53.9780 explore P: 0.18114\n",
      "episode:   480 total reward:     -59.90 avg. training loss:     3.8346 explore P: 0.18111\n",
      "episode:   481 total reward:    -685.89 avg. training loss:    53.6680 explore P: 0.18066\n",
      "episode:   482 total reward:  -49829.19 avg. training loss:    36.0107 explore P: 0.17842\n",
      "episode:   483 total reward:   -2774.23 avg. training loss:    16.5919 explore P: 0.17760\n",
      "episode:   484 total reward:    -440.30 avg. training loss:    25.9920 explore P: 0.17734\n",
      "episode:   485 total reward:     -74.74 avg. training loss:     3.7311 explore P: 0.17729\n",
      "episode:   486 total reward:     -29.99 avg. training loss:     1.1159 explore P: 0.17728\n",
      "episode:   487 total reward:     -29.99 avg. training loss:   148.8698 explore P: 0.17726\n",
      "episode:   488 total reward:    -206.32 avg. training loss:     6.7201 explore P: 0.17714\n",
      "episode:   489 total reward:    -510.86 avg. training loss:    30.6874 explore P: 0.17658\n",
      "episode:   490 total reward:    -528.64 avg. training loss:    32.3943 explore P: 0.17598\n",
      "episode:   491 total reward:    -582.25 avg. training loss:    34.5936 explore P: 0.17524\n",
      "episode:   492 total reward:   -6507.10 avg. training loss:    32.4494 explore P: 0.17410\n",
      "episode:   493 total reward:    -148.96 avg. training loss:    48.8959 explore P: 0.17401\n",
      "episode:   494 total reward:    -119.54 avg. training loss:    70.0535 explore P: 0.17395\n",
      "episode:   495 total reward:     -74.82 avg. training loss:    95.1996 explore P: 0.17391\n",
      "episode:   496 total reward:     -29.99 avg. training loss:   179.7088 explore P: 0.17389\n",
      "episode:   497 total reward:     -29.99 avg. training loss:     5.1590 explore P: 0.17388\n",
      ">>> target estimator network updated (36000)\n",
      "episode:   498 total reward:    -402.39 avg. training loss:    62.6143 explore P: 0.17365\n",
      "episode:   499 total reward:    -509.29 avg. training loss:    22.4161 explore P: 0.17321\n",
      "episode:   500 total reward:   -2588.34 avg. training loss:    25.7257 explore P: 0.17246\n",
      "episode:   501 total reward:    -794.44 avg. training loss:    25.9441 explore P: 0.17182\n",
      "episode:   502 total reward:   -1336.68 avg. training loss:    22.4797 explore P: 0.17086\n",
      "episode:   503 total reward:    -625.39 avg. training loss:    24.6883 explore P: 0.17027\n",
      "episode:   504 total reward:   -1167.30 avg. training loss:    30.7057 explore P: 0.16962\n",
      "episode:   505 total reward:    -443.28 avg. training loss:    15.7988 explore P: 0.16908\n",
      "episode:   506 total reward:    -273.75 avg. training loss:     9.5536 explore P: 0.16893\n",
      "episode:   507 total reward:   -1081.87 avg. training loss:    29.8773 explore P: 0.16814\n",
      "episode:   508 total reward:    -119.53 avg. training loss:     9.3861 explore P: 0.16807\n",
      "episode:   509 total reward:    -163.89 avg. training loss:    32.7205 explore P: 0.16799\n",
      "episode:   510 total reward:    -514.13 avg. training loss:    26.3923 explore P: 0.16721\n",
      "episode:   511 total reward:   -1504.90 avg. training loss:    25.1157 explore P: 0.16652\n",
      "episode:   512 total reward:   -2329.09 avg. training loss:    33.8079 explore P: 0.16582\n",
      ">>> target estimator network updated (37000)\n",
      "episode:   513 total reward:  -45976.87 avg. training loss:    23.3420 explore P: 0.16347\n",
      "episode:   514 total reward:     -29.99 avg. training loss:    92.3014 explore P: 0.16345\n",
      "episode:   515 total reward:   -1626.92 avg. training loss:    42.3482 explore P: 0.16241\n",
      "episode:   516 total reward:     -59.92 avg. training loss:    49.6305 explore P: 0.16238\n",
      "episode:   517 total reward:    -177.99 avg. training loss:    95.0808 explore P: 0.16229\n",
      "episode:   518 total reward:     -29.99 avg. training loss:     7.6583 explore P: 0.16228\n",
      "episode:   519 total reward:     -29.99 avg. training loss:    12.3390 explore P: 0.16226\n",
      "episode:   520 total reward:     -29.99 avg. training loss:    14.1564 explore P: 0.16225\n",
      "episode:   521 total reward:     -44.93 avg. training loss:    16.2280 explore P: 0.16222\n",
      "episode:   522 total reward:     -29.99 avg. training loss:     3.0539 explore P: 0.16221\n",
      "episode:   523 total reward:   -3142.45 avg. training loss:    14.5585 explore P: 0.16140\n",
      "episode:   524 total reward:   -2411.59 avg. training loss:    27.8436 explore P: 0.16067\n",
      "episode:   525 total reward:    -232.80 avg. training loss:    28.3301 explore P: 0.16055\n",
      "episode:   526 total reward:    -453.10 avg. training loss:     5.5873 explore P: 0.16028\n",
      "episode:   527 total reward:   -4400.53 avg. training loss:    16.6725 explore P: 0.15928\n",
      ">>> target estimator network updated (38000)\n",
      "episode:   528 total reward:  -72313.87 avg. training loss:    20.3892 explore P: 0.15682\n",
      "episode:   529 total reward:    -498.00 avg. training loss:    32.4701 explore P: 0.15650\n",
      "episode:   530 total reward:     -74.75 avg. training loss:    10.2314 explore P: 0.15647\n",
      "episode:   531 total reward:    -359.63 avg. training loss:    30.1748 explore P: 0.15628\n",
      "episode:   532 total reward:     -29.98 avg. training loss:   103.0674 explore P: 0.15626\n",
      "episode:   533 total reward:     -29.98 avg. training loss:     6.3908 explore P: 0.15625\n",
      "episode:   534 total reward:   -1976.94 avg. training loss:    27.7299 explore P: 0.15553\n",
      "episode:   535 total reward:     -29.99 avg. training loss:     1.3216 explore P: 0.15552\n",
      "episode:   536 total reward:     -29.99 avg. training loss:     2.8565 explore P: 0.15550\n",
      "episode:   537 total reward:     -29.99 avg. training loss:    23.1891 explore P: 0.15549\n",
      "episode:   538 total reward:     -29.99 avg. training loss:    62.4471 explore P: 0.15548\n",
      "episode:   539 total reward:     -29.99 avg. training loss:     7.0375 explore P: 0.15546\n",
      "episode:   540 total reward:     -44.95 avg. training loss:    46.3466 explore P: 0.15544\n",
      "episode:   541 total reward:     -29.99 avg. training loss:   119.0376 explore P: 0.15542\n",
      "episode:   542 total reward:    -390.29 avg. training loss:    48.0621 explore P: 0.15523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:   543 total reward:     -29.98 avg. training loss:     5.3778 explore P: 0.15521\n",
      "episode:   544 total reward:     -29.98 avg. training loss:     8.0511 explore P: 0.15520\n",
      "episode:   545 total reward:    -452.12 avg. training loss:    31.0906 explore P: 0.15488\n",
      "episode:   546 total reward:     -29.99 avg. training loss:     7.1364 explore P: 0.15487\n",
      "episode:   547 total reward:    -320.61 avg. training loss:    41.5023 explore P: 0.15471\n",
      "episode:   548 total reward:     -29.98 avg. training loss:     6.2045 explore P: 0.15469\n",
      "episode:   549 total reward:    -104.63 avg. training loss:     3.1800 explore P: 0.15464\n",
      "episode:   550 total reward:     -44.97 avg. training loss:    10.9811 explore P: 0.15462\n",
      "episode:   551 total reward:     -29.98 avg. training loss:     2.5746 explore P: 0.15460\n",
      "episode:   552 total reward:     -29.98 avg. training loss:   101.5098 explore P: 0.15459\n",
      "episode:   553 total reward:     -44.97 avg. training loss:    69.7377 explore P: 0.15457\n",
      "episode:   554 total reward:     -29.98 avg. training loss:     4.3650 explore P: 0.15455\n",
      "episode:   555 total reward:    -487.00 avg. training loss:    29.4698 explore P: 0.15428\n",
      "episode:   556 total reward:    -260.13 avg. training loss:    39.2809 explore P: 0.15415\n",
      "episode:   557 total reward:    -869.57 avg. training loss:    34.4563 explore P: 0.15355\n",
      "episode:   558 total reward:    -148.96 avg. training loss:    20.6161 explore P: 0.15347\n",
      "episode:   559 total reward:     -59.92 avg. training loss:     1.9168 explore P: 0.15345\n",
      "episode:   560 total reward:     -59.92 avg. training loss:     4.0265 explore P: 0.15342\n",
      "episode:   561 total reward:     -59.93 avg. training loss:     2.9595 explore P: 0.15339\n",
      "episode:   562 total reward:     -29.98 avg. training loss:     3.8639 explore P: 0.15337\n",
      "episode:   563 total reward:     -59.87 avg. training loss:     2.1940 explore P: 0.15335\n",
      "episode:   564 total reward:     -29.98 avg. training loss:     4.0176 explore P: 0.15333\n",
      "episode:   565 total reward:     -29.98 avg. training loss:     3.4045 explore P: 0.15332\n",
      "episode:   566 total reward:     -29.98 avg. training loss:     0.7895 explore P: 0.15330\n",
      "episode:   567 total reward:     -44.95 avg. training loss:     5.9210 explore P: 0.15328\n",
      "episode:   568 total reward:     -74.90 avg. training loss:    28.3794 explore P: 0.15324\n",
      "episode:   569 total reward:    -203.15 avg. training loss:    32.9191 explore P: 0.15314\n",
      "episode:   570 total reward:    -315.06 avg. training loss:    15.2639 explore P: 0.15299\n",
      "episode:   571 total reward:     -29.98 avg. training loss:    19.3112 explore P: 0.15297\n",
      "episode:   572 total reward:     -29.98 avg. training loss:     3.6917 explore P: 0.15296\n",
      "episode:   573 total reward:     -44.97 avg. training loss:   122.5014 explore P: 0.15294\n",
      "episode:   574 total reward:     -29.98 avg. training loss:    95.0734 explore P: 0.15292\n",
      "episode:   575 total reward:     -29.98 avg. training loss:    13.1173 explore P: 0.15291\n",
      "episode:   576 total reward:     -44.96 avg. training loss:    12.3921 explore P: 0.15289\n",
      "episode:   577 total reward:     -29.98 avg. training loss:    46.2008 explore P: 0.15287\n",
      "episode:   578 total reward:     -29.98 avg. training loss:     5.9193 explore P: 0.15286\n",
      "episode:   579 total reward:     -29.98 avg. training loss:    10.0660 explore P: 0.15284\n",
      "episode:   580 total reward:     -44.97 avg. training loss:     7.7389 explore P: 0.15282\n",
      "episode:   581 total reward:     -29.98 avg. training loss:   111.9816 explore P: 0.15281\n",
      "episode:   582 total reward:     -29.99 avg. training loss:    36.6587 explore P: 0.15279\n",
      "episode:   583 total reward:     -29.98 avg. training loss:     2.4789 explore P: 0.15278\n",
      "episode:   584 total reward:     -29.98 avg. training loss:     6.0297 explore P: 0.15277\n",
      "episode:   585 total reward:    -164.08 avg. training loss:    39.3503 explore P: 0.15269\n",
      "episode:   586 total reward:    -935.11 avg. training loss:    37.4512 explore P: 0.15212\n",
      "episode:   587 total reward:     -29.98 avg. training loss:    74.4329 explore P: 0.15210\n",
      "episode:   588 total reward:     -44.97 avg. training loss:   162.8178 explore P: 0.15208\n",
      "episode:   589 total reward:     -29.98 avg. training loss:    25.9131 explore P: 0.15207\n",
      "episode:   590 total reward:    -950.51 avg. training loss:    21.6754 explore P: 0.15145\n",
      "episode:   591 total reward:    -104.59 avg. training loss:     3.0154 explore P: 0.15140\n",
      "episode:   592 total reward:     -29.98 avg. training loss:   114.4048 explore P: 0.15139\n",
      "episode:   593 total reward:     -29.98 avg. training loss:    88.1874 explore P: 0.15137\n",
      ">>> target estimator network updated (39000)\n",
      "episode:   594 total reward:   -1606.87 avg. training loss:    21.0001 explore P: 0.15070\n",
      "episode:   595 total reward:    -176.61 avg. training loss:     8.8894 explore P: 0.15062\n",
      "episode:   596 total reward:     -29.98 avg. training loss:     1.0994 explore P: 0.15061\n",
      "episode:   597 total reward:     -29.98 avg. training loss:     2.1352 explore P: 0.15059\n",
      "episode:   598 total reward:   -1173.51 avg. training loss:    19.8458 explore P: 0.15004\n",
      "episode:   599 total reward:     -59.86 avg. training loss:     4.1993 explore P: 0.15001\n",
      "episode:   600 total reward:    -353.65 avg. training loss:    25.0949 explore P: 0.14983\n",
      "episode:   601 total reward:     -59.93 avg. training loss:     3.0182 explore P: 0.14981\n",
      "episode:   602 total reward:     -29.98 avg. training loss:     3.4018 explore P: 0.14979\n",
      "episode:   603 total reward:     -29.98 avg. training loss:    36.2784 explore P: 0.14978\n",
      "episode:   604 total reward:     -29.98 avg. training loss:    67.4070 explore P: 0.14976\n",
      "episode:   605 total reward:     -29.98 avg. training loss:    83.0049 explore P: 0.14975\n",
      "episode:   606 total reward:     -29.98 avg. training loss:     5.5782 explore P: 0.14974\n",
      "episode:   607 total reward:     -29.98 avg. training loss:     3.7370 explore P: 0.14972\n",
      "episode:   608 total reward:     -29.98 avg. training loss:     2.4296 explore P: 0.14971\n",
      "episode:   609 total reward:     -29.98 avg. training loss:    14.2734 explore P: 0.14969\n",
      "episode:   610 total reward:     -29.98 avg. training loss:     2.9953 explore P: 0.14968\n",
      "episode:   611 total reward:     -44.96 avg. training loss:     4.0262 explore P: 0.14966\n",
      "episode:   612 total reward:    -104.26 avg. training loss:    60.9415 explore P: 0.14961\n",
      "episode:   613 total reward:     -29.98 avg. training loss:     9.0199 explore P: 0.14960\n",
      "episode:   614 total reward:     -29.98 avg. training loss:     5.4017 explore P: 0.14958\n",
      "episode:   615 total reward:     -29.98 avg. training loss:     4.0588 explore P: 0.14957\n",
      "episode:   616 total reward:     -29.98 avg. training loss:    24.6226 explore P: 0.14955\n",
      "episode:   617 total reward:     -59.87 avg. training loss:    60.1530 explore P: 0.14953\n",
      "episode:   618 total reward:    -249.93 avg. training loss:    16.2603 explore P: 0.14941\n",
      "episode:   619 total reward:     -29.98 avg. training loss:     4.3977 explore P: 0.14939\n",
      "episode:   620 total reward:     -29.98 avg. training loss:    77.0653 explore P: 0.14938\n",
      "episode:   621 total reward:     -29.98 avg. training loss:     7.9357 explore P: 0.14937\n",
      "episode:   622 total reward:     -29.98 avg. training loss:    24.6667 explore P: 0.14935\n",
      "episode:   623 total reward:     -29.98 avg. training loss:     2.8733 explore P: 0.14934\n",
      "episode:   624 total reward:     -29.98 avg. training loss:     3.3754 explore P: 0.14932\n",
      "episode:   625 total reward:     -44.97 avg. training loss:   146.0957 explore P: 0.14930\n",
      "episode:   626 total reward:     -29.98 avg. training loss:    15.5277 explore P: 0.14929\n",
      "episode:   627 total reward:   -2048.59 avg. training loss:    21.8018 explore P: 0.14848\n",
      "episode:   628 total reward:     -29.98 avg. training loss:   344.4779 explore P: 0.14846\n",
      "episode:   629 total reward:     -29.98 avg. training loss:    70.1308 explore P: 0.14845\n",
      "episode:   630 total reward:     -29.98 avg. training loss:     4.8869 explore P: 0.14844\n",
      "episode:   631 total reward:     -29.98 avg. training loss:     1.6446 explore P: 0.14842\n",
      "episode:   632 total reward:     -29.98 avg. training loss:     2.2029 explore P: 0.14841\n",
      "episode:   633 total reward:     -29.98 avg. training loss:     4.6614 explore P: 0.14839\n",
      "episode:   634 total reward:     -29.98 avg. training loss:     3.5023 explore P: 0.14838\n",
      "episode:   635 total reward:     -29.98 avg. training loss:     1.7527 explore P: 0.14837\n",
      "episode:   636 total reward:     -29.98 avg. training loss:   103.5112 explore P: 0.14835\n",
      "episode:   637 total reward:     -74.77 avg. training loss:     4.3001 explore P: 0.14832\n",
      "episode:   638 total reward:     -29.98 avg. training loss:     3.3509 explore P: 0.14830\n",
      "episode:   639 total reward:    -104.58 avg. training loss:    17.1207 explore P: 0.14826\n",
      "episode:   640 total reward:     -29.98 avg. training loss:     4.3403 explore P: 0.14824\n",
      "episode:   641 total reward:     -29.98 avg. training loss:     0.9956 explore P: 0.14823\n",
      "episode:   642 total reward:     -44.95 avg. training loss:    79.5262 explore P: 0.14821\n",
      "episode:   643 total reward:     -29.98 avg. training loss:     4.1031 explore P: 0.14819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:   644 total reward:     -29.98 avg. training loss:    96.7921 explore P: 0.14818\n",
      "episode:   645 total reward:     -29.98 avg. training loss:     4.9587 explore P: 0.14817\n",
      "episode:   646 total reward:     -29.98 avg. training loss:     3.9445 explore P: 0.14815\n",
      "episode:   647 total reward:     -29.98 avg. training loss:     6.0076 explore P: 0.14814\n",
      "episode:   648 total reward:     -29.98 avg. training loss:    31.3477 explore P: 0.14812\n",
      "episode:   649 total reward:     -29.98 avg. training loss:   116.1124 explore P: 0.14811\n",
      "episode:   650 total reward:     -29.98 avg. training loss:    12.7629 explore P: 0.14810\n",
      "episode:   651 total reward:     -44.97 avg. training loss:     5.8592 explore P: 0.14808\n",
      "episode:   652 total reward:    -162.87 avg. training loss:    34.8361 explore P: 0.14800\n",
      "episode:   653 total reward:     -29.98 avg. training loss:     1.8563 explore P: 0.14799\n",
      "episode:   654 total reward:     -29.98 avg. training loss:     1.7006 explore P: 0.14797\n",
      "episode:   655 total reward:     -59.92 avg. training loss:     1.6577 explore P: 0.14794\n",
      "episode:   656 total reward:     -59.93 avg. training loss:    28.9368 explore P: 0.14792\n",
      "episode:   657 total reward:     -29.98 avg. training loss:    80.2411 explore P: 0.14790\n",
      "episode:   658 total reward:     -89.58 avg. training loss:     7.0477 explore P: 0.14786\n",
      "episode:   659 total reward:     -29.98 avg. training loss:     2.8738 explore P: 0.14785\n",
      "episode:   660 total reward:     -29.98 avg. training loss:   110.9031 explore P: 0.14783\n",
      "episode:   661 total reward:     -29.98 avg. training loss:   114.8675 explore P: 0.14782\n",
      "episode:   662 total reward:     -29.98 avg. training loss:    21.0469 explore P: 0.14781\n",
      "episode:   663 total reward:     -29.98 avg. training loss:     4.4689 explore P: 0.14779\n",
      "episode:   664 total reward:     -29.98 avg. training loss:     8.1248 explore P: 0.14778\n",
      "episode:   665 total reward:     -89.61 avg. training loss:     8.0440 explore P: 0.14774\n",
      "episode:   666 total reward:     -29.98 avg. training loss:     5.9212 explore P: 0.14772\n",
      "episode:   667 total reward:     -44.97 avg. training loss:    44.1168 explore P: 0.14770\n",
      "episode:   668 total reward:    -104.50 avg. training loss:    38.9668 explore P: 0.14766\n",
      "episode:   669 total reward:     -29.98 avg. training loss:     4.4056 explore P: 0.14764\n",
      "episode:   670 total reward:     -29.98 avg. training loss:     7.7123 explore P: 0.14763\n",
      "episode:   671 total reward:     -29.98 avg. training loss:     5.8598 explore P: 0.14761\n",
      "episode:   672 total reward:     -29.98 avg. training loss:     2.1326 explore P: 0.14760\n",
      "episode:   673 total reward:     -29.98 avg. training loss:     3.8136 explore P: 0.14759\n",
      "episode:   674 total reward:     -59.88 avg. training loss:    42.9688 explore P: 0.14756\n",
      "episode:   675 total reward:     -29.98 avg. training loss:     1.4175 explore P: 0.14755\n",
      "episode:   676 total reward:     -29.98 avg. training loss:     1.9716 explore P: 0.14753\n",
      "episode:   677 total reward:     -29.98 avg. training loss:     2.5811 explore P: 0.14752\n",
      "episode:   678 total reward:     -29.98 avg. training loss:     4.2551 explore P: 0.14750\n",
      "episode:   679 total reward:     -29.98 avg. training loss:     2.4620 explore P: 0.14749\n",
      "episode:   680 total reward:     -44.95 avg. training loss:     1.4520 explore P: 0.14747\n",
      "episode:   681 total reward:     -29.98 avg. training loss:     1.6342 explore P: 0.14746\n",
      "episode:   682 total reward:     -29.98 avg. training loss:     8.4254 explore P: 0.14744\n",
      "episode:   683 total reward:     -29.98 avg. training loss:     1.2205 explore P: 0.14743\n",
      "episode:   684 total reward:     -29.98 avg. training loss:     2.1545 explore P: 0.14741\n",
      "episode:   685 total reward:     -29.98 avg. training loss:     1.0172 explore P: 0.14740\n",
      "episode:   686 total reward:     -29.98 avg. training loss:     1.6346 explore P: 0.14739\n",
      "episode:   687 total reward:     -29.98 avg. training loss:     1.3130 explore P: 0.14737\n",
      "episode:   688 total reward:     -29.98 avg. training loss:   130.2848 explore P: 0.14736\n",
      "episode:   689 total reward:     -59.86 avg. training loss:     0.8299 explore P: 0.14733\n",
      "episode:   690 total reward:     -29.98 avg. training loss:     1.0547 explore P: 0.14732\n",
      "episode:   691 total reward:     -29.98 avg. training loss:    25.2800 explore P: 0.14731\n",
      "episode:   692 total reward:     -74.90 avg. training loss:     4.8501 explore P: 0.14727\n",
      "episode:   693 total reward:     -29.98 avg. training loss:     5.6096 explore P: 0.14726\n",
      "episode:   694 total reward:     -29.98 avg. training loss:     0.6329 explore P: 0.14724\n",
      "episode:   695 total reward:     -44.96 avg. training loss:     1.6099 explore P: 0.14722\n",
      "episode:   696 total reward:     -59.88 avg. training loss:     2.4142 explore P: 0.14720\n",
      "episode:   697 total reward:     -29.98 avg. training loss:   115.2398 explore P: 0.14718\n",
      "episode:   698 total reward:     -74.85 avg. training loss:     5.7749 explore P: 0.14715\n",
      "episode:   699 total reward:     -29.98 avg. training loss:     1.5690 explore P: 0.14713\n",
      "episode:   700 total reward:     -29.98 avg. training loss:     3.1355 explore P: 0.14712\n",
      "episode:   701 total reward:     -29.98 avg. training loss:     1.9778 explore P: 0.14711\n",
      "episode:   702 total reward:     -29.98 avg. training loss:    14.8249 explore P: 0.14709\n",
      "episode:   703 total reward:     -29.98 avg. training loss:     3.4034 explore P: 0.14708\n",
      "episode:   704 total reward:     -29.99 avg. training loss:     2.8212 explore P: 0.14706\n",
      "episode:   705 total reward:     -29.98 avg. training loss:     3.6806 explore P: 0.14705\n",
      "episode:   706 total reward:     -29.98 avg. training loss:     6.6261 explore P: 0.14704\n",
      "episode:   707 total reward:     -29.98 avg. training loss:     4.1609 explore P: 0.14702\n",
      "episode:   708 total reward:     -29.98 avg. training loss:     1.2247 explore P: 0.14701\n",
      "episode:   709 total reward:     -29.98 avg. training loss:    28.9220 explore P: 0.14700\n",
      "episode:   710 total reward:     -44.97 avg. training loss:     7.6811 explore P: 0.14698\n",
      "episode:   711 total reward:     -89.77 avg. training loss:    39.0805 explore P: 0.14693\n",
      "episode:   712 total reward:     -29.98 avg. training loss:     3.4791 explore P: 0.14692\n",
      "episode:   713 total reward:   -6533.79 avg. training loss:    30.2543 explore P: 0.14584\n",
      "episode:   714 total reward:     -29.98 avg. training loss:     1.4328 explore P: 0.14582\n",
      "episode:   715 total reward:     -29.98 avg. training loss:     3.1783 explore P: 0.14581\n",
      "episode:   716 total reward:     -29.98 avg. training loss:     2.8145 explore P: 0.14580\n",
      "episode:   717 total reward:     -29.98 avg. training loss:     2.8410 explore P: 0.14578\n",
      "episode:   718 total reward:     -29.98 avg. training loss:     1.9407 explore P: 0.14577\n",
      "episode:   719 total reward:     -74.90 avg. training loss:    78.4159 explore P: 0.14574\n",
      "episode:   720 total reward:     -29.98 avg. training loss:     1.6329 explore P: 0.14572\n",
      "episode:   721 total reward:     -29.98 avg. training loss:     1.3642 explore P: 0.14571\n",
      "episode:   722 total reward:     -29.98 avg. training loss:    77.2168 explore P: 0.14569\n",
      "episode:   723 total reward:     -29.98 avg. training loss:     1.4666 explore P: 0.14568\n",
      "episode:   724 total reward:     -44.97 avg. training loss:     7.3866 explore P: 0.14566\n",
      "episode:   725 total reward:     -29.98 avg. training loss:     1.2712 explore P: 0.14565\n",
      "episode:   726 total reward:     -29.98 avg. training loss:    82.3622 explore P: 0.14563\n",
      "episode:   727 total reward:     -29.98 avg. training loss:    10.5082 explore P: 0.14562\n",
      "episode:   728 total reward:    -191.06 avg. training loss:    43.5857 explore P: 0.14553\n",
      "episode:   729 total reward:    -275.85 avg. training loss:     7.0416 explore P: 0.14540\n",
      "episode:   730 total reward:     -29.98 avg. training loss:    33.5584 explore P: 0.14539\n",
      "episode:   731 total reward:     -29.98 avg. training loss:    59.5350 explore P: 0.14538\n",
      "episode:   732 total reward:     -59.86 avg. training loss:    94.1558 explore P: 0.14535\n",
      "episode:   733 total reward:     -29.98 avg. training loss:     5.6845 explore P: 0.14534\n",
      "episode:   734 total reward:   -1459.52 avg. training loss:    35.1415 explore P: 0.14473\n",
      "episode:   735 total reward:     -29.98 avg. training loss:   106.2017 explore P: 0.14472\n",
      "episode:   736 total reward:     -29.98 avg. training loss:    14.8183 explore P: 0.14471\n",
      "episode:   737 total reward:     -29.98 avg. training loss:     2.0862 explore P: 0.14469\n",
      "episode:   738 total reward:     -29.98 avg. training loss:    32.3688 explore P: 0.14468\n",
      "episode:   739 total reward:     -29.98 avg. training loss:     3.4010 explore P: 0.14467\n",
      "episode:   740 total reward:     -29.98 avg. training loss:     2.2901 explore P: 0.14465\n",
      "episode:   741 total reward:     -29.98 avg. training loss:     4.6731 explore P: 0.14464\n",
      "episode:   742 total reward:     -89.78 avg. training loss:     1.9943 explore P: 0.14460\n",
      "episode:   743 total reward:     -44.97 avg. training loss:     3.2605 explore P: 0.14458\n",
      "episode:   744 total reward:     -29.98 avg. training loss:     1.1754 explore P: 0.14457\n",
      "episode:   745 total reward:     -29.98 avg. training loss:     4.8914 explore P: 0.14455\n",
      "episode:   746 total reward:     -29.98 avg. training loss:     1.6229 explore P: 0.14454\n",
      "episode:   747 total reward:     -29.98 avg. training loss:    66.0795 explore P: 0.14453\n",
      "episode:   748 total reward:     -29.98 avg. training loss:     5.2238 explore P: 0.14451\n",
      "episode:   749 total reward:     -29.98 avg. training loss:     1.5738 explore P: 0.14450\n",
      "episode:   750 total reward:     -29.98 avg. training loss:     2.3164 explore P: 0.14449\n",
      "episode:   751 total reward:     -29.98 avg. training loss:   106.8907 explore P: 0.14447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:   752 total reward:     -29.98 avg. training loss:     5.3272 explore P: 0.14446\n",
      "episode:   753 total reward:     -29.98 avg. training loss:     2.2305 explore P: 0.14444\n",
      "episode:   754 total reward:     -44.96 avg. training loss:     1.9647 explore P: 0.14442\n",
      "episode:   755 total reward:     -29.98 avg. training loss:     7.4377 explore P: 0.14441\n",
      "episode:   756 total reward:     -59.86 avg. training loss:     3.7254 explore P: 0.14438\n",
      "episode:   757 total reward:     -29.98 avg. training loss:     1.7979 explore P: 0.14437\n",
      "episode:   758 total reward:     -44.97 avg. training loss:     6.0407 explore P: 0.14435\n",
      "episode:   759 total reward:     -29.98 avg. training loss:     1.4323 explore P: 0.14434\n",
      "episode:   760 total reward:     -29.98 avg. training loss:     1.4078 explore P: 0.14432\n",
      "episode:   761 total reward:    -119.24 avg. training loss:    11.4315 explore P: 0.14427\n",
      "episode:   762 total reward:     -29.98 avg. training loss:    17.5475 explore P: 0.14426\n",
      "episode:   763 total reward:     -29.98 avg. training loss:     1.7992 explore P: 0.14424\n",
      "episode:   764 total reward:    -248.62 avg. training loss:     8.9320 explore P: 0.14413\n",
      "episode:   765 total reward:     -29.98 avg. training loss:   119.9043 explore P: 0.14412\n",
      "episode:   766 total reward:     -29.98 avg. training loss:     4.5049 explore P: 0.14410\n",
      "episode:   767 total reward:     -29.98 avg. training loss:     6.3669 explore P: 0.14409\n",
      "episode:   768 total reward:     -29.98 avg. training loss:    33.1159 explore P: 0.14408\n",
      "episode:   769 total reward:     -29.98 avg. training loss:     4.0177 explore P: 0.14406\n",
      "episode:   770 total reward:    -162.79 avg. training loss:     3.6646 explore P: 0.14399\n",
      ">>> target estimator network updated (40000)\n",
      "episode:   771 total reward:     -29.98 avg. training loss:     4.1614 explore P: 0.14398\n",
      "episode:   772 total reward:     -29.98 avg. training loss:     2.8031 explore P: 0.14396\n",
      "episode:   773 total reward:     -29.98 avg. training loss:     3.1729 explore P: 0.14395\n",
      "episode:   774 total reward:     -44.97 avg. training loss:     2.7872 explore P: 0.14393\n",
      "episode:   775 total reward:     -29.98 avg. training loss:     6.6290 explore P: 0.14391\n",
      "episode:   776 total reward:     -29.98 avg. training loss:     3.8252 explore P: 0.14390\n",
      "episode:   777 total reward:     -29.98 avg. training loss:     2.1246 explore P: 0.14389\n",
      "episode:   778 total reward:     -29.98 avg. training loss:     4.6825 explore P: 0.14387\n",
      "episode:   779 total reward:     -29.98 avg. training loss:     2.0505 explore P: 0.14386\n",
      "episode:   780 total reward:     -29.98 avg. training loss:     1.5431 explore P: 0.14385\n",
      "episode:   781 total reward:     -29.98 avg. training loss:     1.5520 explore P: 0.14383\n",
      "episode:   782 total reward:     -29.98 avg. training loss:   127.4336 explore P: 0.14382\n",
      "episode:   783 total reward:     -74.83 avg. training loss:    36.1599 explore P: 0.14379\n",
      "episode:   784 total reward:     -89.79 avg. training loss:     6.1605 explore P: 0.14375\n",
      "episode:   785 total reward:     -44.97 avg. training loss:     5.2111 explore P: 0.14373\n",
      "episode:   786 total reward:     -29.98 avg. training loss:   125.0695 explore P: 0.14371\n",
      "episode:   787 total reward:     -59.88 avg. training loss:    50.0527 explore P: 0.14369\n",
      "episode:   788 total reward:     -74.82 avg. training loss:     5.1146 explore P: 0.14365\n",
      "episode:   789 total reward:     -29.98 avg. training loss:   121.4874 explore P: 0.14364\n",
      "episode:   790 total reward:  -12433.73 avg. training loss:    31.3272 explore P: 0.14256\n",
      "episode:   791 total reward:     -44.96 avg. training loss:     2.1371 explore P: 0.14254\n",
      "episode:   792 total reward:     -59.88 avg. training loss:    12.7204 explore P: 0.14251\n",
      "episode:   793 total reward:     -29.98 avg. training loss:     1.3903 explore P: 0.14250\n",
      "episode:   794 total reward:     -29.98 avg. training loss:   112.6340 explore P: 0.14248\n",
      "episode:   795 total reward:     -29.98 avg. training loss:    67.9534 explore P: 0.14247\n",
      "episode:   796 total reward:     -29.98 avg. training loss:    93.0558 explore P: 0.14246\n",
      "episode:   797 total reward:     -29.98 avg. training loss:     5.7922 explore P: 0.14244\n",
      "episode:   798 total reward:     -74.90 avg. training loss:     9.1587 explore P: 0.14241\n",
      "episode:   799 total reward:     -29.98 avg. training loss:     2.9697 explore P: 0.14240\n",
      "episode:   800 total reward:     -29.98 avg. training loss:     3.5776 explore P: 0.14238\n",
      "episode:   801 total reward:     -29.98 avg. training loss:     4.4442 explore P: 0.14237\n",
      "episode:   802 total reward:     -29.98 avg. training loss:     2.1775 explore P: 0.14236\n",
      "episode:   803 total reward:     -29.98 avg. training loss:     3.8456 explore P: 0.14234\n",
      "episode:   804 total reward:     -29.98 avg. training loss:     1.5098 explore P: 0.14233\n",
      "episode:   805 total reward:     -29.99 avg. training loss:   128.2223 explore P: 0.14232\n",
      "episode:   806 total reward:     -44.97 avg. training loss:     7.3308 explore P: 0.14230\n",
      "episode:   807 total reward:     -44.95 avg. training loss:    71.5315 explore P: 0.14228\n",
      "episode:   808 total reward:     -29.98 avg. training loss:   110.1640 explore P: 0.14226\n",
      "episode:   809 total reward:    -522.47 avg. training loss:    37.5161 explore P: 0.14200\n",
      "episode:   810 total reward:     -29.98 avg. training loss:     1.8046 explore P: 0.14199\n",
      "episode:   811 total reward:     -29.98 avg. training loss:    21.0150 explore P: 0.14197\n",
      "episode:   812 total reward:     -29.98 avg. training loss:   228.3888 explore P: 0.14196\n",
      "episode:   813 total reward:     -29.98 avg. training loss:    16.6984 explore P: 0.14195\n",
      "episode:   814 total reward:     -29.98 avg. training loss:    12.4867 explore P: 0.14193\n",
      "episode:   815 total reward:     -29.98 avg. training loss:    94.0517 explore P: 0.14192\n",
      "episode:   816 total reward:    -494.86 avg. training loss:    50.4084 explore P: 0.14159\n",
      "episode:   817 total reward:     -29.98 avg. training loss:     4.3501 explore P: 0.14158\n",
      "episode:   818 total reward:     -44.97 avg. training loss:     6.7114 explore P: 0.14156\n",
      "episode:   819 total reward:     -29.98 avg. training loss:    15.8282 explore P: 0.14155\n",
      "episode:   820 total reward:     -29.98 avg. training loss:   141.5061 explore P: 0.14153\n",
      "episode:   821 total reward:    -408.65 avg. training loss:    24.8724 explore P: 0.14129\n",
      "episode:   822 total reward:     -29.98 avg. training loss:     8.2726 explore P: 0.14128\n",
      "episode:   823 total reward:     -29.98 avg. training loss:     6.3842 explore P: 0.14126\n",
      "episode:   824 total reward:     -59.86 avg. training loss:     2.8517 explore P: 0.14124\n",
      "episode:   825 total reward:     -29.98 avg. training loss:     2.9744 explore P: 0.14122\n",
      "episode:   826 total reward:     -74.78 avg. training loss:    27.2006 explore P: 0.14119\n",
      "episode:   827 total reward:     -29.98 avg. training loss:     8.8851 explore P: 0.14118\n",
      "episode:   828 total reward:     -29.98 avg. training loss:    95.1918 explore P: 0.14116\n",
      "episode:   829 total reward:     -29.98 avg. training loss:     1.3913 explore P: 0.14115\n",
      "episode:   830 total reward:     -29.98 avg. training loss:    10.5738 explore P: 0.14114\n",
      "episode:   831 total reward:     -29.98 avg. training loss:     9.6942 explore P: 0.14113\n",
      "episode:   832 total reward:     -29.98 avg. training loss:    10.5551 explore P: 0.14111\n",
      "episode:   833 total reward:     -29.98 avg. training loss:     1.7835 explore P: 0.14110\n",
      "episode:   834 total reward:     -29.98 avg. training loss:     1.3393 explore P: 0.14109\n",
      "episode:   835 total reward:     -29.98 avg. training loss:     1.7979 explore P: 0.14107\n",
      "episode:   836 total reward:     -29.98 avg. training loss:   200.5793 explore P: 0.14106\n",
      "episode:   837 total reward:     -29.98 avg. training loss:   119.8523 explore P: 0.14105\n",
      "episode:   838 total reward:     -29.98 avg. training loss:    86.8788 explore P: 0.14103\n",
      "episode:   839 total reward:     -29.98 avg. training loss:    35.9495 explore P: 0.14102\n",
      "episode:   840 total reward:     -29.98 avg. training loss:     5.4803 explore P: 0.14101\n",
      "episode:   841 total reward:     -29.98 avg. training loss:    13.2325 explore P: 0.14099\n",
      "episode:   842 total reward:     -29.98 avg. training loss:    10.8725 explore P: 0.14098\n",
      "episode:   843 total reward:     -29.98 avg. training loss:     8.4911 explore P: 0.14097\n",
      "episode:   844 total reward:     -29.98 avg. training loss:     7.9286 explore P: 0.14096\n",
      "episode:   845 total reward:     -29.98 avg. training loss:     6.7277 explore P: 0.14094\n",
      "episode:   846 total reward:     -29.98 avg. training loss:     4.2131 explore P: 0.14093\n",
      "episode:   847 total reward:     -29.98 avg. training loss:    42.0179 explore P: 0.14092\n",
      "episode:   848 total reward:     -29.98 avg. training loss:     3.3938 explore P: 0.14090\n",
      "episode:   849 total reward:     -29.98 avg. training loss:    70.4339 explore P: 0.14089\n",
      "episode:   850 total reward:     -29.98 avg. training loss:     4.1949 explore P: 0.14088\n",
      "episode:   851 total reward:     -59.86 avg. training loss:    43.6611 explore P: 0.14085\n",
      "episode:   852 total reward:     -29.98 avg. training loss:    90.9592 explore P: 0.14084\n",
      "episode:   853 total reward:     -59.90 avg. training loss:     1.8066 explore P: 0.14081\n",
      "episode:   854 total reward:     -59.86 avg. training loss:     2.0025 explore P: 0.14079\n",
      "episode:   855 total reward:     -29.98 avg. training loss:     6.4744 explore P: 0.14077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:   856 total reward:     -74.82 avg. training loss:    12.9744 explore P: 0.14074\n",
      "episode:   857 total reward:     -29.98 avg. training loss:     1.7495 explore P: 0.14073\n",
      "episode:   858 total reward:     -29.98 avg. training loss:     1.1549 explore P: 0.14071\n",
      "episode:   859 total reward:     -59.86 avg. training loss:   123.5940 explore P: 0.14069\n",
      "episode:   860 total reward:     -29.98 avg. training loss:   162.3439 explore P: 0.14067\n",
      "episode:   861 total reward:     -44.97 avg. training loss:     2.7402 explore P: 0.14065\n",
      "episode:   862 total reward:     -29.98 avg. training loss:     5.8474 explore P: 0.14064\n",
      "episode:   863 total reward:     -29.98 avg. training loss:     4.0900 explore P: 0.14063\n",
      "episode:   864 total reward:     -29.98 avg. training loss:     5.5353 explore P: 0.14062\n",
      "episode:   865 total reward:     -59.90 avg. training loss:    12.4628 explore P: 0.14059\n",
      "episode:   866 total reward:     -29.98 avg. training loss:     1.8363 explore P: 0.14058\n",
      "episode:   867 total reward:     -29.98 avg. training loss:     9.3793 explore P: 0.14056\n",
      "episode:   868 total reward:     -29.98 avg. training loss:     4.4146 explore P: 0.14055\n",
      "episode:   869 total reward:     -29.98 avg. training loss:     1.3186 explore P: 0.14054\n",
      "episode:   870 total reward:     -29.98 avg. training loss:     1.7234 explore P: 0.14052\n",
      "episode:   871 total reward:     -29.98 avg. training loss:     1.6735 explore P: 0.14051\n",
      "episode:   872 total reward:     -89.73 avg. training loss:    60.0986 explore P: 0.14047\n",
      "episode:   873 total reward:     -29.98 avg. training loss:   122.1726 explore P: 0.14046\n",
      "episode:   874 total reward:     -29.98 avg. training loss:     5.6592 explore P: 0.14045\n",
      "episode:   875 total reward:     -29.98 avg. training loss:     4.9422 explore P: 0.14043\n",
      "episode:   876 total reward:     -29.98 avg. training loss:     3.5755 explore P: 0.14042\n",
      "episode:   877 total reward:     -29.98 avg. training loss:     7.5125 explore P: 0.14041\n",
      "episode:   878 total reward:    -119.30 avg. training loss:     4.7676 explore P: 0.14035\n",
      "episode:   879 total reward:     -29.98 avg. training loss:     3.1657 explore P: 0.14034\n",
      "episode:   880 total reward:     -29.98 avg. training loss:     2.1358 explore P: 0.14033\n",
      "episode:   881 total reward:    -104.53 avg. training loss:    57.4323 explore P: 0.14028\n",
      "episode:   882 total reward:     -29.98 avg. training loss:    34.1787 explore P: 0.14027\n",
      "episode:   883 total reward:     -29.98 avg. training loss:     2.1372 explore P: 0.14026\n",
      "episode:   884 total reward:     -29.98 avg. training loss:     2.9443 explore P: 0.14024\n",
      "episode:   885 total reward:     -29.98 avg. training loss:    28.3716 explore P: 0.14023\n",
      "episode:   886 total reward:     -29.98 avg. training loss:   127.1553 explore P: 0.14022\n",
      "episode:   887 total reward:     -29.98 avg. training loss:    10.2002 explore P: 0.14020\n",
      "episode:   888 total reward:     -29.98 avg. training loss:     5.1682 explore P: 0.14019\n",
      "episode:   889 total reward:     -29.98 avg. training loss:    68.5750 explore P: 0.14018\n",
      "episode:   890 total reward:     -29.98 avg. training loss:     2.7213 explore P: 0.14017\n",
      "episode:   891 total reward:     -29.98 avg. training loss:     3.5290 explore P: 0.14015\n",
      "episode:   892 total reward:     -29.98 avg. training loss:    15.9005 explore P: 0.14014\n",
      "episode:   893 total reward:     -29.98 avg. training loss:     2.8648 explore P: 0.14013\n",
      "episode:   894 total reward:     -59.90 avg. training loss:    13.0063 explore P: 0.14010\n",
      "episode:   895 total reward:    -148.77 avg. training loss:    15.8039 explore P: 0.14004\n",
      "episode:   896 total reward:     -74.82 avg. training loss:    56.5472 explore P: 0.14000\n",
      "episode:   897 total reward:     -29.98 avg. training loss:   232.5951 explore P: 0.13999\n",
      "episode:   898 total reward:     -29.98 avg. training loss:     2.6843 explore P: 0.13998\n",
      "episode:   899 total reward:     -29.98 avg. training loss:     3.4564 explore P: 0.13996\n",
      "episode:   900 total reward:     -29.98 avg. training loss:   122.2671 explore P: 0.13995\n",
      "episode:   901 total reward:     -44.97 avg. training loss:     4.4198 explore P: 0.13993\n",
      "episode:   902 total reward:     -29.98 avg. training loss:    10.4609 explore P: 0.13992\n",
      "episode:   903 total reward:     -29.98 avg. training loss:     3.0553 explore P: 0.13991\n",
      "episode:   904 total reward:     -29.98 avg. training loss:    57.1174 explore P: 0.13989\n",
      "episode:   905 total reward:     -29.98 avg. training loss:     3.9585 explore P: 0.13988\n",
      "episode:   906 total reward:     -29.98 avg. training loss:     2.5207 explore P: 0.13987\n",
      "episode:   907 total reward:     -29.98 avg. training loss:     4.4509 explore P: 0.13985\n",
      "episode:   908 total reward:    -134.16 avg. training loss:     2.9251 explore P: 0.13979\n",
      "episode:   909 total reward:     -29.98 avg. training loss:     1.4223 explore P: 0.13978\n",
      "episode:   910 total reward:     -74.84 avg. training loss:    24.3050 explore P: 0.13975\n",
      "episode:   911 total reward:     -29.98 avg. training loss:     1.5730 explore P: 0.13974\n",
      "episode:   912 total reward:     -29.98 avg. training loss:    87.7574 explore P: 0.13972\n",
      "episode:   913 total reward:     -29.98 avg. training loss:     2.4157 explore P: 0.13971\n",
      "episode:   914 total reward:     -29.98 avg. training loss:    10.2362 explore P: 0.13970\n",
      "episode:   915 total reward:     -29.98 avg. training loss:    14.4489 explore P: 0.13968\n",
      "episode:   916 total reward:     -29.98 avg. training loss:     2.6928 explore P: 0.13967\n",
      "episode:   917 total reward:     -29.98 avg. training loss:     3.1649 explore P: 0.13966\n",
      "episode:   918 total reward:     -29.98 avg. training loss:     2.3720 explore P: 0.13965\n",
      "episode:   919 total reward:     -29.98 avg. training loss:     2.3138 explore P: 0.13963\n",
      "episode:   920 total reward:     -74.82 avg. training loss:    16.2698 explore P: 0.13960\n",
      "episode:   921 total reward:     -29.98 avg. training loss:     9.2686 explore P: 0.13959\n",
      "episode:   922 total reward:    -205.16 avg. training loss:     3.3629 explore P: 0.13950\n",
      "episode:   923 total reward:     -29.98 avg. training loss:    70.5086 explore P: 0.13948\n",
      "episode:   924 total reward:     -59.87 avg. training loss:    20.7538 explore P: 0.13946\n",
      "episode:   925 total reward:     -29.98 avg. training loss:     5.2135 explore P: 0.13944\n",
      "episode:   926 total reward:     -44.96 avg. training loss:    79.7811 explore P: 0.13943\n",
      "episode:   927 total reward:     -29.98 avg. training loss:    39.5420 explore P: 0.13941\n",
      "episode:   928 total reward:     -29.98 avg. training loss:    65.9589 explore P: 0.13940\n",
      "episode:   929 total reward:     -29.98 avg. training loss:    77.7620 explore P: 0.13939\n",
      "episode:   930 total reward:     -29.98 avg. training loss:     6.7746 explore P: 0.13937\n",
      "episode:   931 total reward:     -29.98 avg. training loss:     5.7141 explore P: 0.13936\n",
      "episode:   932 total reward:     -59.86 avg. training loss:     4.8565 explore P: 0.13933\n",
      "episode:   933 total reward:     -29.98 avg. training loss:     4.4907 explore P: 0.13932\n",
      "episode:   934 total reward:     -29.98 avg. training loss:     3.8019 explore P: 0.13931\n",
      "episode:   935 total reward:     -44.96 avg. training loss:     3.0217 explore P: 0.13929\n",
      "episode:   936 total reward:     -29.98 avg. training loss:    34.7950 explore P: 0.13928\n",
      "episode:   937 total reward:     -29.98 avg. training loss:     2.5424 explore P: 0.13926\n",
      "episode:   938 total reward:     -29.98 avg. training loss:   278.6851 explore P: 0.13925\n",
      "episode:   939 total reward:     -29.98 avg. training loss:    24.7808 explore P: 0.13924\n",
      "episode:   940 total reward:     -29.98 avg. training loss:     2.1099 explore P: 0.13922\n",
      "episode:   941 total reward:     -29.98 avg. training loss:   123.3257 explore P: 0.13921\n",
      "episode:   942 total reward:     -29.98 avg. training loss:   122.3031 explore P: 0.13920\n",
      "episode:   943 total reward:     -29.98 avg. training loss:     9.2053 explore P: 0.13919\n",
      "episode:   944 total reward:     -29.98 avg. training loss:    58.1354 explore P: 0.13917\n",
      "episode:   945 total reward:     -29.98 avg. training loss:     7.1436 explore P: 0.13916\n",
      "episode:   946 total reward:     -29.98 avg. training loss:     9.2777 explore P: 0.13915\n",
      "episode:   947 total reward:     -29.98 avg. training loss:     4.8223 explore P: 0.13913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:   948 total reward:   -1157.60 avg. training loss:    27.9630 explore P: 0.13863\n",
      ">>> target estimator network updated (41000)\n",
      "episode:   949 total reward:  -46287.23 avg. training loss:    24.6039 explore P: 0.13695\n",
      "episode:   950 total reward:     -29.98 avg. training loss:     6.6043 explore P: 0.13694\n",
      "episode:   951 total reward:     -29.98 avg. training loss:   131.9847 explore P: 0.13693\n",
      "episode:   952 total reward:     -29.98 avg. training loss:     9.4440 explore P: 0.13691\n",
      "episode:   953 total reward:     -44.97 avg. training loss:    11.9916 explore P: 0.13689\n",
      "episode:   954 total reward:     -29.98 avg. training loss:     5.1918 explore P: 0.13688\n",
      "episode:   955 total reward:     -29.98 avg. training loss:    53.7917 explore P: 0.13687\n",
      "episode:   956 total reward:     -29.98 avg. training loss:    84.4515 explore P: 0.13686\n",
      "episode:   957 total reward:     -29.98 avg. training loss:     3.7259 explore P: 0.13684\n",
      "episode:   958 total reward:     -29.98 avg. training loss:   160.6684 explore P: 0.13683\n",
      "episode:   959 total reward:     -29.98 avg. training loss:     7.1907 explore P: 0.13682\n",
      "episode:   960 total reward:     -29.98 avg. training loss:   221.9667 explore P: 0.13681\n",
      "episode:   961 total reward:    -147.97 avg. training loss:     7.4845 explore P: 0.13674\n",
      "episode:   962 total reward:     -29.98 avg. training loss:    93.9435 explore P: 0.13673\n",
      "episode:   963 total reward:     -29.98 avg. training loss:     9.4809 explore P: 0.13672\n",
      "episode:   964 total reward:     -29.98 avg. training loss:     4.5489 explore P: 0.13670\n",
      "episode:   965 total reward:     -29.98 avg. training loss:    78.9288 explore P: 0.13669\n",
      "episode:   966 total reward:     -44.96 avg. training loss:     3.3166 explore P: 0.13667\n",
      "episode:   967 total reward:  -53147.30 avg. training loss:    26.7564 explore P: 0.13491\n",
      "episode:   968 total reward:     -29.98 avg. training loss:     1.5548 explore P: 0.13489\n",
      "episode:   969 total reward:     -29.98 avg. training loss:     1.5417 explore P: 0.13488\n",
      "episode:   970 total reward:     -59.90 avg. training loss:     2.1076 explore P: 0.13486\n",
      "episode:   971 total reward:     -29.98 avg. training loss:     1.6183 explore P: 0.13484\n",
      "episode:   972 total reward:   -1423.41 avg. training loss:    43.0905 explore P: 0.13433\n",
      "episode:   973 total reward:    -377.65 avg. training loss:    23.3778 explore P: 0.13415\n",
      "episode:   974 total reward:     -29.98 avg. training loss:     3.5201 explore P: 0.13414\n",
      "episode:   975 total reward:     -29.98 avg. training loss:   116.9994 explore P: 0.13413\n",
      "episode:   976 total reward:     -29.98 avg. training loss:     4.3589 explore P: 0.13411\n",
      "episode:   977 total reward:     -29.98 avg. training loss:     7.5942 explore P: 0.13410\n",
      "episode:   978 total reward:     -29.98 avg. training loss:     8.6173 explore P: 0.13409\n",
      "episode:   979 total reward:     -29.98 avg. training loss:     5.9181 explore P: 0.13408\n",
      "episode:   980 total reward:     -29.98 avg. training loss:     4.5285 explore P: 0.13406\n",
      "episode:   981 total reward:     -29.98 avg. training loss:     2.6106 explore P: 0.13405\n",
      "episode:   982 total reward:     -29.98 avg. training loss:     4.2593 explore P: 0.13404\n",
      "episode:   983 total reward:    -163.97 avg. training loss:     2.8758 explore P: 0.13397\n",
      "episode:   984 total reward:     -59.87 avg. training loss:    63.0958 explore P: 0.13395\n",
      "episode:   985 total reward:     -29.98 avg. training loss:     4.4853 explore P: 0.13393\n",
      "episode:   986 total reward:     -29.98 avg. training loss:     3.6962 explore P: 0.13392\n",
      "episode:   987 total reward:     -29.98 avg. training loss:     2.6201 explore P: 0.13391\n",
      "episode:   988 total reward:    -385.57 avg. training loss:    16.0440 explore P: 0.13366\n",
      "episode:   989 total reward:     -29.98 avg. training loss:     2.0935 explore P: 0.13364\n",
      "episode:   990 total reward:     -29.98 avg. training loss:     2.0667 explore P: 0.13363\n",
      "episode:   991 total reward:     -29.99 avg. training loss:     3.2839 explore P: 0.13362\n",
      "episode:   992 total reward:     -29.98 avg. training loss:     5.9732 explore P: 0.13361\n",
      "episode:   993 total reward:     -29.98 avg. training loss:   131.8230 explore P: 0.13359\n",
      "episode:   994 total reward:     -29.98 avg. training loss:     2.7148 explore P: 0.13358\n",
      "episode:   995 total reward:     -29.98 avg. training loss:     4.1446 explore P: 0.13357\n",
      "episode:   996 total reward:     -29.98 avg. training loss:    13.4952 explore P: 0.13356\n",
      "episode:   997 total reward:     -29.98 avg. training loss:     2.6964 explore P: 0.13354\n",
      "episode:   998 total reward:     -29.98 avg. training loss:     3.5571 explore P: 0.13353\n",
      "episode:   999 total reward:     -29.98 avg. training loss:     1.9541 explore P: 0.13352\n",
      "episode:  1000 total reward:     -29.98 avg. training loss:     3.0304 explore P: 0.13351\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "\n",
    "batch_size = 200\n",
    "train_episodes = 1000\n",
    "\n",
    "labels = ['time', 'x', 'y', 'z', 'rotor_speed1', 'rotor_speed2', 'rotor_speed3', 'rotor_speed4']\n",
    "results = {x : [] for x in labels}\n",
    "\n",
    "rewards_list = []\n",
    "losses_list = []\n",
    "results_list = []\n",
    "loss = 0.\n",
    "for ep in range (1, train_episodes+1):\n",
    "    total_reward = 0.\n",
    "    step = 0\n",
    "    ep_loss = 0.\n",
    "    #1 - setting initial rotor speeds on high value helped to get the agent learning\n",
    "    rotor_speeds = np.full (4, (env.action_high - env.action_low)/2.)\n",
    "    env.reset ()\n",
    "    state, reward, done = env.step (rotor_speeds)\n",
    "    state = np.array ([1., state[14], env.sim.v[2], state[14] * env.sim.v[2]])\n",
    "    results = {x : [] for x in labels}\n",
    "    while True:        \n",
    "        # select action\n",
    "        action = agent.select_action (np.reshape(state, [1, 4]))        \n",
    "        if action == 1:\n",
    "            rotor_speeds = (rotor_speeds-200)\n",
    "        elif action == 8:\n",
    "            rotor_speeds = (rotor_speeds+200)\n",
    "        elif action == 2:\n",
    "            rotor_speeds = (rotor_speeds-100)\n",
    "        elif action == 9:\n",
    "            rotor_speeds = (rotor_speeds+100)\n",
    "        elif action == 3:\n",
    "            rotor_speeds = (rotor_speeds-50)\n",
    "        elif action == 10:\n",
    "            rotor_speeds = (rotor_speeds+50)\n",
    "        elif action == 4:\n",
    "            rotor_speeds = (rotor_speeds-20)\n",
    "        elif action == 11:\n",
    "            rotor_speeds = (rotor_speeds+20)\n",
    "        elif action == 5:\n",
    "            rotor_speeds = (rotor_speeds-10)\n",
    "        elif action == 12:\n",
    "            rotor_speeds = (rotor_speeds+10)\n",
    "        elif action == 6:\n",
    "            rotor_speeds = (rotor_speeds-5)\n",
    "        elif action == 13:\n",
    "            rotor_speeds = (rotor_speeds+5)\n",
    "        elif action == 7:\n",
    "            rotor_speeds = (rotor_speeds-1)\n",
    "        elif action == 14:\n",
    "            rotor_speeds = (rotor_speeds+1)\n",
    "        \n",
    "        if (rotor_speeds < env.action_low).all ():\n",
    "            rotor_speeds = np.full (4, env.action_low)\n",
    "        if (rotor_speeds > env.action_high).all ():\n",
    "            rotor_speeds = np.full (4, env.action_high)\n",
    "        \n",
    "        # Take action, get new state and reward\n",
    "        next_state, reward, done = env.step (rotor_speeds)\n",
    "        \n",
    "        total_reward += reward\n",
    "        \n",
    "        to_write = [env.sim.time] + list(env.sim.pose) + list(rotor_speeds)\n",
    "        for ii in range(len(labels)):\n",
    "            results[labels[ii]].append(to_write[ii])\n",
    "        \n",
    "        if done:\n",
    "            # the episode ends so no next state\n",
    "            next_state = np.zeros (state.shape)\n",
    "            rewards_list.append ((ep, total_reward))\n",
    "        else:\n",
    "            next_state = np.array ([1., next_state[14], env.sim.v[2], next_state[14] * env.sim.v[2]])\n",
    "            state = next_state\n",
    "        \n",
    "        # add experience to memory\n",
    "        agent.store ((state, action, reward, next_state))\n",
    "        \n",
    "        loss = agent.learn (batch_size)\n",
    "        losses_list.append (loss)\n",
    "        \n",
    "        step += 1\n",
    "        ep_loss += loss\n",
    "        if done:\n",
    "            ep_loss /= (step*1.)\n",
    "            print('episode: {:5d}'.format(ep),\n",
    "                  'total reward: {:10.2f}'.format(total_reward),\n",
    "                  'avg. training loss: {:10.4f}'.format(ep_loss),\n",
    "                  'explore P: {:.5f}'.format(agent.explore_p))\n",
    "            \n",
    "            break\n",
    "        \n",
    "    results_list.append (results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "ep_begin = 949\n",
    "\n",
    "fig, axs = plt.subplots (2, 1)\n",
    "\n",
    "\n",
    "axs[0].plot(results_list[(ep_begin-1)]['time'], results_list[(ep_begin-1)]['x'], label='x')\n",
    "axs[0].plot(results_list[(ep_begin-1)]['time'], results_list[(ep_begin-1)]['y'], label='y')\n",
    "axs[0].plot(results_list[(ep_begin-1)]['time'], results_list[(ep_begin-1)]['z'], label='z')\n",
    "axs[0].legend(loc='upper left')\n",
    "axs[0].set (title='position - episode {}'.format (ep_begin))\n",
    "\n",
    "axs[1].plot(results_list[(ep_begin-1)]['time'], results_list[(ep_begin-1)]['rotor_speed4'], label='rtsp')\n",
    "axs[1].legend(loc='upper left')\n",
    "axs[1].set (title='rotor speeds - episode {}'.format (ep_begin))\n",
    "\n",
    "_ = plt.ylim()\n",
    "fig.set_size_inches ((12., 8.), forward=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig, axs = plt.subplots ()\n",
    "\n",
    "axs.plot(list (range (0, len (losses_list))), losses_list, label='loss')\n",
    "axs.legend(loc='upper left')\n",
    "axs.set (title='loss - all learn cycles')\n",
    "\n",
    "_ = plt.ylim()\n",
    "fig.set_size_inches ((10., 6.), forward=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[250. 250. 250. 250.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[ 0.  0. 10.]\n",
      "[ 0.          0.         -0.36801508]\n",
      "[0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print (rotor_speeds)\n",
    "print (env.sim.pose)\n",
    "print (env.target_pos)\n",
    "print(env.sim.v)\n",
    "print(env.sim.angular_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Plot the Rewards\n",
    "\n",
    "Once you are satisfied with your performance, plot the episode rewards, either from a single run, or averaged over multiple runs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Total Reward')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## TODO: Plot the rewards.\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N\n",
    "\n",
    "eps, rews = np.array(rewards_list).T\n",
    "smoothed_rews = running_mean(rews, 10)\n",
    "plt.plot(eps[-len(smoothed_rews):], smoothed_rews)\n",
    "plt.plot(eps, rews, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Reflections\n",
    "\n",
    "**Question 1**: Describe the task that you specified in `task.py`.  How did you design the reward function?\n",
    "\n",
    "**Answer**:\n",
    "\n",
    "The task in my <a href='#My-Environment'>environment</a> is to reach a target position of [x,y,z]=[0,0,10]. The initial values for the quadcopter's position, velocity and angles are always 0.\n",
    "\n",
    "Therefor the quadcopter's rotors all get the same speed from the agent, to ensure movement up and down only.\n",
    "\n",
    "The reward function consists of three values:\n",
    "- flying time: the longer the higher rewarded (contributes 20% to total reward)\n",
    "  - reason: the agent shall learn to stay as long as possible in an episode\n",
    "- distance to target position: the closer, the higher rewarded (contributes 50% to total reward)\n",
    "  - reason: since it is the goal in the given environment\n",
    "- change in velocity: the higher the change, the lower rewarded (contributes 30% of total reward)\n",
    "  - reason: after many trials I found out, that the position belonging to a given rotor speed is delayed (see picture below) and therefor I tried to smooth the actions and penalize high changes in velocity\n",
    "\n",
    "<img src='.\\stuff\\delay-act-pos.png' width=50% align=left />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2**: Discuss your agent briefly, using the following questions as a guide:\n",
    "\n",
    "- What learning algorithm(s) did you try? What worked best for you?\n",
    "- What was your final choice of hyperparameters (such as $\\alpha$, $\\gamma$, $\\epsilon$, etc.)?\n",
    "- What neural network architecture did you use (if any)? Specify layers, sizes, activation functions, etc.\n",
    "\n",
    "**Answer**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choice of learning algorithm\n",
    "\n",
    "First I examined the environment with help of the definition of a Markov Decision Process:\n",
    "\n",
    "In general a MDP consits of\n",
    "- a Model (defined by transitions T)\n",
    "- a finite State space S\n",
    "- a finite Action space A\n",
    "- Rewards R\n",
    "\n",
    "RL algorithms can be divided into\n",
    "- model-based, algorithms need: T and R\n",
    "- value-based, model-free, algorithms need: Q\n",
    "- policy-based\n",
    "\n",
    "Basically, all RL algorithms work well with discrete spaces (States, Actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quadcopter environment has the following:\n",
    "##### State space S\n",
    "Currently, every state holds the 6 degrees of freedom\n",
    "- 3 translational, position (x, y, z) and\n",
    "- 3 rotational, Euler angles ($\\varphi, \\vartheta, \\psi$)\n",
    "\n",
    "The state space is large and values are continuous, but has boundaries given by the physics simulation. Exceeding the boundaries or exceeding the runtime leads to termination of the episode. By that the task is episodic.\n",
    "\n",
    "Furthermore, we need to use function approximation to enable the agent to choose correct actions and minimize the computation time.\n",
    "\n",
    "Action repeats are used, [...] in order to make the problems approximately fully observable in the high dimensional environment [...]. For each timestep of the agent, we step the simulation 3 timesteps, repeating the agent’s action and rendering each time (arXiv:1509.02971, p. 5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of a state (3 action repeats * 6 dof): 18\n",
      "S lower boundary [-150. -150.    0.]\n",
      "S upper boundary [150. 150. 300.]\n"
     ]
    }
   ],
   "source": [
    "# State space S\n",
    "task = Task ()\n",
    "\n",
    "print ('Size of a state (3 action repeats * 6 dof):', task.state_size)\n",
    "\n",
    "print ('S lower boundary', task.sim.lower_bounds)\n",
    "print ('S upper boundary', task.sim.upper_bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Action space A\n",
    "One action consits of 4 independent values, representing the thrust / rotor speeds.\n",
    "\n",
    "The action space values are continuous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of an action: 4\n",
      "A boundaries: (0, 900)\n"
     ]
    }
   ],
   "source": [
    "print ('Size of an action:', task.action_size)\n",
    "print ('A boundaries:', (task.action_low, task.action_high))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reward function R\n",
    "The base reward function given by class Task is\n",
    "- R = 1 if target positon reached\n",
    "- else: R = 1 - 30% of absolute difference between actual positon and target position\n",
    "\n",
    "Thus, the reward will be a high negative scalar, if the quadcopter is far away from the target position\n",
    "\n",
    "The reward function was changed to better overcome the own defined goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quadcopter has 4 motors\n",
    "<pre>\n",
    "       front\n",
    "     (1)   (2)\n",
    "        \\ /\n",
    "right    o     right\n",
    "        / \\\n",
    "     (3)   (4)\n",
    "        rear\n",
    "</pre>\n",
    "Motors (1) and (4) rotate in dir1, Motors (2) and (3) rotate in dir2\n",
    "\n",
    "- pitch +: increase speed of (3) and (4)\n",
    "- pitch -: increase speed of (1) and (2)\n",
    "- roll +: increase speed of (1) and (3)\n",
    "- roll -: increase speed of (2) and (4)\n",
    "- yaw +: increase speed of (2) and (3)\n",
    "- yaw -: increase speed of (1) and (4)\n",
    "\n",
    "#### Approach\n",
    "With above information, I chose the following approach:\n",
    "- continuous S: function approximation via neural network\n",
    "- continuous A -> is going to be discretized in (+-200, +-100, +-50, +-20, +-10, +-5, +-1)\n",
    "- Q-Learning\n",
    "  - off-policy method: better overcomes exploration-exploitation dilemma\n",
    "  - supports batch learning (for experience replay)\n",
    "- experience replay, to decouple agent-environment activities from agent-learn activities\n",
    "- additional Q Target network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture of the agent\n",
    "The generalized architecture was built upon the post of <a href='https://jaromiru.com/2016/10/03/lets-make-a-dqn-implementation/'>jaromiru</a>, who specified the agent with a brain and a memory acting in an environment.\n",
    "\n",
    "<img src='.\\stuff\\dqn.png' width=70% align=left>\n",
    "\n",
    "The Agent consist of 2 Brains: one MLP (see architecture below) and one Taget Estimator network (Q Target).\n",
    "\n",
    "Furthermore the Agent has a Memory to hold the last 20000 samples of the environment it is interacting with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The architecture of the Brain and the Agent's hyperparameter:\n",
    "\n",
    "<img src='.\\stuff\\agent_architecture.png' width=60% align=left>\n",
    "\n",
    "The neural network is a standard MLP with:\n",
    "- input layer with tensor = (,4) (see below why 4-dimensional)\n",
    "- ouput layer with tensor = (,15) and linear activation\n",
    "- 4 hidden layer with activation ReLU\n",
    "- last hidden layer contains an L2 regularizer with $\\lambda$=0.07\n",
    "\n",
    "The target estimator will be updated every 1000 learn cycles. The source weights (the Brain itself) will be accounted with 12.5% ($\\tau$=0.125)\n",
    "\n",
    "discount rate: $\\gamma$ = 0.9 - since the Agent shall be more farsighted in its judgements\n",
    "\n",
    "learning rate: $\\alpha$ = 0.001 - shorter gradient descent steps worked better, but learning was slower\n",
    "\n",
    "exploration rate: $\\epsilon§ begins at 1.0, is minimum 0.01 and decays exponential with 5e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### feature vector x(s)\n",
    "Why is the input layer 4-dimensional?\n",
    "\n",
    "I used an approach of Sutton's book \"Reinforcement Learning: An Introduction\". As of section 9.5 Feature Construction for Linear Methods, one way to build the feature vector is with polynomials, to *\"[...] take into account any interactions between these dimensions.\"* (p. 170).\n",
    "\n",
    "This is why my input tensor consists of:\n",
    "- 1.0 (*\"[...] allows the representation of aﬃne functions in the original state numbers [...]\"* (p. 170))\n",
    "- position z on z axis\n",
    "- velocity v_z in z axis\n",
    "- product z * v_z\n",
    "\n",
    "In fact, this helped the Brain to learn better (compared to original 3 positions x,y,z).\n",
    "\n",
    "#### batch size\n",
    "The batch size for each learn cycle is 300. Higher batch sizes seemed to work better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3**: Using the episode rewards plot, discuss how the agent learned over time.\n",
    "\n",
    "- Was it an easy task to learn or hard?\n",
    "- Was there a gradual learning curve, or an aha moment?\n",
    "- How good was the final performance of the agent? (e.g. mean rewards over the last 10 episodes)\n",
    "\n",
    "**Answer**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To teach an agent this task is hard to me, since the results show somehow no learning progress.\n",
    "The first 200 episodes the agent learned something, but then suddenly all episodes ended within a part of a second and I couldn't find out why. There is no proof detectable, that the episode ended due to being too high or too low - this is strange to me. (By the way: I had the same issue in the DQN section with cartpole mini project, but I think this was related to the Memory size)\n",
    "\n",
    "Even with <a href='https://keras.io/getting-started/faq/#how-can-i-obtain-reproducible-results-using-keras-during-development'> preparing the environment as given by Keras documentation</a> I was not able to find out the reasons.\n",
    "\n",
    "The rewards as printed above can't be evaluated in this case. Yes, there is an increase in the rewards over progressing episodes, but this increase comes from the sudden episode endings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4**: Briefly summarize your experience working on this project. You can use the following prompts for ideas.\n",
    "\n",
    "- What was the hardest part of the project? (e.g. getting started, plotting, specifying the task, etc.)\n",
    "- Did you find anything interesting in how the quadcopter or your agent behaved?\n",
    "\n",
    "**Answer**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole project was challenging in many ways:\n",
    "- How does the environment work? - I need to know what the system is doing, where boundaries are, what states and actions I can take\n",
    "- How shall I shape the reward function w.r.t. the goal of the task? - By far the hardest part, since without good knowledge about the behavior of the system, wrong rewarding leads to wrong learning\n",
    "- What kind of learning algorithm shall I take? - It takes a while to figure out what RL algorithm works best\n",
    "- How do I verify, that the agent is learning? - Look at rewards or at loss function of the MLP?\n",
    "\n",
    "In fact I tried many many hours (better say weeks) to figure out how the agent shall learn the given task and it still excites me. It is an evolutional process for me solving the task. But on the other hand I got demotivated since progress was slow compared to time given solving this project. Nevertheless I feel confident to solve it one day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
