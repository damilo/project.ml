{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Train a Quadcopter How to Fly\n",
    "\n",
    "Design an agent to fly a quadcopter, and then train it using a reinforcement learning algorithm of your choice! \n",
    "\n",
    "Try to apply the techniques you have learnt, but also feel free to come up with innovative ideas and test them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "Take a look at the files in the directory to better understand the structure of the project. \n",
    "\n",
    "- `task.py`: Define your task (environment) in this file.\n",
    "- `agents/`: Folder containing reinforcement learning agents.\n",
    "    - `policy_search.py`: A sample agent has been provided here.\n",
    "    - `agent.py`: Develop your agent here.\n",
    "- `physics_sim.py`: This file contains the simulator for the quadcopter.  **DO NOT MODIFY THIS FILE**.\n",
    "\n",
    "For this project, you will define your own task in `task.py`.  Although we have provided a example task to get you started, you are encouraged to change it.  Later in this notebook, you will learn more about how to amend this file.\n",
    "\n",
    "You will also design a reinforcement learning agent in `agent.py` to complete your chosen task.  \n",
    "\n",
    "You are welcome to create any additional files to help you to organize your code.  For instance, you may find it useful to define a `model.py` file defining any needed neural network architectures.\n",
    "\n",
    "## Controlling the Quadcopter\n",
    "\n",
    "We provide a sample agent in the code cell below to show you how to use the sim to control the quadcopter.  This agent is even simpler than the sample agent that you'll examine (in `agents/policy_search.py`) later in this notebook!\n",
    "\n",
    "The agent controls the quadcopter by setting the revolutions per second on each of its four rotors.  The provided agent in the `Basic_Agent` class below always selects a random action for each of the four rotors.  These four speeds are returned by the `act` method as a list of four floating-point numbers.  \n",
    "\n",
    "For this project, the agent that you will implement in `agents/agent.py` will have a far more intelligent method for selecting actions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Basic_Agent():\n",
    "    def __init__(self, task):\n",
    "        self.task = task\n",
    "    \n",
    "    def act(self):\n",
    "        new_thrust = random.gauss(450., 25.)\n",
    "        return [new_thrust + random.gauss(0., 1.) for x in range(4)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code cell below to have the agent select actions to control the quadcopter.  \n",
    "\n",
    "Feel free to change the provided values of `runtime`, `init_pose`, `init_velocities`, and `init_angle_velocities` below to change the starting conditions of the quadcopter.\n",
    "\n",
    "The `labels` list below annotates statistics that are saved while running the simulation.  All of this information is saved in a text file `data.txt` and stored in the dictionary `results`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "from task import Task\n",
    "\n",
    "# Modify the values below to give the quadcopter a different starting position.\n",
    "runtime = 5.                                     # time limit of the episode\n",
    "init_pose = np.array([0., 0., 10., 0., 0., 0.])  # initial pose # [x, y, z, phi, theta, psi]\n",
    "init_velocities = np.array([0., 0., 0.])         # initial velocities # x_velocity, y_velocity, z_velocity\n",
    "init_angle_velocities = np.array([0., 0., 0.])   # initial angle velocities # phi_velocity, theta_velocity, psi_velocity\n",
    "file_output = 'data.txt'                         # file name for saved results\n",
    "\n",
    "# Setup\n",
    "task = Task(init_pose, init_velocities, init_angle_velocities, runtime)\n",
    "agent = Basic_Agent(task)\n",
    "done = False\n",
    "labels = ['time', 'x', 'y', 'z', 'phi', 'theta', 'psi', 'x_velocity',\n",
    "          'y_velocity', 'z_velocity', 'phi_velocity', 'theta_velocity',\n",
    "          'psi_velocity', 'rotor_speed1', 'rotor_speed2', 'rotor_speed3', 'rotor_speed4']\n",
    "results = {x : [] for x in labels}\n",
    "\n",
    "# Run the simulation, and save the results.\n",
    "with open(file_output, 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(labels)\n",
    "    \n",
    "    # reset environment\n",
    "    initial_state = task.reset ()\n",
    "    print ('initial state: {} {} {}'.format (initial_state[:6], initial_state[6:12], initial_state[12:]))\n",
    "    \n",
    "    while True:\n",
    "        rotor_speeds = agent.act()\n",
    "        _, _, done = task.step(rotor_speeds)\n",
    "        to_write = [task.sim.time] + list(task.sim.pose) + list(task.sim.v) + list(task.sim.angular_v) + list(rotor_speeds)\n",
    "        for ii in range(len(labels)):\n",
    "            results[labels[ii]].append(to_write[ii])\n",
    "        writer.writerow(to_write)\n",
    "        \n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code cell below to visualize how the position of the quadcopter evolved during the simulation. Next, you can plot the Euler angles (the rotation of the quadcopter over the $x$-, $y$-, and $z$-axes),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig, axs = plt.subplots (1, 2)\n",
    "\n",
    "axs[0].plot(results['time'], results['x'], label='x')\n",
    "axs[0].plot(results['time'], results['y'], label='y')\n",
    "axs[0].plot(results['time'], results['z'], label='z')\n",
    "axs[0].legend(loc='upper left')\n",
    "axs[0].set (title='position')\n",
    "\n",
    "axs[1].plot(results['time'], results['phi'], label='phi')\n",
    "axs[1].plot(results['time'], results['theta'], label='theta')\n",
    "axs[1].plot(results['time'], results['psi'], label='psi')\n",
    "axs[1].legend(loc='upper left')\n",
    "axs[1].set (title='Euler angles')\n",
    "\n",
    "_ = plt.ylim()\n",
    "fig.set_size_inches ((14., 6.), forward=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code cell visualizes the velocity of the quadcopter. before plotting the velocities (in radians per second) corresponding to each of the Euler angles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots (1, 2)\n",
    "\n",
    "axs[0].plot(results['time'], results['x_velocity'], label='x_hat')\n",
    "axs[0].plot(results['time'], results['y_velocity'], label='y_hat')\n",
    "axs[0].plot(results['time'], results['z_velocity'], label='z_hat')\n",
    "axs[0].legend(loc='upper left')\n",
    "axs[0].set (title='velocity')\n",
    "\n",
    "axs[1].plot(results['time'], results['phi_velocity'], label='phi_velocity')\n",
    "axs[1].plot(results['time'], results['theta_velocity'], label='theta_velocity')\n",
    "axs[1].plot(results['time'], results['psi_velocity'], label='psi_velocity')\n",
    "axs[1].legend(loc='upper left')\n",
    "axs[1].set (title='velocity in Euler angles')\n",
    "\n",
    "_ = plt.ylim()\n",
    "fig.set_size_inches ((14., 6.), forward=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can use the code cell below to print the agent's choice of actions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(results['time'], results['rotor_speed1'], label='Rotor 1 revolutions / second')\n",
    "plt.plot(results['time'], results['rotor_speed2'], label='Rotor 2 revolutions / second')\n",
    "plt.plot(results['time'], results['rotor_speed3'], label='Rotor 3 revolutions / second')\n",
    "plt.plot(results['time'], results['rotor_speed4'], label='Rotor 4 revolutions / second')\n",
    "plt.legend()\n",
    "_ = plt.ylim()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When specifying a task, you will derive the environment state from the simulator.  Run the code cell below to print the values of the following variables at the end of the simulation:\n",
    "- `task.sim.pose` (the position of the quadcopter in ($x,y,z$) dimensions and the Euler angles),\n",
    "- `task.sim.v` (the velocity of the quadcopter in ($x,y,z$) dimensions), and\n",
    "- `task.sim.angular_v` (radians/second for each of the three Euler angles)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the pose, velocity, and angular velocity of the quadcopter at the end of the episode\n",
    "print(task.sim.pose)\n",
    "print(task.sim.v)\n",
    "print(task.sim.angular_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the sample task in `task.py`, we use the 6-dimensional pose of the quadcopter to construct the state of the environment at each timestep.  However, when amending the task for your purposes, you are welcome to expand the size of the state vector by including the velocity information.  You can use any combination of the pose, velocity, and angular velocity - feel free to tinker here, and construct the state to suit your task.\n",
    "\n",
    "## The Task\n",
    "\n",
    "A sample task has been provided for you in `task.py`.  Open this file in a new window now. \n",
    "\n",
    "The `__init__()` method is used to initialize several variables that are needed to specify the task.  \n",
    "- The simulator is initialized as an instance of the `PhysicsSim` class (from `physics_sim.py`).  \n",
    "- Inspired by the methodology in the original DDPG paper, we make use of action repeats.  For each timestep of the agent, we step the simulation `action_repeats` timesteps.  If you are not familiar with action repeats, please read the **Results** section in [the DDPG paper](https://arxiv.org/abs/1509.02971).\n",
    "- We set the number of elements in the state vector.  For the sample task, we only work with the 6-dimensional pose information.  To set the size of the state (`state_size`), we must take action repeats into account.  \n",
    "- The environment will always have a 4-dimensional action space, with one entry for each rotor (`action_size=4`). You can set the minimum (`action_low`) and maximum (`action_high`) values of each entry here.\n",
    "- The sample task in this provided file is for the agent to reach a target position.  We specify that target position as a variable.\n",
    "\n",
    "The `reset()` method resets the simulator.  The agent should call this method every time the episode ends.  You can see an example of this in the code cell below.\n",
    "\n",
    "The `step()` method is perhaps the most important.  It accepts the agent's choice of action `rotor_speeds`, which is used to prepare the next state to pass on to the agent.  Then, the reward is computed from `get_reward()`.  The episode is considered done if the time limit has been exceeded, or the quadcopter has travelled outside of the bounds of the simulation.\n",
    "\n",
    "In the next section, you will learn how to test the performance of an agent on this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Agent\n",
    "\n",
    "The sample agent given in `agents/policy_search.py` uses a very simplistic linear policy to directly compute the action vector as a dot product of the state vector and a matrix of weights. Then, it randomly perturbs the parameters by adding some Gaussian noise, to produce a different policy. Based on the average reward obtained in each episode (`score`), it keeps track of the best set of parameters found so far, how the score is changing, and accordingly tweaks a scaling factor to widen or tighten the noise.\n",
    "\n",
    "Run the code cell below to see how the agent performs on the sample task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "from agents.policy_search import PolicySearch_Agent\n",
    "from task import Task\n",
    "\n",
    "num_episodes = 1000\n",
    "target_pos = np.array([0., 0., 10.])\n",
    "task = Task(target_pos=target_pos)\n",
    "agent = PolicySearch_Agent(task) \n",
    "\n",
    "for i_episode in range(1, num_episodes+1):\n",
    "    state = agent.reset_episode() # start a new episode\n",
    "    while True:\n",
    "        action = agent.act(state) \n",
    "        next_state, reward, done = task.step(action)\n",
    "        agent.step(reward, done)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            print(\"\\rEpisode = {:4d}, score = {:7.3f} (best = {:7.3f}), noise_scale = {}\".format(\n",
    "                i_episode, agent.score, agent.best_score, agent.noise_scale), end=\"\")  # [debug]\n",
    "            break\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This agent should perform very poorly on this task.  And that's where you come in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Define the Task, Design the Agent, and Train Your Agent!\n",
    "\n",
    "Amend `task.py` to specify a task of your choosing.  If you're unsure what kind of task to specify, you may like to teach your quadcopter to takeoff, hover in place, land softly, or reach a target pose.  \n",
    "\n",
    "After specifying your task, use the sample agent in `agents/policy_search.py` as a template to define your own agent in `agents/agent.py`.  You can borrow whatever you need from the sample agent, including ideas on how you might modularize your code (using helper methods like `act()`, `learn()`, `reset_episode()`, etc.).\n",
    "\n",
    "Note that it is **highly unlikely** that the first agent and task that you specify will learn well.  You will likely have to tweak various hyperparameters and the reward function for your task until you arrive at reasonably good behavior.\n",
    "\n",
    "As you develop your agent, it's important to keep an eye on how it's performing. Use the code above as inspiration to build in a mechanism to log/save the total rewards obtained in each episode to file.  If the episode rewards are gradually increasing, this is an indication that your agent is learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### RL system - General\n",
    "<img src=\".\\stuff\\rl.system.gen.PNG\" width=\"50%\" \\>\n",
    "\n",
    "In general a MDP consits of\n",
    "- a Model (defined by transitions T)\n",
    "- a finite State space S\n",
    "- a finite Action space A\n",
    "- Rewards R\n",
    "\n",
    "RL algorithms can be divided into\n",
    "- model-based, algorithms need: T and R\n",
    "- value-based, model-free, algorithms need: Q\n",
    "- policy-based\n",
    "\n",
    "Basically, all RL algorithms work well with discrete spaces (States, Actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quadcopter\n",
    "- episodic task\n",
    "- Goal: reach a target position - Pos (x, y, z) = (0, 0, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### State space S\n",
    "Currently, every state holds the 6 degrees of freedom\n",
    "- 3 translational, position (x, y, z) and\n",
    "- 3 rotational, Euler angles ($\\varphi, \\vartheta, \\psi$)\n",
    "\n",
    "The state space is large and values are continuous, but has boundaries given by the physics simulation. Exceeding the boundaries or exceeding the runtime leads to termination of the episode.\n",
    "\n",
    "By that, we need to use function approximation to enable the agent to choose correct actions and minimize the computation time.\n",
    "\n",
    "Action repeats are used, [...] in order to make the problems approximately fully observable in the high dimensional environment [...]. For each timestep of the agent, we step the simulation 3 timesteps, repeating the agentâ€™s action and rendering each time (arXiv:1509.02971, p. 5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = Task ()\n",
    "\n",
    "print ('Size of a state (3 action repeats * 6 dof):', task.state_size)\n",
    "\n",
    "print ('S lower boundary', task.sim.lower_bounds)\n",
    "print ('S upper boundary', task.sim.upper_bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Action space A\n",
    "One action consits of 4 independent values, representing the thrust / rotor speeds.\n",
    "\n",
    "The action space values are continuous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Size of an action:', task.action_size)\n",
    "print ('A boundaries:', (task.action_low, task.action_high))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reward function R\n",
    "Currently, the reward function is\n",
    "- R = 1 if target positon reached\n",
    "- else: R = 1 - 30% of absolute difference between actual positon and target position\n",
    "\n",
    "Thus, the reward will be a high negative scalar, if the quadcopter is far away from the target position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My Environment\n",
    "The environment is derived by the given class 'Task'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import task\n",
    "\n",
    "class MyTask (Task):\n",
    "    \n",
    "    def __init__ (self):\n",
    "        Task.__init__ (self)\n",
    "        \n",
    "        # Goal\n",
    "        self.target_pos = np.array ([0., 0., 10.])\n",
    "    \n",
    "    # reward function (overwrite)\n",
    "    def get_reward (self):\n",
    "        return super ().get_reward ()\n",
    "    \n",
    "\n",
    "env = MyTask ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My Agent\n",
    "The agent is defined in <a href=\".\\agents\\agent.py\">agent.py</a>.\n",
    "\n",
    "This agent discretizes the actions:\n",
    "- 3 actions: -1 = 'decrease_speed', 0 = 'no_action', 1 = 'increase_speed'\n",
    "- if boundaries of action space exceeded, then 'no_action' is taken\n",
    "\n",
    "Actions upon cases:\n",
    "Quadcopter has 4 motors\n",
    "<pre>\n",
    "       front\n",
    "     (1)   (2)\n",
    "        \\ /\n",
    "right    o     right\n",
    "        / \\\n",
    "     (3)   (4)\n",
    "        rear\n",
    "</pre>\n",
    "Motors (1) and (4) rotate in dir1, Motors (2) and (3) rotate in dir2\n",
    "\n",
    "- pitch +: increase speed of (3) and (4)\n",
    "- pitch -: increase speed of (1) and (2)\n",
    "- roll +: increase speed of (1) and (3)\n",
    "- roll -: increase speed of (2) and (4)\n",
    "- yaw +: increase speed of (2) and (3)\n",
    "- yaw -: increase speed of (1) and (4)\n",
    "\n",
    "#### Approach\n",
    "- continuous S\n",
    "- continuous A\n",
    "- Q-Learning\n",
    "  - off-policy method: better overcomes exploration-exploitation dilemma\n",
    "  - supports batch learning (for experience replay)\n",
    "- option: experience replay\n",
    "- option: function approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DQN\n",
    "import tensorflow as tf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Train your agent here.\n",
    "# first try with discrete actions\n",
    "from agents.agent import Agent\n",
    "\n",
    "num_episodes = 1000\n",
    "env = MyTask ()\n",
    "agent = Agent ()\n",
    "\n",
    "\"\"\"\n",
    "for i_episode in range(1, num_episodes+1):\n",
    "    state = agent.reset_episode() # start a new episode\n",
    "    while True:\n",
    "        action = agent.act(state) \n",
    "        next_state, reward, done = task.step(action)\n",
    "        agent.step(reward, done)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            print(\"\\rEpisode = {:4d}, score = {:7.3f} (best = {:7.3f}), noise_scale = {}\".format(\n",
    "                i_episode, agent.score, agent.best_score, agent.noise_scale), end=\"\")  # [debug]\n",
    "            break\n",
    "    sys.stdout.flush()\n",
    "\"\"\"\n",
    "\n",
    "for i_episode in range (1, num_episodes+1):\n",
    "    # begin the episode\n",
    "    state = env.reset ()\n",
    "    print (state)\n",
    "\n",
    "    while True:\n",
    "        # agent selects an action\n",
    "        action = agent.select_action (state)\n",
    "        # agent performs the selected action\n",
    "        next_state, reward, done, _ = env.step (action)\n",
    "\n",
    "        # agent performs internal updates based on sampled experience\n",
    "        agent.step (state, action, reward, next_state, done)\n",
    "\n",
    "        # update the state (s <- s') to next time step\n",
    "        state = next_state\n",
    "        if done:\n",
    "            print(\"\\rEpisode = {:4d}, score = {:7.3f} (best = {:7.3f}), noise_scale = {}\".format(\n",
    "            i_episode, agent.score, agent.best_score, agent.noise_scale), end=\"\")  # [debug]\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Plot the Rewards\n",
    "\n",
    "Once you are satisfied with your performance, plot the episode rewards, either from a single run, or averaged over multiple runs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Plot the rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Reflections\n",
    "\n",
    "**Question 1**: Describe the task that you specified in `task.py`.  How did you design the reward function?\n",
    "\n",
    "**Answer**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2**: Discuss your agent briefly, using the following questions as a guide:\n",
    "\n",
    "- What learning algorithm(s) did you try? What worked best for you?\n",
    "- What was your final choice of hyperparameters (such as $\\alpha$, $\\gamma$, $\\epsilon$, etc.)?\n",
    "- What neural network architecture did you use (if any)? Specify layers, sizes, activation functions, etc.\n",
    "\n",
    "**Answer**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3**: Using the episode rewards plot, discuss how the agent learned over time.\n",
    "\n",
    "- Was it an easy task to learn or hard?\n",
    "- Was there a gradual learning curve, or an aha moment?\n",
    "- How good was the final performance of the agent? (e.g. mean rewards over the last 10 episodes)\n",
    "\n",
    "**Answer**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4**: Briefly summarize your experience working on this project. You can use the following prompts for ideas.\n",
    "\n",
    "- What was the hardest part of the project? (e.g. getting started, plotting, specifying the task, etc.)\n",
    "- Did you find anything interesting in how the quadcopter or your agent behaved?\n",
    "\n",
    "**Answer**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "# Create the Cart-Pole game environment\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import agent\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "tf.reset_default_graph()\n",
    "ag = agent.Agentv2 (state_size=4, action_size=2)\n",
    "\n",
    "# Make a bunch of random actions and store the experiences\n",
    "# Start new episode\n",
    "env.reset()\n",
    "# Take one random step to get the pole and cart moving\n",
    "state, reward, done, _ = env.step(env.action_space.sample())\n",
    "for ii in range(20):\n",
    "\n",
    "    # Make a random action\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "    if done:\n",
    "        # The simulation fails so no next state\n",
    "        next_state = np.zeros(state.shape)\n",
    "        # Add experience to memory\n",
    "        ag.store ((state, action, reward, next_state))\n",
    "\n",
    "        # Start new episode\n",
    "        env.reset()\n",
    "        # Take one random step to get the pole and cart moving\n",
    "        state, reward, done, _ = env.step(env.action_space.sample())\n",
    "    else:\n",
    "        # Add experience to memory\n",
    "        ag.store ((state, action, reward, next_state))\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras import optimizers\n",
    "\n",
    "dqn = agent.MLPQNet (4, 2, 0.001)\n",
    "dqn.print_architecture ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample mini-batch from memory\n",
    "batch = ag._memory.sample (5)\n",
    "states = np.array ([each[0] for each in batch])\n",
    "actions = np.array ([each[1] for each in batch])\n",
    "rewards = np.array ([each[2] for each in batch])\n",
    "next_states = np.array ([each[3] for each in batch])\n",
    "\n",
    "next_states[3] = np.zeros (states[0].shape)\n",
    "print (next_states.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target values - the values we want to become\n",
    "p_sa_ = dqn.Net.predict (next_states)\n",
    "# set target values to 0 for states where episode ends\n",
    "episode_ends = (next_states == np.zeros (states[0].shape)).all (axis=1)\n",
    "p_sa_[episode_ends] = np.zeros (ag.action_size)\n",
    "print (p_sa_)\n",
    "target_vals = rewards + 0.7 * np.max (p_sa_, axis=1)\n",
    "print ('target', target_vals)\n",
    "\n",
    "\n",
    "# get approximate values\n",
    "approx_vals = dqn.Net.predict (states)\n",
    "print ('approx', approx_vals)\n",
    "\n",
    "# update approximate value for taken action to target value\n",
    "print (actions)\n",
    "for i in range (approx_vals.shape[0]):\n",
    "    approx_vals[i,actions[i]] = target_vals[i]\n",
    "\n",
    "print (approx_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target values - the values we want to become\n",
    "p_sa_ = dqn.Net.predict (next_states)\n",
    "# set target values to 0 for states where episode ends\n",
    "episode_ends = (next_states == np.zeros (states[0].shape)).all (axis=1)\n",
    "p_sa_[episode_ends] = np.zeros (ag.action_size)\n",
    "print (p_sa_)\n",
    "target_vals = rewards + 0.7 * np.max (p_sa_, axis=1)\n",
    "print ('target', target_vals)\n",
    "\n",
    "\n",
    "# get approximate values\n",
    "approx_vals = dqn.Net.predict (states)\n",
    "print ('approx', approx_vals)\n",
    "\n",
    "# update approximate value for taken action to target value\n",
    "print (actions)\n",
    "for i in range (approx_vals.shape[0]):\n",
    "    approx_vals[i,actions[i]] = target_vals[i]\n",
    "\n",
    "print ('new targets', approx_vals)\n",
    "\n",
    "for state, action, reward, next_state in batch:\n",
    "    # if done, make our target reward\n",
    "    target = reward\n",
    "    if not (next_state == np.zeros (states[0].shape)).all ():\n",
    "        # predict the future discounted reward\n",
    "        p_sa_ = dqn.Net.predict(np.reshape(next_state, [1, 4]))\n",
    "        print ('action probs ns', p_sa_)\n",
    "        target = reward + 0.7 * np.max (p_sa_)\n",
    "        print ('target', target)\n",
    "        # make the agent to approximately map\n",
    "        # the current state to future discounted reward\n",
    "        # We'll call that target_f\n",
    "        target_f = dqn.Net.predict(np.reshape(state, [1, 4]))\n",
    "        print ('action probs s', target_f)\n",
    "        target_f[0][action] = target\n",
    "        print ('new target', target_f)\n",
    "        # Train the Neural Net with the state and target_f\n",
    "        #dqn.Net.fit(state, target_f, epochs=1, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_episodes = 1000\n",
    "max_steps = 200\n",
    "batch_size = 20\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    #saver = tf.train.Saver ()\n",
    "    sess.run (tf.global_variables_initializer())\n",
    "    \n",
    "    # Make a bunch of random actions and store the experiences\n",
    "    # Start new episode\n",
    "    env.reset()\n",
    "    # Take one random step to get the pole and cart moving\n",
    "    state, reward, done, _ = env.step(env.action_space.sample())\n",
    "    for ii in range(20):\n",
    "\n",
    "        # Make a random action\n",
    "        action = env.action_space.sample()\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        if done:\n",
    "            # The simulation fails so no next state\n",
    "            next_state = np.zeros(state.shape)\n",
    "            # Add experience to memory\n",
    "            ag.store ((state, action, reward, next_state))\n",
    "\n",
    "            # Start new episode\n",
    "            env.reset()\n",
    "            # Take one random step to get the pole and cart moving\n",
    "            state, reward, done, _ = env.step(env.action_space.sample())\n",
    "        else:\n",
    "            # Add experience to memory\n",
    "            ag.store ((state, action, reward, next_state))\n",
    "            state = next_state\n",
    "\n",
    "    \n",
    "    # GPI with DQN\n",
    "    env.reset ()\n",
    "    state, reward, done, _ = env.step (env.action_space.sample())\n",
    "    \n",
    "    step = 0\n",
    "    rewards_list = []\n",
    "    for ep in range (1, train_episodes):\n",
    "        total_reward = 0\n",
    "        t = 0\n",
    "        while t < max_steps:\n",
    "            step += 1\n",
    "            # Uncomment this next line to watch the training\n",
    "            # env.render ()\n",
    "\n",
    "            # Explore or Exploit\n",
    "            action = ag.act (state, step, sess)\n",
    "\n",
    "            # Take action, get new state and reward\n",
    "            next_state, reward, done, _ = env.step (action)\n",
    "\n",
    "            total_reward += reward\n",
    "\n",
    "            if done:\n",
    "                # the episode ends so no next state\n",
    "                next_state = np.zeros (state.shape)\n",
    "                t = max_steps\n",
    "\n",
    "                print('Episode: {}'.format(ep),\n",
    "                      'Total reward: {}'.format(total_reward),\n",
    "                      'Training loss: {:.4f}'.format(loss),\n",
    "                      'Explore P: {:.4f}'.format(ag.explore_p))\n",
    "\n",
    "                rewards_list.append((ep, total_reward))\n",
    "\n",
    "                # Add experience to memory\n",
    "                ag.store ((state, action, reward, next_state))\n",
    "\n",
    "                # Start new episode\n",
    "                env.reset ()\n",
    "                # Take one random step to get the pole and cart moving\n",
    "                state, reward, done, _ = env.step(env.action_space.sample())\n",
    "\n",
    "            else:\n",
    "                # Add experience to memory\n",
    "                ag.store ((state, action, reward, next_state))\n",
    "                state = next_state\n",
    "                t += 1\n",
    "\n",
    "            loss = ag.learn (batch_size, sess)\n",
    "    \n",
    "    #timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    #saver.save (sess, \"agent.brain_\"+timestr+\".ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N\n",
    "\n",
    "eps, rews = np.array(rewards_list).T\n",
    "smoothed_rews = running_mean(rews, 10)\n",
    "plt.plot(eps[-len(smoothed_rews):], smoothed_rews)\n",
    "plt.plot(eps, rews, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "# Create the Cart-Pole game environment\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1 Total reward: 16.0 Training loss: [0.4688514769077301] Explore P: 0.9984\n",
      "Episode: 2 Total reward: 61.0 Training loss: [0.39209699630737305] Explore P: 0.9924\n",
      "Episode: 3 Total reward: 37.0 Training loss: [0.47647225856781006] Explore P: 0.9888\n",
      "Episode: 4 Total reward: 19.0 Training loss: [0.5083566308021545] Explore P: 0.9869\n",
      "Episode: 5 Total reward: 12.0 Training loss: [0.4448961019515991] Explore P: 0.9857\n",
      "Episode: 6 Total reward: 12.0 Training loss: [0.39392074942588806] Explore P: 0.9846\n",
      "Episode: 7 Total reward: 9.0 Training loss: [0.47412946820259094] Explore P: 0.9837\n",
      "Episode: 8 Total reward: 22.0 Training loss: [0.48176196217536926] Explore P: 0.9816\n",
      "Episode: 9 Total reward: 11.0 Training loss: [0.48420068621635437] Explore P: 0.9805\n",
      "Episode: 10 Total reward: 10.0 Training loss: [0.45781558752059937] Explore P: 0.9795\n",
      "Episode: 11 Total reward: 26.0 Training loss: [0.3893066644668579] Explore P: 0.9770\n",
      "Episode: 12 Total reward: 42.0 Training loss: [0.45747628808021545] Explore P: 0.9730\n",
      "Episode: 13 Total reward: 13.0 Training loss: [0.4827476143836975] Explore P: 0.9717\n",
      "Episode: 14 Total reward: 31.0 Training loss: [0.404748797416687] Explore P: 0.9687\n",
      "Episode: 15 Total reward: 15.0 Training loss: [0.5893325209617615] Explore P: 0.9673\n",
      "Episode: 16 Total reward: 20.0 Training loss: [0.4204089045524597] Explore P: 0.9654\n",
      "Episode: 17 Total reward: 14.0 Training loss: [0.4159718155860901] Explore P: 0.9640\n",
      "Episode: 18 Total reward: 20.0 Training loss: [0.47670528292655945] Explore P: 0.9621\n",
      "Episode: 19 Total reward: 25.0 Training loss: [0.3852350115776062] Explore P: 0.9598\n",
      "Episode: 20 Total reward: 22.0 Training loss: [0.4253619909286499] Explore P: 0.9577\n",
      "Episode: 21 Total reward: 61.0 Training loss: [0.4059516489505768] Explore P: 0.9519\n",
      "Episode: 22 Total reward: 11.0 Training loss: [0.40115562081336975] Explore P: 0.9509\n",
      "Episode: 23 Total reward: 76.0 Training loss: [0.3961905241012573] Explore P: 0.9437\n",
      "Episode: 24 Total reward: 17.0 Training loss: [0.4660579562187195] Explore P: 0.9422\n",
      "Episode: 25 Total reward: 9.0 Training loss: [0.4028869569301605] Explore P: 0.9413\n",
      "Episode: 26 Total reward: 11.0 Training loss: [0.4009656310081482] Explore P: 0.9403\n",
      "Episode: 27 Total reward: 16.0 Training loss: [0.6033883094787598] Explore P: 0.9388\n",
      "Episode: 28 Total reward: 33.0 Training loss: [0.40148526430130005] Explore P: 0.9358\n",
      "Episode: 29 Total reward: 11.0 Training loss: [0.4010234773159027] Explore P: 0.9347\n",
      "Episode: 30 Total reward: 11.0 Training loss: [0.45844346284866333] Explore P: 0.9337\n",
      "Episode: 31 Total reward: 21.0 Training loss: [0.39805445075035095] Explore P: 0.9318\n",
      "Episode: 32 Total reward: 14.0 Training loss: [0.43012961745262146] Explore P: 0.9305\n",
      "Episode: 33 Total reward: 24.0 Training loss: [0.42555075883865356] Explore P: 0.9283\n",
      "Episode: 34 Total reward: 12.0 Training loss: [0.4466473460197449] Explore P: 0.9272\n",
      "Episode: 35 Total reward: 8.0 Training loss: [0.4647998511791229] Explore P: 0.9264\n",
      "Episode: 36 Total reward: 15.0 Training loss: [0.5565563440322876] Explore P: 0.9251\n",
      "Episode: 37 Total reward: 46.0 Training loss: [0.5862021446228027] Explore P: 0.9209\n",
      "Episode: 38 Total reward: 14.0 Training loss: [0.3958541452884674] Explore P: 0.9196\n",
      "Episode: 39 Total reward: 12.0 Training loss: [0.4430321455001831] Explore P: 0.9185\n",
      "Episode: 40 Total reward: 22.0 Training loss: [0.40127745270729065] Explore P: 0.9165\n",
      "Episode: 41 Total reward: 12.0 Training loss: [0.39898738265037537] Explore P: 0.9154\n",
      "Episode: 42 Total reward: 22.0 Training loss: [0.3992111086845398] Explore P: 0.9134\n",
      "Episode: 43 Total reward: 17.0 Training loss: [0.39748215675354004] Explore P: 0.9119\n",
      "Episode: 44 Total reward: 29.0 Training loss: [0.39491796493530273] Explore P: 0.9093\n",
      "Episode: 45 Total reward: 21.0 Training loss: [0.37089163064956665] Explore P: 0.9074\n",
      "Episode: 46 Total reward: 27.0 Training loss: [0.41273385286331177] Explore P: 0.9050\n",
      "Episode: 47 Total reward: 39.0 Training loss: [0.41089174151420593] Explore P: 0.9015\n",
      "Episode: 48 Total reward: 34.0 Training loss: [0.46684324741363525] Explore P: 0.8985\n",
      "Episode: 49 Total reward: 17.0 Training loss: [0.39557719230651855] Explore P: 0.8970\n",
      "Episode: 50 Total reward: 15.0 Training loss: [0.39557603001594543] Explore P: 0.8956\n",
      "Episode: 51 Total reward: 13.0 Training loss: [0.36731767654418945] Explore P: 0.8945\n",
      "Episode: 52 Total reward: 20.0 Training loss: [0.4128825068473816] Explore P: 0.8927\n",
      "Episode: 53 Total reward: 21.0 Training loss: [0.3758111596107483] Explore P: 0.8909\n",
      "Episode: 54 Total reward: 46.0 Training loss: [0.396320104598999] Explore P: 0.8868\n",
      "Episode: 55 Total reward: 17.0 Training loss: [0.38732630014419556] Explore P: 0.8853\n",
      "Episode: 56 Total reward: 10.0 Training loss: [0.0775822326540947] Explore P: 0.8845\n",
      "Episode: 57 Total reward: 18.0 Training loss: [0.45437508821487427] Explore P: 0.8829\n",
      "Episode: 58 Total reward: 20.0 Training loss: [0.383235365152359] Explore P: 0.8811\n",
      "Episode: 59 Total reward: 34.0 Training loss: [0.41319841146469116] Explore P: 0.8782\n",
      "Episode: 60 Total reward: 16.0 Training loss: [0.3945126235485077] Explore P: 0.8768\n",
      "Episode: 61 Total reward: 65.0 Training loss: [0.391821026802063] Explore P: 0.8712\n",
      "Episode: 62 Total reward: 16.0 Training loss: [0.4279812276363373] Explore P: 0.8698\n",
      "Episode: 63 Total reward: 16.0 Training loss: [0.359575092792511] Explore P: 0.8684\n",
      "Episode: 64 Total reward: 11.0 Training loss: [0.39346519112586975] Explore P: 0.8675\n",
      "Episode: 65 Total reward: 17.0 Training loss: [0.4116215705871582] Explore P: 0.8660\n",
      "Episode: 66 Total reward: 16.0 Training loss: [0.35946834087371826] Explore P: 0.8647\n",
      "Episode: 67 Total reward: 12.0 Training loss: [0.5179842114448547] Explore P: 0.8636\n",
      "Episode: 68 Total reward: 19.0 Training loss: [0.4706560969352722] Explore P: 0.8620\n",
      "Episode: 69 Total reward: 22.0 Training loss: [0.38866662979125977] Explore P: 0.8601\n",
      "Episode: 70 Total reward: 16.0 Training loss: [0.3785202205181122] Explore P: 0.8588\n",
      "Episode: 71 Total reward: 34.0 Training loss: [0.357102632522583] Explore P: 0.8559\n",
      "Episode: 72 Total reward: 20.0 Training loss: [0.412150114774704] Explore P: 0.8542\n",
      "Episode: 73 Total reward: 10.0 Training loss: [0.38054460287094116] Explore P: 0.8534\n",
      "Episode: 74 Total reward: 12.0 Training loss: [0.4842473864555359] Explore P: 0.8524\n",
      "Episode: 75 Total reward: 19.0 Training loss: [0.3875247538089752] Explore P: 0.8508\n",
      "Episode: 76 Total reward: 14.0 Training loss: [0.3535800576210022] Explore P: 0.8496\n",
      "Episode: 77 Total reward: 13.0 Training loss: [0.4099469780921936] Explore P: 0.8485\n",
      "Episode: 78 Total reward: 20.0 Training loss: [0.38873010873794556] Explore P: 0.8468\n",
      "Episode: 79 Total reward: 14.0 Training loss: [0.3810674250125885] Explore P: 0.8456\n",
      "Episode: 80 Total reward: 21.0 Training loss: [0.056233000010252] Explore P: 0.8439\n",
      "Episode: 81 Total reward: 29.0 Training loss: [0.4837159216403961] Explore P: 0.8415\n",
      "Episode: 82 Total reward: 90.0 Training loss: [0.36868205666542053] Explore P: 0.8340\n",
      "Episode: 83 Total reward: 42.0 Training loss: [0.3500659167766571] Explore P: 0.8306\n",
      "Episode: 84 Total reward: 23.0 Training loss: [0.3503643870353699] Explore P: 0.8287\n",
      "Episode: 85 Total reward: 16.0 Training loss: [0.36717572808265686] Explore P: 0.8274\n",
      "Episode: 86 Total reward: 11.0 Training loss: [0.34929096698760986] Explore P: 0.8265\n",
      "Episode: 87 Total reward: 25.0 Training loss: [0.377772718667984] Explore P: 0.8244\n",
      "Episode: 88 Total reward: 18.0 Training loss: [0.4089316725730896] Explore P: 0.8230\n",
      "Episode: 89 Total reward: 21.0 Training loss: [0.4207625091075897] Explore P: 0.8213\n",
      "Episode: 90 Total reward: 14.0 Training loss: [0.4121164083480835] Explore P: 0.8201\n",
      "Episode: 91 Total reward: 15.0 Training loss: [0.4064231216907501] Explore P: 0.8189\n",
      "Episode: 92 Total reward: 26.0 Training loss: [0.406434565782547] Explore P: 0.8168\n",
      "Episode: 93 Total reward: 15.0 Training loss: [0.3740638792514801] Explore P: 0.8156\n",
      "Episode: 94 Total reward: 56.0 Training loss: [0.3442411422729492] Explore P: 0.8111\n",
      "Episode: 95 Total reward: 31.0 Training loss: [0.34438568353652954] Explore P: 0.8086\n",
      "Episode: 96 Total reward: 57.0 Training loss: [0.40612873435020447] Explore P: 0.8041\n",
      "Episode: 97 Total reward: 32.0 Training loss: [0.371024489402771] Explore P: 0.8016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 98 Total reward: 15.0 Training loss: [0.40715810656547546] Explore P: 0.8004\n",
      "Episode: 99 Total reward: 15.0 Training loss: [0.41753560304641724] Explore P: 0.7992\n",
      "Episode: 100 Total reward: 29.0 Training loss: [0.3715587258338928] Explore P: 0.7969\n",
      "Episode: 101 Total reward: 19.0 Training loss: [0.3689105808734894] Explore P: 0.7954\n",
      "Episode: 102 Total reward: 19.0 Training loss: [0.40496885776519775] Explore P: 0.7939\n",
      "Episode: 103 Total reward: 16.0 Training loss: [0.3429243862628937] Explore P: 0.7927\n",
      "Episode: 104 Total reward: 52.0 Training loss: [0.41208159923553467] Explore P: 0.7886\n",
      "Episode: 105 Total reward: 8.0 Training loss: [0.3405519127845764] Explore P: 0.7880\n",
      "Episode: 106 Total reward: 10.0 Training loss: [0.3619441092014313] Explore P: 0.7872\n",
      "Episode: 107 Total reward: 8.0 Training loss: [0.4001903831958771] Explore P: 0.7866\n",
      "Episode: 108 Total reward: 30.0 Training loss: [0.42109522223472595] Explore P: 0.7843\n",
      "Episode: 109 Total reward: 10.0 Training loss: [0.3392685055732727] Explore P: 0.7835\n",
      "Episode: 110 Total reward: 9.0 Training loss: [0.4053174555301666] Explore P: 0.7828\n",
      "Episode: 111 Total reward: 12.0 Training loss: [0.4115995466709137] Explore P: 0.7819\n",
      "Episode: 112 Total reward: 48.0 Training loss: [0.4417296350002289] Explore P: 0.7782\n",
      "Episode: 113 Total reward: 23.0 Training loss: [0.3687118887901306] Explore P: 0.7764\n",
      "Episode: 114 Total reward: 12.0 Training loss: [0.358735054731369] Explore P: 0.7755\n",
      "Episode: 115 Total reward: 11.0 Training loss: [0.478863000869751] Explore P: 0.7746\n",
      "Episode: 116 Total reward: 27.0 Training loss: [0.4293352961540222] Explore P: 0.7726\n",
      "Episode: 117 Total reward: 27.0 Training loss: [0.4437086880207062] Explore P: 0.7705\n",
      "Episode: 118 Total reward: 19.0 Training loss: [0.36702051758766174] Explore P: 0.7691\n",
      "Episode: 119 Total reward: 19.0 Training loss: [0.42300575971603394] Explore P: 0.7676\n",
      "Episode: 120 Total reward: 11.0 Training loss: [0.36561113595962524] Explore P: 0.7668\n",
      "Episode: 121 Total reward: 26.0 Training loss: [0.3626308739185333] Explore P: 0.7648\n",
      "Episode: 122 Total reward: 19.0 Training loss: [0.4138326048851013] Explore P: 0.7634\n",
      "Episode: 123 Total reward: 11.0 Training loss: [0.366655558347702] Explore P: 0.7626\n",
      "Episode: 124 Total reward: 15.0 Training loss: [0.3939909040927887] Explore P: 0.7615\n",
      "Episode: 125 Total reward: 10.0 Training loss: [0.3666326105594635] Explore P: 0.7607\n",
      "Episode: 126 Total reward: 16.0 Training loss: [0.36220821738243103] Explore P: 0.7595\n",
      "Episode: 127 Total reward: 21.0 Training loss: [0.33599480986595154] Explore P: 0.7579\n",
      "Episode: 128 Total reward: 9.0 Training loss: [0.40556085109710693] Explore P: 0.7573\n",
      "Episode: 129 Total reward: 16.0 Training loss: [0.41883397102355957] Explore P: 0.7561\n",
      "Episode: 130 Total reward: 16.0 Training loss: [0.3618861436843872] Explore P: 0.7549\n",
      "Episode: 131 Total reward: 12.0 Training loss: [0.3615976572036743] Explore P: 0.7540\n",
      "Episode: 132 Total reward: 20.0 Training loss: [0.39769309759140015] Explore P: 0.7525\n",
      "Episode: 133 Total reward: 25.0 Training loss: [0.36210620403289795] Explore P: 0.7506\n",
      "Episode: 134 Total reward: 16.0 Training loss: [0.3605071008205414] Explore P: 0.7494\n",
      "Episode: 135 Total reward: 17.0 Training loss: [0.3614836633205414] Explore P: 0.7482\n",
      "Episode: 136 Total reward: 8.0 Training loss: [0.39598146080970764] Explore P: 0.7476\n",
      "Episode: 137 Total reward: 15.0 Training loss: [0.07294393330812454] Explore P: 0.7465\n",
      "Episode: 138 Total reward: 25.0 Training loss: [0.4127463698387146] Explore P: 0.7447\n",
      "Episode: 139 Total reward: 12.0 Training loss: [0.36226871609687805] Explore P: 0.7438\n",
      "Episode: 140 Total reward: 14.0 Training loss: [0.362681120634079] Explore P: 0.7428\n",
      "Episode: 141 Total reward: 19.0 Training loss: [0.35095638036727905] Explore P: 0.7414\n",
      "Episode: 142 Total reward: 10.0 Training loss: [0.43223699927330017] Explore P: 0.7406\n",
      "Episode: 143 Total reward: 11.0 Training loss: [0.3621443212032318] Explore P: 0.7398\n",
      "Episode: 144 Total reward: 14.0 Training loss: [0.35789528489112854] Explore P: 0.7388\n",
      "Episode: 145 Total reward: 25.0 Training loss: [0.3299235701560974] Explore P: 0.7370\n",
      "Episode: 146 Total reward: 14.0 Training loss: [0.399231493473053] Explore P: 0.7360\n",
      "Episode: 147 Total reward: 17.0 Training loss: [0.3607449233531952] Explore P: 0.7347\n",
      "Episode: 148 Total reward: 14.0 Training loss: [0.3590603172779083] Explore P: 0.7337\n",
      "Episode: 149 Total reward: 17.0 Training loss: [0.3596358597278595] Explore P: 0.7325\n",
      "Episode: 150 Total reward: 21.0 Training loss: [0.35777920484542847] Explore P: 0.7310\n",
      "Episode: 151 Total reward: 15.0 Training loss: [0.39734718203544617] Explore P: 0.7299\n",
      "Episode: 152 Total reward: 12.0 Training loss: [0.3999224305152893] Explore P: 0.7290\n",
      "Episode: 153 Total reward: 8.0 Training loss: [0.34150373935699463] Explore P: 0.7285\n",
      "Episode: 154 Total reward: 16.0 Training loss: [0.35397064685821533] Explore P: 0.7273\n",
      "Episode: 155 Total reward: 17.0 Training loss: [0.39726153016090393] Explore P: 0.7261\n",
      "Episode: 156 Total reward: 17.0 Training loss: [0.39474013447761536] Explore P: 0.7249\n",
      "Episode: 157 Total reward: 23.0 Training loss: [0.3269761800765991] Explore P: 0.7232\n",
      "Episode: 158 Total reward: 21.0 Training loss: [0.34405893087387085] Explore P: 0.7217\n",
      "Episode: 159 Total reward: 12.0 Training loss: [0.34461963176727295] Explore P: 0.7209\n",
      "Episode: 160 Total reward: 11.0 Training loss: [0.4086114466190338] Explore P: 0.7201\n",
      "Episode: 161 Total reward: 10.0 Training loss: [0.35352960228919983] Explore P: 0.7194\n",
      "Episode: 162 Total reward: 15.0 Training loss: [0.34610694646835327] Explore P: 0.7183\n",
      "Episode: 163 Total reward: 10.0 Training loss: [0.35574573278427124] Explore P: 0.7176\n",
      "Episode: 164 Total reward: 22.0 Training loss: [0.05013350769877434] Explore P: 0.7161\n",
      "Episode: 165 Total reward: 23.0 Training loss: [0.3926388919353485] Explore P: 0.7144\n",
      "Episode: 166 Total reward: 17.0 Training loss: [0.4019790291786194] Explore P: 0.7132\n",
      "Episode: 167 Total reward: 20.0 Training loss: [0.357269287109375] Explore P: 0.7118\n",
      "Episode: 168 Total reward: 17.0 Training loss: [0.3889563977718353] Explore P: 0.7106\n",
      "Episode: 169 Total reward: 15.0 Training loss: [0.35291430354118347] Explore P: 0.7096\n",
      "Episode: 170 Total reward: 21.0 Training loss: [0.3496567904949188] Explore P: 0.7081\n",
      "Episode: 171 Total reward: 19.0 Training loss: [0.40562722086906433] Explore P: 0.7068\n",
      "Episode: 172 Total reward: 15.0 Training loss: [0.3299982249736786] Explore P: 0.7058\n",
      "Episode: 173 Total reward: 12.0 Training loss: [0.3581444323062897] Explore P: 0.7049\n",
      "Episode: 174 Total reward: 13.0 Training loss: [0.4151415526866913] Explore P: 0.7040\n",
      "Episode: 175 Total reward: 20.0 Training loss: [0.3266848623752594] Explore P: 0.7026\n",
      "Episode: 176 Total reward: 20.0 Training loss: [0.35622644424438477] Explore P: 0.7013\n",
      "Episode: 177 Total reward: 13.0 Training loss: [0.35123369097709656] Explore P: 0.7004\n",
      "Episode: 178 Total reward: 10.0 Training loss: [0.4668729305267334] Explore P: 0.6997\n",
      "Episode: 179 Total reward: 16.0 Training loss: [0.3477630913257599] Explore P: 0.6986\n",
      "Episode: 180 Total reward: 24.0 Training loss: [0.34877097606658936] Explore P: 0.6969\n",
      "Episode: 181 Total reward: 11.0 Training loss: [0.34875407814979553] Explore P: 0.6962\n",
      "Episode: 182 Total reward: 16.0 Training loss: [0.40227726101875305] Explore P: 0.6951\n",
      "Episode: 183 Total reward: 20.0 Training loss: [0.40947553515434265] Explore P: 0.6937\n",
      "Episode: 184 Total reward: 15.0 Training loss: [0.35118240118026733] Explore P: 0.6927\n",
      "Episode: 185 Total reward: 12.0 Training loss: [0.3495239019393921] Explore P: 0.6918\n",
      "Episode: 186 Total reward: 19.0 Training loss: [0.34855958819389343] Explore P: 0.6906\n",
      "Episode: 187 Total reward: 11.0 Training loss: [0.4192042052745819] Explore P: 0.6898\n",
      "Episode: 188 Total reward: 26.0 Training loss: [0.3997842073440552] Explore P: 0.6880\n",
      "Episode: 189 Total reward: 9.0 Training loss: [0.3550707995891571] Explore P: 0.6874\n",
      "Episode: 190 Total reward: 20.0 Training loss: [0.38672366738319397] Explore P: 0.6861\n",
      "Episode: 191 Total reward: 10.0 Training loss: [0.3943099081516266] Explore P: 0.6854\n",
      "Episode: 192 Total reward: 13.0 Training loss: [0.397310346364975] Explore P: 0.6845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 193 Total reward: 24.0 Training loss: [0.3987644910812378] Explore P: 0.6829\n",
      "Episode: 194 Total reward: 25.0 Training loss: [0.34927046298980713] Explore P: 0.6812\n",
      "Episode: 195 Total reward: 10.0 Training loss: [0.3276810050010681] Explore P: 0.6806\n",
      "Episode: 196 Total reward: 25.0 Training loss: [0.32336530089378357] Explore P: 0.6789\n",
      "Episode: 197 Total reward: 25.0 Training loss: [0.4382794499397278] Explore P: 0.6772\n",
      "Episode: 198 Total reward: 13.0 Training loss: [0.3519129753112793] Explore P: 0.6763\n",
      "Episode: 199 Total reward: 14.0 Training loss: [0.3936276435852051] Explore P: 0.6754\n",
      "Episode: 200 Total reward: 22.0 Training loss: [0.35049647092819214] Explore P: 0.6739\n",
      "Episode: 201 Total reward: 15.0 Training loss: [0.3883589506149292] Explore P: 0.6730\n",
      "Episode: 202 Total reward: 8.0 Training loss: [0.3456515669822693] Explore P: 0.6724\n",
      "Episode: 203 Total reward: 12.0 Training loss: [0.3439328670501709] Explore P: 0.6716\n",
      "Episode: 204 Total reward: 14.0 Training loss: [0.32493600249290466] Explore P: 0.6707\n",
      "Episode: 205 Total reward: 18.0 Training loss: [0.34218189120292664] Explore P: 0.6695\n",
      "Episode: 206 Total reward: 39.0 Training loss: [0.32506680488586426] Explore P: 0.6669\n",
      "Episode: 207 Total reward: 16.0 Training loss: [0.34316182136535645] Explore P: 0.6659\n",
      "Episode: 208 Total reward: 17.0 Training loss: [0.013441594317555428] Explore P: 0.6648\n",
      "Episode: 209 Total reward: 9.0 Training loss: [0.32246047258377075] Explore P: 0.6642\n",
      "Episode: 210 Total reward: 18.0 Training loss: [0.40048453211784363] Explore P: 0.6630\n",
      "Episode: 211 Total reward: 25.0 Training loss: [0.34261733293533325] Explore P: 0.6614\n",
      "Episode: 212 Total reward: 26.0 Training loss: [0.388429194688797] Explore P: 0.6597\n",
      "Episode: 213 Total reward: 15.0 Training loss: [0.32222458720207214] Explore P: 0.6587\n",
      "Episode: 214 Total reward: 15.0 Training loss: [0.34767165780067444] Explore P: 0.6578\n",
      "Episode: 215 Total reward: 11.0 Training loss: [0.34031328558921814] Explore P: 0.6570\n",
      "Episode: 216 Total reward: 14.0 Training loss: [0.32289788126945496] Explore P: 0.6561\n",
      "Episode: 217 Total reward: 10.0 Training loss: [0.34500759840011597] Explore P: 0.6555\n",
      "Episode: 218 Total reward: 15.0 Training loss: [0.40078970789909363] Explore P: 0.6545\n",
      "Episode: 219 Total reward: 9.0 Training loss: [0.32215616106987] Explore P: 0.6539\n",
      "Episode: 220 Total reward: 18.0 Training loss: [0.34289026260375977] Explore P: 0.6528\n",
      "Episode: 221 Total reward: 28.0 Training loss: [0.34439200162887573] Explore P: 0.6510\n",
      "Episode: 222 Total reward: 11.0 Training loss: [0.32229626178741455] Explore P: 0.6503\n",
      "Episode: 223 Total reward: 12.0 Training loss: [0.34165701270103455] Explore P: 0.6495\n",
      "Episode: 224 Total reward: 28.0 Training loss: [0.3461155295372009] Explore P: 0.6477\n",
      "Episode: 225 Total reward: 9.0 Training loss: [0.3369298577308655] Explore P: 0.6471\n",
      "Episode: 226 Total reward: 17.0 Training loss: [0.3839449882507324] Explore P: 0.6461\n",
      "Episode: 227 Total reward: 14.0 Training loss: [0.3377906382083893] Explore P: 0.6452\n",
      "Episode: 228 Total reward: 14.0 Training loss: [0.3406424820423126] Explore P: 0.6443\n",
      "Episode: 229 Total reward: 37.0 Training loss: [0.38318580389022827] Explore P: 0.6419\n",
      "Episode: 230 Total reward: 9.0 Training loss: [0.00400103535503149] Explore P: 0.6414\n",
      "Episode: 231 Total reward: 11.0 Training loss: [0.343054860830307] Explore P: 0.6407\n",
      "Episode: 232 Total reward: 13.0 Training loss: [0.3211941123008728] Explore P: 0.6399\n",
      "Episode: 233 Total reward: 8.0 Training loss: [0.40446391701698303] Explore P: 0.6394\n",
      "Episode: 234 Total reward: 10.0 Training loss: [0.3388408422470093] Explore P: 0.6387\n",
      "Episode: 235 Total reward: 10.0 Training loss: [0.3834312856197357] Explore P: 0.6381\n",
      "Episode: 236 Total reward: 26.0 Training loss: [0.3396574556827545] Explore P: 0.6365\n",
      "Episode: 237 Total reward: 10.0 Training loss: [0.3836453855037689] Explore P: 0.6358\n",
      "Episode: 238 Total reward: 12.0 Training loss: [0.3476254940032959] Explore P: 0.6351\n",
      "Episode: 239 Total reward: 10.0 Training loss: [0.0019353587413206697] Explore P: 0.6345\n",
      "Episode: 240 Total reward: 9.0 Training loss: [0.3940742313861847] Explore P: 0.6339\n",
      "Episode: 241 Total reward: 11.0 Training loss: [0.33811256289482117] Explore P: 0.6332\n",
      "Episode: 242 Total reward: 26.0 Training loss: [0.3415077328681946] Explore P: 0.6316\n",
      "Episode: 243 Total reward: 18.0 Training loss: [0.34408190846443176] Explore P: 0.6305\n",
      "Episode: 244 Total reward: 13.0 Training loss: [0.32049959897994995] Explore P: 0.6297\n",
      "Episode: 245 Total reward: 13.0 Training loss: [0.3373261094093323] Explore P: 0.6289\n",
      "Episode: 246 Total reward: 27.0 Training loss: [0.3203427195549011] Explore P: 0.6272\n",
      "Episode: 247 Total reward: 11.0 Training loss: [0.34620201587677] Explore P: 0.6265\n",
      "Episode: 248 Total reward: 10.0 Training loss: [0.33875417709350586] Explore P: 0.6259\n",
      "Episode: 249 Total reward: 10.0 Training loss: [0.3459746539592743] Explore P: 0.6253\n",
      "Episode: 250 Total reward: 15.0 Training loss: [0.391453355550766] Explore P: 0.6244\n",
      "Episode: 251 Total reward: 16.0 Training loss: [0.3844570517539978] Explore P: 0.6234\n",
      "Episode: 252 Total reward: 19.0 Training loss: [0.32252997159957886] Explore P: 0.6222\n",
      "Episode: 253 Total reward: 16.0 Training loss: [0.33867108821868896] Explore P: 0.6212\n",
      "Episode: 254 Total reward: 10.0 Training loss: [0.3761906921863556] Explore P: 0.6206\n",
      "Episode: 255 Total reward: 16.0 Training loss: [0.0019610305316746235] Explore P: 0.6197\n",
      "Episode: 256 Total reward: 17.0 Training loss: [0.3800848722457886] Explore P: 0.6186\n",
      "Episode: 257 Total reward: 14.0 Training loss: [0.34478795528411865] Explore P: 0.6178\n",
      "Episode: 258 Total reward: 12.0 Training loss: [0.3231777846813202] Explore P: 0.6170\n",
      "Episode: 259 Total reward: 16.0 Training loss: [0.38082677125930786] Explore P: 0.6161\n",
      "Episode: 260 Total reward: 14.0 Training loss: [0.3784816861152649] Explore P: 0.6152\n",
      "Episode: 261 Total reward: 13.0 Training loss: [0.32074111700057983] Explore P: 0.6144\n",
      "Episode: 262 Total reward: 12.0 Training loss: [0.3417837917804718] Explore P: 0.6137\n",
      "Episode: 263 Total reward: 43.0 Training loss: [0.32152411341667175] Explore P: 0.6111\n",
      "Episode: 264 Total reward: 15.0 Training loss: [0.37847599387168884] Explore P: 0.6102\n",
      "Episode: 265 Total reward: 12.0 Training loss: [0.3446306884288788] Explore P: 0.6095\n",
      "Episode: 266 Total reward: 16.0 Training loss: [0.3374389111995697] Explore P: 0.6085\n",
      "Episode: 267 Total reward: 12.0 Training loss: [0.32132717967033386] Explore P: 0.6078\n",
      "Episode: 268 Total reward: 18.0 Training loss: [0.3777000606060028] Explore P: 0.6068\n",
      "Episode: 269 Total reward: 13.0 Training loss: [0.3386687934398651] Explore P: 0.6060\n",
      "Episode: 270 Total reward: 16.0 Training loss: [0.3412691354751587] Explore P: 0.6050\n",
      "Episode: 271 Total reward: 14.0 Training loss: [0.33340373635292053] Explore P: 0.6042\n",
      "Episode: 272 Total reward: 10.0 Training loss: [0.33363091945648193] Explore P: 0.6036\n",
      "Episode: 273 Total reward: 22.0 Training loss: [0.3210599422454834] Explore P: 0.6023\n",
      "Episode: 274 Total reward: 19.0 Training loss: [0.3405483663082123] Explore P: 0.6012\n",
      "Episode: 275 Total reward: 11.0 Training loss: [0.3779047727584839] Explore P: 0.6005\n",
      "Episode: 276 Total reward: 24.0 Training loss: [0.3283710777759552] Explore P: 0.5991\n",
      "Episode: 277 Total reward: 19.0 Training loss: [0.3368655741214752] Explore P: 0.5980\n",
      "Episode: 278 Total reward: 9.0 Training loss: [0.32006916403770447] Explore P: 0.5975\n",
      "Episode: 279 Total reward: 7.0 Training loss: [0.3315737545490265] Explore P: 0.5970\n",
      "Episode: 280 Total reward: 15.0 Training loss: [0.33791273832321167] Explore P: 0.5962\n",
      "Episode: 281 Total reward: 15.0 Training loss: [0.32017406821250916] Explore P: 0.5953\n",
      "Episode: 282 Total reward: 22.0 Training loss: [0.32158488035202026] Explore P: 0.5940\n",
      "Episode: 283 Total reward: 12.0 Training loss: [0.3398204445838928] Explore P: 0.5933\n",
      "Episode: 284 Total reward: 11.0 Training loss: [0.334336519241333] Explore P: 0.5927\n",
      "Episode: 285 Total reward: 9.0 Training loss: [0.0005560552817769349] Explore P: 0.5921\n",
      "Episode: 286 Total reward: 38.0 Training loss: [0.34009188413619995] Explore P: 0.5899\n",
      "Episode: 287 Total reward: 18.0 Training loss: [0.32976362109184265] Explore P: 0.5889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 288 Total reward: 14.0 Training loss: [0.3327372968196869] Explore P: 0.5881\n",
      "Episode: 289 Total reward: 12.0 Training loss: [0.32124054431915283] Explore P: 0.5874\n",
      "Episode: 290 Total reward: 10.0 Training loss: [0.32159867882728577] Explore P: 0.5868\n",
      "Episode: 291 Total reward: 23.0 Training loss: [0.0008080892730504274] Explore P: 0.5855\n",
      "Episode: 292 Total reward: 12.0 Training loss: [0.32261329889297485] Explore P: 0.5848\n",
      "Episode: 293 Total reward: 19.0 Training loss: [0.37269487977027893] Explore P: 0.5837\n",
      "Episode: 294 Total reward: 10.0 Training loss: [0.37318286299705505] Explore P: 0.5831\n",
      "Episode: 295 Total reward: 10.0 Training loss: [0.3660750985145569] Explore P: 0.5826\n",
      "Episode: 296 Total reward: 11.0 Training loss: [0.3329896032810211] Explore P: 0.5819\n",
      "Episode: 297 Total reward: 11.0 Training loss: [0.34174370765686035] Explore P: 0.5813\n",
      "Episode: 298 Total reward: 15.0 Training loss: [0.3299858272075653] Explore P: 0.5804\n",
      "Episode: 299 Total reward: 7.0 Training loss: [0.00046921506873331964] Explore P: 0.5800\n",
      "Episode: 300 Total reward: 9.0 Training loss: [0.0032232149969786406] Explore P: 0.5795\n",
      "Episode: 301 Total reward: 17.0 Training loss: [0.3312271237373352] Explore P: 0.5786\n",
      "Episode: 302 Total reward: 12.0 Training loss: [0.33650362491607666] Explore P: 0.5779\n",
      "Episode: 303 Total reward: 16.0 Training loss: [0.335279643535614] Explore P: 0.5770\n",
      "Episode: 304 Total reward: 15.0 Training loss: [0.34171584248542786] Explore P: 0.5761\n",
      "Episode: 305 Total reward: 15.0 Training loss: [0.3752465546131134] Explore P: 0.5753\n",
      "Episode: 306 Total reward: 12.0 Training loss: [0.36461538076400757] Explore P: 0.5746\n",
      "Episode: 307 Total reward: 12.0 Training loss: [0.3352171778678894] Explore P: 0.5739\n",
      "Episode: 308 Total reward: 13.0 Training loss: [0.37757542729377747] Explore P: 0.5732\n",
      "Episode: 309 Total reward: 17.0 Training loss: [0.3331936001777649] Explore P: 0.5722\n",
      "Episode: 310 Total reward: 13.0 Training loss: [0.3374803364276886] Explore P: 0.5715\n",
      "Episode: 311 Total reward: 9.0 Training loss: [0.3409341871738434] Explore P: 0.5710\n",
      "Episode: 312 Total reward: 10.0 Training loss: [0.3345261216163635] Explore P: 0.5704\n",
      "Episode: 313 Total reward: 14.0 Training loss: [0.33926358819007874] Explore P: 0.5696\n",
      "Episode: 314 Total reward: 11.0 Training loss: [0.3209706246852875] Explore P: 0.5690\n",
      "Episode: 315 Total reward: 10.0 Training loss: [0.36525335907936096] Explore P: 0.5685\n",
      "Episode: 316 Total reward: 12.0 Training loss: [0.3402903974056244] Explore P: 0.5678\n",
      "Episode: 317 Total reward: 15.0 Training loss: [0.3374163806438446] Explore P: 0.5670\n",
      "Episode: 318 Total reward: 13.0 Training loss: [0.3326144516468048] Explore P: 0.5662\n",
      "Episode: 319 Total reward: 9.0 Training loss: [0.32993364334106445] Explore P: 0.5657\n",
      "Episode: 320 Total reward: 16.0 Training loss: [0.0003610692510847002] Explore P: 0.5649\n",
      "Episode: 321 Total reward: 13.0 Training loss: [0.3317982256412506] Explore P: 0.5641\n",
      "Episode: 322 Total reward: 10.0 Training loss: [0.34029051661491394] Explore P: 0.5636\n",
      "Episode: 323 Total reward: 16.0 Training loss: [0.33032578229904175] Explore P: 0.5627\n",
      "Episode: 324 Total reward: 12.0 Training loss: [0.3403693437576294] Explore P: 0.5620\n",
      "Episode: 325 Total reward: 16.0 Training loss: [0.000617210054770112] Explore P: 0.5611\n",
      "Episode: 326 Total reward: 10.0 Training loss: [0.3311927914619446] Explore P: 0.5606\n",
      "Episode: 327 Total reward: 11.0 Training loss: [0.3624385595321655] Explore P: 0.5600\n",
      "Episode: 328 Total reward: 16.0 Training loss: [0.3679639995098114] Explore P: 0.5591\n",
      "Episode: 329 Total reward: 9.0 Training loss: [0.33169272541999817] Explore P: 0.5586\n",
      "Episode: 330 Total reward: 15.0 Training loss: [0.3318488597869873] Explore P: 0.5578\n",
      "Episode: 331 Total reward: 11.0 Training loss: [0.33079469203948975] Explore P: 0.5572\n",
      "Episode: 332 Total reward: 12.0 Training loss: [0.3343723714351654] Explore P: 0.5565\n",
      "Episode: 333 Total reward: 13.0 Training loss: [0.33894315361976624] Explore P: 0.5558\n",
      "Episode: 334 Total reward: 20.0 Training loss: [0.33910560607910156] Explore P: 0.5547\n",
      "Episode: 335 Total reward: 16.0 Training loss: [0.3280318081378937] Explore P: 0.5539\n",
      "Episode: 336 Total reward: 7.0 Training loss: [0.3668016493320465] Explore P: 0.5535\n",
      "Episode: 337 Total reward: 10.0 Training loss: [0.329558402299881] Explore P: 0.5529\n",
      "Episode: 338 Total reward: 17.0 Training loss: [0.0005652584368363023] Explore P: 0.5520\n",
      "Episode: 339 Total reward: 10.0 Training loss: [0.33523088693618774] Explore P: 0.5515\n",
      "Episode: 340 Total reward: 11.0 Training loss: [0.3658491373062134] Explore P: 0.5509\n",
      "Episode: 341 Total reward: 20.0 Training loss: [0.33507010340690613] Explore P: 0.5498\n",
      "Episode: 342 Total reward: 9.0 Training loss: [0.33186057209968567] Explore P: 0.5493\n",
      "Episode: 343 Total reward: 13.0 Training loss: [0.36527350544929504] Explore P: 0.5486\n",
      "Episode: 344 Total reward: 11.0 Training loss: [0.33064475655555725] Explore P: 0.5480\n",
      "Episode: 345 Total reward: 22.0 Training loss: [0.36070048809051514] Explore P: 0.5468\n",
      "Episode: 346 Total reward: 18.0 Training loss: [0.3327396810054779] Explore P: 0.5459\n",
      "Episode: 347 Total reward: 9.0 Training loss: [0.35973578691482544] Explore P: 0.5454\n",
      "Episode: 348 Total reward: 10.0 Training loss: [0.36376574635505676] Explore P: 0.5449\n",
      "Episode: 349 Total reward: 13.0 Training loss: [0.3378320336341858] Explore P: 0.5442\n",
      "Episode: 350 Total reward: 13.0 Training loss: [0.32659366726875305] Explore P: 0.5435\n",
      "Episode: 351 Total reward: 13.0 Training loss: [0.31989586353302] Explore P: 0.5428\n",
      "Episode: 352 Total reward: 20.0 Training loss: [0.3629920184612274] Explore P: 0.5417\n",
      "Episode: 353 Total reward: 25.0 Training loss: [0.3646109402179718] Explore P: 0.5404\n",
      "Episode: 354 Total reward: 10.0 Training loss: [0.33048442006111145] Explore P: 0.5399\n",
      "Episode: 355 Total reward: 14.0 Training loss: [0.33130747079849243] Explore P: 0.5391\n",
      "Episode: 356 Total reward: 13.0 Training loss: [0.36291828751564026] Explore P: 0.5384\n",
      "Episode: 357 Total reward: 13.0 Training loss: [0.3647860586643219] Explore P: 0.5377\n",
      "Episode: 358 Total reward: 11.0 Training loss: [0.36252057552337646] Explore P: 0.5372\n",
      "Episode: 359 Total reward: 10.0 Training loss: [0.3377562463283539] Explore P: 0.5366\n",
      "Episode: 360 Total reward: 11.0 Training loss: [0.3334897458553314] Explore P: 0.5361\n",
      "Episode: 361 Total reward: 11.0 Training loss: [0.3632362484931946] Explore P: 0.5355\n",
      "Episode: 362 Total reward: 11.0 Training loss: [0.0016791680827736855] Explore P: 0.5349\n",
      "Episode: 363 Total reward: 12.0 Training loss: [0.33155322074890137] Explore P: 0.5343\n",
      "Episode: 364 Total reward: 10.0 Training loss: [0.36000269651412964] Explore P: 0.5337\n",
      "Episode: 365 Total reward: 14.0 Training loss: [0.33817124366760254] Explore P: 0.5330\n",
      "Episode: 366 Total reward: 32.0 Training loss: [0.36432743072509766] Explore P: 0.5313\n",
      "Episode: 367 Total reward: 17.0 Training loss: [0.36418479681015015] Explore P: 0.5305\n",
      "Episode: 368 Total reward: 14.0 Training loss: [0.35808607935905457] Explore P: 0.5297\n",
      "Episode: 369 Total reward: 14.0 Training loss: [0.35820886492729187] Explore P: 0.5290\n",
      "Episode: 370 Total reward: 8.0 Training loss: [0.3210470378398895] Explore P: 0.5286\n",
      "Episode: 371 Total reward: 16.0 Training loss: [0.32903042435646057] Explore P: 0.5278\n",
      "Episode: 372 Total reward: 9.0 Training loss: [0.32784751057624817] Explore P: 0.5273\n",
      "Episode: 373 Total reward: 11.0 Training loss: [0.33390623331069946] Explore P: 0.5267\n",
      "Episode: 374 Total reward: 12.0 Training loss: [0.33701422810554504] Explore P: 0.5261\n",
      "Episode: 375 Total reward: 8.0 Training loss: [0.3313092589378357] Explore P: 0.5257\n",
      "Episode: 376 Total reward: 13.0 Training loss: [0.33044928312301636] Explore P: 0.5250\n",
      "Episode: 377 Total reward: 20.0 Training loss: [0.3339686691761017] Explore P: 0.5240\n",
      "Episode: 378 Total reward: 12.0 Training loss: [0.3351658582687378] Explore P: 0.5234\n",
      "Episode: 379 Total reward: 21.0 Training loss: [0.3278489410877228] Explore P: 0.5223\n",
      "Episode: 380 Total reward: 8.0 Training loss: [0.35757240653038025] Explore P: 0.5219\n",
      "Episode: 381 Total reward: 12.0 Training loss: [0.3577885329723358] Explore P: 0.5213\n",
      "Episode: 382 Total reward: 10.0 Training loss: [0.3581109941005707] Explore P: 0.5208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 383 Total reward: 10.0 Training loss: [0.325427383184433] Explore P: 0.5203\n",
      "Episode: 384 Total reward: 12.0 Training loss: [0.3306494355201721] Explore P: 0.5196\n",
      "Episode: 385 Total reward: 15.0 Training loss: [0.3282284736633301] Explore P: 0.5189\n",
      "Episode: 386 Total reward: 10.0 Training loss: [0.35573700070381165] Explore P: 0.5184\n",
      "Episode: 387 Total reward: 13.0 Training loss: [0.35525602102279663] Explore P: 0.5177\n",
      "Episode: 388 Total reward: 10.0 Training loss: [0.3276337683200836] Explore P: 0.5172\n",
      "Episode: 389 Total reward: 9.0 Training loss: [0.35706570744514465] Explore P: 0.5167\n",
      "Episode: 390 Total reward: 12.0 Training loss: [0.3359619081020355] Explore P: 0.5161\n",
      "Episode: 391 Total reward: 13.0 Training loss: [0.3326334059238434] Explore P: 0.5155\n",
      "Episode: 392 Total reward: 14.0 Training loss: [0.3550807535648346] Explore P: 0.5148\n",
      "Episode: 393 Total reward: 11.0 Training loss: [0.33599162101745605] Explore P: 0.5142\n",
      "Episode: 394 Total reward: 9.0 Training loss: [0.3230551481246948] Explore P: 0.5138\n",
      "Episode: 395 Total reward: 15.0 Training loss: [0.3317151963710785] Explore P: 0.5130\n",
      "Episode: 396 Total reward: 15.0 Training loss: [0.3332881033420563] Explore P: 0.5123\n",
      "Episode: 397 Total reward: 13.0 Training loss: [0.3557084798812866] Explore P: 0.5116\n",
      "Episode: 398 Total reward: 9.0 Training loss: [0.3368556499481201] Explore P: 0.5111\n",
      "Episode: 399 Total reward: 10.0 Training loss: [0.3302099108695984] Explore P: 0.5106\n",
      "Episode: 400 Total reward: 13.0 Training loss: [0.33547520637512207] Explore P: 0.5100\n",
      "Episode: 401 Total reward: 20.0 Training loss: [0.3259136974811554] Explore P: 0.5090\n",
      "Episode: 402 Total reward: 20.0 Training loss: [0.3276333212852478] Explore P: 0.5080\n",
      "Episode: 403 Total reward: 10.0 Training loss: [0.35997623205184937] Explore P: 0.5075\n",
      "Episode: 404 Total reward: 10.0 Training loss: [0.32410261034965515] Explore P: 0.5070\n",
      "Episode: 405 Total reward: 14.0 Training loss: [0.3316459655761719] Explore P: 0.5063\n",
      "Episode: 406 Total reward: 14.0 Training loss: [0.3557680547237396] Explore P: 0.5056\n",
      "Episode: 407 Total reward: 12.0 Training loss: [0.32916969060897827] Explore P: 0.5050\n",
      "Episode: 408 Total reward: 15.0 Training loss: [0.3303786814212799] Explore P: 0.5043\n",
      "Episode: 409 Total reward: 14.0 Training loss: [0.000157054906594567] Explore P: 0.5036\n",
      "Episode: 410 Total reward: 18.0 Training loss: [0.3247271478176117] Explore P: 0.5027\n",
      "Episode: 411 Total reward: 18.0 Training loss: [0.35899215936660767] Explore P: 0.5018\n",
      "Episode: 412 Total reward: 9.0 Training loss: [0.3296959400177002] Explore P: 0.5014\n",
      "Episode: 413 Total reward: 14.0 Training loss: [0.3534421920776367] Explore P: 0.5007\n",
      "Episode: 414 Total reward: 11.0 Training loss: [0.3546493947505951] Explore P: 0.5001\n",
      "Episode: 415 Total reward: 8.0 Training loss: [0.3330845832824707] Explore P: 0.4998\n",
      "Episode: 416 Total reward: 12.0 Training loss: [0.32434526085853577] Explore P: 0.4992\n",
      "Episode: 417 Total reward: 9.0 Training loss: [0.3345201909542084] Explore P: 0.4987\n",
      "Episode: 418 Total reward: 21.0 Training loss: [0.32458585500717163] Explore P: 0.4977\n",
      "Episode: 419 Total reward: 10.0 Training loss: [0.35836759209632874] Explore P: 0.4972\n",
      "Episode: 420 Total reward: 12.0 Training loss: [0.35201114416122437] Explore P: 0.4966\n",
      "Episode: 421 Total reward: 17.0 Training loss: [0.32553380727767944] Explore P: 0.4958\n",
      "Episode: 422 Total reward: 13.0 Training loss: [5.145153045305051e-05] Explore P: 0.4952\n",
      "Episode: 423 Total reward: 13.0 Training loss: [0.33372431993484497] Explore P: 0.4945\n",
      "Episode: 424 Total reward: 9.0 Training loss: [0.32638105750083923] Explore P: 0.4941\n",
      "Episode: 425 Total reward: 10.0 Training loss: [7.666840974707156e-05] Explore P: 0.4936\n",
      "Episode: 426 Total reward: 11.0 Training loss: [0.32675400376319885] Explore P: 0.4931\n",
      "Episode: 427 Total reward: 11.0 Training loss: [0.3296234905719757] Explore P: 0.4926\n",
      "Episode: 428 Total reward: 16.0 Training loss: [0.35577279329299927] Explore P: 0.4918\n",
      "Episode: 429 Total reward: 13.0 Training loss: [0.3271789848804474] Explore P: 0.4912\n",
      "Episode: 430 Total reward: 15.0 Training loss: [0.352896124124527] Explore P: 0.4904\n",
      "Episode: 431 Total reward: 12.0 Training loss: [0.3521058261394501] Explore P: 0.4899\n",
      "Episode: 432 Total reward: 8.0 Training loss: [0.32641229033470154] Explore P: 0.4895\n",
      "Episode: 433 Total reward: 21.0 Training loss: [4.8147005145438015e-05] Explore P: 0.4885\n",
      "Episode: 434 Total reward: 15.0 Training loss: [0.350337952375412] Explore P: 0.4878\n",
      "Episode: 435 Total reward: 12.0 Training loss: [0.3290241062641144] Explore P: 0.4872\n",
      "Episode: 436 Total reward: 13.0 Training loss: [4.7096473281271756e-05] Explore P: 0.4866\n",
      "Episode: 437 Total reward: 17.0 Training loss: [0.348380982875824] Explore P: 0.4858\n",
      "Episode: 438 Total reward: 13.0 Training loss: [0.32750505208969116] Explore P: 0.4851\n",
      "Episode: 439 Total reward: 15.0 Training loss: [0.32665133476257324] Explore P: 0.4844\n",
      "Episode: 440 Total reward: 17.0 Training loss: [0.35556381940841675] Explore P: 0.4836\n",
      "Episode: 441 Total reward: 11.0 Training loss: [0.35655444860458374] Explore P: 0.4831\n",
      "Episode: 442 Total reward: 12.0 Training loss: [0.32077112793922424] Explore P: 0.4825\n",
      "Episode: 443 Total reward: 24.0 Training loss: [0.3232667148113251] Explore P: 0.4814\n",
      "Episode: 444 Total reward: 11.0 Training loss: [0.32275909185409546] Explore P: 0.4809\n",
      "Episode: 445 Total reward: 10.0 Training loss: [6.570481491507962e-05] Explore P: 0.4804\n",
      "Episode: 446 Total reward: 13.0 Training loss: [0.32491806149482727] Explore P: 0.4798\n",
      "Episode: 447 Total reward: 14.0 Training loss: [0.32756534218788147] Explore P: 0.4791\n",
      "Episode: 448 Total reward: 9.0 Training loss: [0.33267343044281006] Explore P: 0.4787\n",
      "Episode: 449 Total reward: 21.0 Training loss: [0.3280962407588959] Explore P: 0.4777\n",
      "Episode: 450 Total reward: 10.0 Training loss: [0.3499349355697632] Explore P: 0.4773\n",
      "Episode: 451 Total reward: 13.0 Training loss: [0.33108699321746826] Explore P: 0.4767\n",
      "Episode: 452 Total reward: 11.0 Training loss: [0.32834336161613464] Explore P: 0.4761\n",
      "Episode: 453 Total reward: 11.0 Training loss: [0.34686994552612305] Explore P: 0.4756\n",
      "Episode: 454 Total reward: 28.0 Training loss: [0.34987273812294006] Explore P: 0.4743\n",
      "Episode: 455 Total reward: 8.0 Training loss: [0.3462553918361664] Explore P: 0.4740\n",
      "Episode: 456 Total reward: 9.0 Training loss: [0.32495564222335815] Explore P: 0.4735\n",
      "Episode: 457 Total reward: 8.0 Training loss: [0.3279195725917816] Explore P: 0.4732\n",
      "Episode: 458 Total reward: 9.0 Training loss: [0.3494742810726166] Explore P: 0.4728\n",
      "Episode: 459 Total reward: 13.0 Training loss: [0.3487291932106018] Explore P: 0.4722\n",
      "Episode: 460 Total reward: 15.0 Training loss: [0.00020345118537079543] Explore P: 0.4715\n",
      "Episode: 461 Total reward: 12.0 Training loss: [0.33017611503601074] Explore P: 0.4709\n",
      "Episode: 462 Total reward: 9.0 Training loss: [0.3232777416706085] Explore P: 0.4705\n",
      "Episode: 463 Total reward: 11.0 Training loss: [0.3305205702781677] Explore P: 0.4700\n",
      "Episode: 464 Total reward: 13.0 Training loss: [0.32034042477607727] Explore P: 0.4694\n",
      "Episode: 465 Total reward: 15.0 Training loss: [0.0010431535774841905] Explore P: 0.4687\n",
      "Episode: 466 Total reward: 16.0 Training loss: [0.0007599691161885858] Explore P: 0.4680\n",
      "Episode: 467 Total reward: 9.0 Training loss: [0.3301379978656769] Explore P: 0.4676\n",
      "Episode: 468 Total reward: 9.0 Training loss: [0.3553537428379059] Explore P: 0.4671\n",
      "Episode: 469 Total reward: 12.0 Training loss: [0.3240884244441986] Explore P: 0.4666\n",
      "Episode: 470 Total reward: 10.0 Training loss: [0.3325324058532715] Explore P: 0.4661\n",
      "Episode: 471 Total reward: 9.0 Training loss: [0.3318764567375183] Explore P: 0.4657\n",
      "Episode: 472 Total reward: 11.0 Training loss: [0.32858771085739136] Explore P: 0.4652\n",
      "Episode: 473 Total reward: 11.0 Training loss: [0.32322612404823303] Explore P: 0.4647\n",
      "Episode: 474 Total reward: 10.0 Training loss: [0.32411062717437744] Explore P: 0.4643\n",
      "Episode: 475 Total reward: 10.0 Training loss: [0.3224368691444397] Explore P: 0.4638\n",
      "Episode: 476 Total reward: 14.0 Training loss: [9.774730278877541e-05] Explore P: 0.4632\n",
      "Episode: 477 Total reward: 9.0 Training loss: [0.3316957950592041] Explore P: 0.4628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 478 Total reward: 18.0 Training loss: [0.35328003764152527] Explore P: 0.4620\n",
      "Episode: 479 Total reward: 11.0 Training loss: [0.3296900689601898] Explore P: 0.4615\n",
      "Episode: 480 Total reward: 8.0 Training loss: [0.331447958946228] Explore P: 0.4611\n",
      "Episode: 481 Total reward: 10.0 Training loss: [0.32737964391708374] Explore P: 0.4607\n",
      "Episode: 482 Total reward: 15.0 Training loss: [0.34854409098625183] Explore P: 0.4600\n",
      "Episode: 483 Total reward: 14.0 Training loss: [0.3293924033641815] Explore P: 0.4594\n",
      "Episode: 484 Total reward: 16.0 Training loss: [0.34883958101272583] Explore P: 0.4586\n",
      "Episode: 485 Total reward: 9.0 Training loss: [0.3313477337360382] Explore P: 0.4582\n",
      "Episode: 486 Total reward: 10.0 Training loss: [0.32101771235466003] Explore P: 0.4578\n",
      "Episode: 487 Total reward: 12.0 Training loss: [0.34638458490371704] Explore P: 0.4572\n",
      "Episode: 488 Total reward: 12.0 Training loss: [0.3222644031047821] Explore P: 0.4567\n",
      "Episode: 489 Total reward: 9.0 Training loss: [0.32504358887672424] Explore P: 0.4563\n",
      "Episode: 490 Total reward: 17.0 Training loss: [0.3287532329559326] Explore P: 0.4555\n",
      "Episode: 491 Total reward: 13.0 Training loss: [0.33164411783218384] Explore P: 0.4550\n",
      "Episode: 492 Total reward: 8.0 Training loss: [1.3379490155784879e-05] Explore P: 0.4546\n",
      "Episode: 493 Total reward: 12.0 Training loss: [0.33079662919044495] Explore P: 0.4541\n",
      "Episode: 494 Total reward: 20.0 Training loss: [0.33229944109916687] Explore P: 0.4532\n",
      "Episode: 495 Total reward: 22.0 Training loss: [0.3298974633216858] Explore P: 0.4522\n",
      "Episode: 496 Total reward: 13.0 Training loss: [0.3292417526245117] Explore P: 0.4516\n",
      "Episode: 497 Total reward: 12.0 Training loss: [0.3205346167087555] Explore P: 0.4511\n",
      "Episode: 498 Total reward: 7.0 Training loss: [0.3289649486541748] Explore P: 0.4508\n",
      "Episode: 499 Total reward: 12.0 Training loss: [0.3250867426395416] Explore P: 0.4503\n",
      "Episode: 500 Total reward: 11.0 Training loss: [0.3323213756084442] Explore P: 0.4498\n",
      "Episode: 501 Total reward: 23.0 Training loss: [0.32502201199531555] Explore P: 0.4488\n",
      "Episode: 502 Total reward: 9.0 Training loss: [0.32542330026626587] Explore P: 0.4484\n",
      "Episode: 503 Total reward: 22.0 Training loss: [0.3309035897254944] Explore P: 0.4474\n",
      "Episode: 504 Total reward: 15.0 Training loss: [0.3517000377178192] Explore P: 0.4468\n",
      "Episode: 505 Total reward: 12.0 Training loss: [0.32915571331977844] Explore P: 0.4462\n",
      "Episode: 506 Total reward: 19.0 Training loss: [0.32465818524360657] Explore P: 0.4454\n",
      "Episode: 507 Total reward: 16.0 Training loss: [0.3263886272907257] Explore P: 0.4447\n",
      "Episode: 508 Total reward: 12.0 Training loss: [4.514960528467782e-05] Explore P: 0.4442\n",
      "Episode: 509 Total reward: 13.0 Training loss: [0.0005258909077383578] Explore P: 0.4436\n",
      "Episode: 510 Total reward: 25.0 Training loss: [0.33183377981185913] Explore P: 0.4426\n",
      "Episode: 511 Total reward: 12.0 Training loss: [0.3235747814178467] Explore P: 0.4420\n",
      "Episode: 512 Total reward: 13.0 Training loss: [0.321595162153244] Explore P: 0.4415\n",
      "Episode: 513 Total reward: 12.0 Training loss: [0.3227183520793915] Explore P: 0.4410\n",
      "Episode: 514 Total reward: 16.0 Training loss: [0.346957266330719] Explore P: 0.4403\n",
      "Episode: 515 Total reward: 9.0 Training loss: [0.32493579387664795] Explore P: 0.4399\n",
      "Episode: 516 Total reward: 10.0 Training loss: [0.33169448375701904] Explore P: 0.4394\n",
      "Episode: 517 Total reward: 11.0 Training loss: [0.32853108644485474] Explore P: 0.4390\n",
      "Episode: 518 Total reward: 11.0 Training loss: [0.35229751467704773] Explore P: 0.4385\n",
      "Episode: 519 Total reward: 16.0 Training loss: [0.3248032033443451] Explore P: 0.4378\n",
      "Episode: 520 Total reward: 12.0 Training loss: [0.3308432698249817] Explore P: 0.4373\n",
      "Episode: 521 Total reward: 13.0 Training loss: [0.33144116401672363] Explore P: 0.4368\n",
      "Episode: 522 Total reward: 13.0 Training loss: [0.000315500219585374] Explore P: 0.4362\n",
      "Episode: 523 Total reward: 8.0 Training loss: [0.32826313376426697] Explore P: 0.4359\n",
      "Episode: 524 Total reward: 16.0 Training loss: [0.32600727677345276] Explore P: 0.4352\n",
      "Episode: 525 Total reward: 9.0 Training loss: [0.32317623496055603] Explore P: 0.4348\n",
      "Episode: 526 Total reward: 14.0 Training loss: [0.32900330424308777] Explore P: 0.4342\n",
      "Episode: 527 Total reward: 10.0 Training loss: [0.3206924796104431] Explore P: 0.4338\n",
      "Episode: 528 Total reward: 19.0 Training loss: [0.33982083201408386] Explore P: 0.4330\n",
      "Episode: 529 Total reward: 12.0 Training loss: [0.3463873565196991] Explore P: 0.4325\n",
      "Episode: 530 Total reward: 8.0 Training loss: [0.3258990943431854] Explore P: 0.4321\n",
      "Episode: 531 Total reward: 15.0 Training loss: [0.33055809140205383] Explore P: 0.4315\n",
      "Episode: 532 Total reward: 17.0 Training loss: [0.3446071147918701] Explore P: 0.4308\n",
      "Episode: 533 Total reward: 14.0 Training loss: [0.32256507873535156] Explore P: 0.4302\n",
      "Episode: 534 Total reward: 15.0 Training loss: [0.32601651549339294] Explore P: 0.4296\n",
      "Episode: 535 Total reward: 10.0 Training loss: [0.32174015045166016] Explore P: 0.4291\n",
      "Episode: 536 Total reward: 9.0 Training loss: [0.33015018701553345] Explore P: 0.4288\n",
      "Episode: 537 Total reward: 14.0 Training loss: [0.3230077922344208] Explore P: 0.4282\n",
      "Episode: 538 Total reward: 13.0 Training loss: [0.3444471061229706] Explore P: 0.4276\n",
      "Episode: 539 Total reward: 26.0 Training loss: [0.3370836079120636] Explore P: 0.4265\n",
      "Episode: 540 Total reward: 15.0 Training loss: [0.3256145417690277] Explore P: 0.4259\n",
      "Episode: 541 Total reward: 13.0 Training loss: [0.3303318917751312] Explore P: 0.4254\n",
      "Episode: 542 Total reward: 10.0 Training loss: [0.3273911774158478] Explore P: 0.4250\n",
      "Episode: 543 Total reward: 10.0 Training loss: [0.32742801308631897] Explore P: 0.4246\n",
      "Episode: 544 Total reward: 12.0 Training loss: [0.32194799184799194] Explore P: 0.4241\n",
      "Episode: 545 Total reward: 11.0 Training loss: [0.3233695328235626] Explore P: 0.4236\n",
      "Episode: 546 Total reward: 16.0 Training loss: [0.3284553289413452] Explore P: 0.4229\n",
      "Episode: 547 Total reward: 11.0 Training loss: [5.44149679626571e-06] Explore P: 0.4225\n",
      "Episode: 548 Total reward: 13.0 Training loss: [0.3241681754589081] Explore P: 0.4220\n",
      "Episode: 549 Total reward: 8.0 Training loss: [0.32078298926353455] Explore P: 0.4216\n",
      "Episode: 550 Total reward: 12.0 Training loss: [0.3256564736366272] Explore P: 0.4211\n",
      "Episode: 551 Total reward: 19.0 Training loss: [0.3199827969074249] Explore P: 0.4203\n",
      "Episode: 552 Total reward: 17.0 Training loss: [0.3403882086277008] Explore P: 0.4197\n",
      "Episode: 553 Total reward: 11.0 Training loss: [0.32468387484550476] Explore P: 0.4192\n",
      "Episode: 554 Total reward: 8.0 Training loss: [0.3271632790565491] Explore P: 0.4189\n",
      "Episode: 555 Total reward: 9.0 Training loss: [0.3220055103302002] Explore P: 0.4185\n",
      "Episode: 556 Total reward: 8.0 Training loss: [0.34989413619041443] Explore P: 0.4182\n",
      "Episode: 557 Total reward: 11.0 Training loss: [0.3258516490459442] Explore P: 0.4177\n",
      "Episode: 558 Total reward: 16.0 Training loss: [0.3239501714706421] Explore P: 0.4171\n",
      "Episode: 559 Total reward: 10.0 Training loss: [0.32093098759651184] Explore P: 0.4167\n",
      "Episode: 560 Total reward: 8.0 Training loss: [0.3260270357131958] Explore P: 0.4163\n",
      "Episode: 561 Total reward: 23.0 Training loss: [0.3239489793777466] Explore P: 0.4154\n",
      "Episode: 562 Total reward: 20.0 Training loss: [0.3216741681098938] Explore P: 0.4146\n",
      "Episode: 563 Total reward: 9.0 Training loss: [0.3278108239173889] Explore P: 0.4142\n",
      "Episode: 564 Total reward: 21.0 Training loss: [0.3428860306739807] Explore P: 0.4134\n",
      "Episode: 565 Total reward: 12.0 Training loss: [0.32263579964637756] Explore P: 0.4129\n",
      "Episode: 566 Total reward: 15.0 Training loss: [0.3274155259132385] Explore P: 0.4123\n",
      "Episode: 567 Total reward: 9.0 Training loss: [0.32153019309043884] Explore P: 0.4119\n",
      "Episode: 568 Total reward: 15.0 Training loss: [0.341288298368454] Explore P: 0.4113\n",
      "Episode: 569 Total reward: 20.0 Training loss: [0.3297829329967499] Explore P: 0.4105\n",
      "Episode: 570 Total reward: 10.0 Training loss: [0.3489935100078583] Explore P: 0.4101\n",
      "Episode: 571 Total reward: 12.0 Training loss: [0.346053808927536] Explore P: 0.4097\n",
      "Episode: 572 Total reward: 12.0 Training loss: [0.32517966628074646] Explore P: 0.4092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 573 Total reward: 16.0 Training loss: [0.3292274475097656] Explore P: 0.4085\n",
      "Episode: 574 Total reward: 14.0 Training loss: [0.3269740641117096] Explore P: 0.4080\n",
      "Episode: 575 Total reward: 11.0 Training loss: [0.3408374786376953] Explore P: 0.4075\n",
      "Episode: 576 Total reward: 14.0 Training loss: [0.00023407650587614626] Explore P: 0.4070\n",
      "Episode: 577 Total reward: 10.0 Training loss: [9.919132025970612e-06] Explore P: 0.4066\n",
      "Episode: 578 Total reward: 10.0 Training loss: [0.32730984687805176] Explore P: 0.4062\n",
      "Episode: 579 Total reward: 13.0 Training loss: [0.32688215374946594] Explore P: 0.4057\n",
      "Episode: 580 Total reward: 15.0 Training loss: [0.32345443964004517] Explore P: 0.4051\n",
      "Episode: 581 Total reward: 9.0 Training loss: [1.1551023817446548e-05] Explore P: 0.4047\n",
      "Episode: 582 Total reward: 10.0 Training loss: [0.3272782564163208] Explore P: 0.4043\n",
      "Episode: 583 Total reward: 10.0 Training loss: [0.34412136673927307] Explore P: 0.4039\n",
      "Episode: 584 Total reward: 11.0 Training loss: [0.3438844084739685] Explore P: 0.4035\n",
      "Episode: 585 Total reward: 14.0 Training loss: [0.3265635073184967] Explore P: 0.4030\n",
      "Episode: 586 Total reward: 10.0 Training loss: [0.3415580689907074] Explore P: 0.4026\n",
      "Episode: 587 Total reward: 12.0 Training loss: [5.800377948617097e-06] Explore P: 0.4021\n",
      "Episode: 588 Total reward: 15.0 Training loss: [0.3235286474227905] Explore P: 0.4015\n",
      "Episode: 589 Total reward: 14.0 Training loss: [0.3234179615974426] Explore P: 0.4010\n",
      "Episode: 590 Total reward: 8.0 Training loss: [0.3380112648010254] Explore P: 0.4006\n",
      "Episode: 591 Total reward: 9.0 Training loss: [0.32240912318229675] Explore P: 0.4003\n",
      "Episode: 592 Total reward: 10.0 Training loss: [0.3204210102558136] Explore P: 0.3999\n",
      "Episode: 593 Total reward: 24.0 Training loss: [0.3272152841091156] Explore P: 0.3990\n",
      "Episode: 594 Total reward: 14.0 Training loss: [0.32047492265701294] Explore P: 0.3984\n",
      "Episode: 595 Total reward: 14.0 Training loss: [0.337698370218277] Explore P: 0.3979\n",
      "Episode: 596 Total reward: 12.0 Training loss: [0.3250138759613037] Explore P: 0.3974\n",
      "Episode: 597 Total reward: 11.0 Training loss: [0.3217652440071106] Explore P: 0.3970\n",
      "Episode: 598 Total reward: 10.0 Training loss: [0.32678931951522827] Explore P: 0.3966\n",
      "Episode: 599 Total reward: 12.0 Training loss: [0.3290424048900604] Explore P: 0.3961\n",
      "Episode: 600 Total reward: 8.0 Training loss: [0.34761008620262146] Explore P: 0.3958\n",
      "Episode: 601 Total reward: 10.0 Training loss: [0.32320263981819153] Explore P: 0.3954\n",
      "Episode: 602 Total reward: 14.0 Training loss: [0.32474809885025024] Explore P: 0.3949\n",
      "Episode: 603 Total reward: 11.0 Training loss: [0.32318592071533203] Explore P: 0.3945\n",
      "Episode: 604 Total reward: 11.0 Training loss: [0.3426035940647125] Explore P: 0.3941\n",
      "Episode: 605 Total reward: 9.0 Training loss: [3.2947270938166184e-06] Explore P: 0.3937\n",
      "Episode: 606 Total reward: 16.0 Training loss: [0.32452499866485596] Explore P: 0.3931\n",
      "Episode: 607 Total reward: 14.0 Training loss: [0.3267148435115814] Explore P: 0.3926\n",
      "Episode: 608 Total reward: 16.0 Training loss: [0.3210003972053528] Explore P: 0.3920\n",
      "Episode: 609 Total reward: 17.0 Training loss: [0.32334229350090027] Explore P: 0.3913\n",
      "Episode: 610 Total reward: 10.0 Training loss: [0.32714298367500305] Explore P: 0.3909\n",
      "Episode: 611 Total reward: 12.0 Training loss: [0.3245204985141754] Explore P: 0.3905\n",
      "Episode: 612 Total reward: 10.0 Training loss: [0.32444310188293457] Explore P: 0.3901\n",
      "Episode: 613 Total reward: 10.0 Training loss: [0.3240661919116974] Explore P: 0.3897\n",
      "Episode: 614 Total reward: 9.0 Training loss: [0.32676056027412415] Explore P: 0.3894\n",
      "Episode: 615 Total reward: 22.0 Training loss: [0.32440492510795593] Explore P: 0.3885\n",
      "Episode: 616 Total reward: 10.0 Training loss: [0.3201569616794586] Explore P: 0.3882\n",
      "Episode: 617 Total reward: 14.0 Training loss: [0.3232225477695465] Explore P: 0.3876\n",
      "Episode: 618 Total reward: 11.0 Training loss: [0.3208244740962982] Explore P: 0.3872\n",
      "Episode: 619 Total reward: 11.0 Training loss: [0.32463139295578003] Explore P: 0.3868\n",
      "Episode: 620 Total reward: 23.0 Training loss: [0.3465457260608673] Explore P: 0.3859\n",
      "Episode: 621 Total reward: 16.0 Training loss: [0.32710474729537964] Explore P: 0.3853\n",
      "Episode: 622 Total reward: 16.0 Training loss: [0.32088133692741394] Explore P: 0.3847\n",
      "Episode: 623 Total reward: 13.0 Training loss: [0.3263508677482605] Explore P: 0.3842\n",
      "Episode: 624 Total reward: 17.0 Training loss: [0.00012066158524248749] Explore P: 0.3836\n",
      "Episode: 625 Total reward: 10.0 Training loss: [0.3283573389053345] Explore P: 0.3832\n",
      "Episode: 626 Total reward: 23.0 Training loss: [0.32166025042533875] Explore P: 0.3824\n",
      "Episode: 627 Total reward: 13.0 Training loss: [0.32618340849876404] Explore P: 0.3819\n",
      "Episode: 628 Total reward: 14.0 Training loss: [0.3221527338027954] Explore P: 0.3814\n",
      "Episode: 629 Total reward: 8.0 Training loss: [0.3213948905467987] Explore P: 0.3811\n",
      "Episode: 630 Total reward: 11.0 Training loss: [0.3206673562526703] Explore P: 0.3807\n",
      "Episode: 631 Total reward: 12.0 Training loss: [0.3242794871330261] Explore P: 0.3802\n",
      "Episode: 632 Total reward: 22.0 Training loss: [0.3471251130104065] Explore P: 0.3794\n",
      "Episode: 633 Total reward: 13.0 Training loss: [0.3230820298194885] Explore P: 0.3789\n",
      "Episode: 634 Total reward: 13.0 Training loss: [0.34667763113975525] Explore P: 0.3784\n",
      "Episode: 635 Total reward: 10.0 Training loss: [0.3210470378398895] Explore P: 0.3781\n",
      "Episode: 636 Total reward: 19.0 Training loss: [0.3263565003871918] Explore P: 0.3774\n",
      "Episode: 637 Total reward: 9.0 Training loss: [0.3216182291507721] Explore P: 0.3771\n",
      "Episode: 638 Total reward: 11.0 Training loss: [0.326108455657959] Explore P: 0.3766\n",
      "Episode: 639 Total reward: 8.0 Training loss: [0.3356062173843384] Explore P: 0.3764\n",
      "Episode: 640 Total reward: 12.0 Training loss: [0.32638922333717346] Explore P: 0.3759\n",
      "Episode: 641 Total reward: 10.0 Training loss: [0.32631462812423706] Explore P: 0.3756\n",
      "Episode: 642 Total reward: 7.0 Training loss: [0.32626107335090637] Explore P: 0.3753\n",
      "Episode: 643 Total reward: 24.0 Training loss: [4.0293762140208855e-05] Explore P: 0.3744\n",
      "Episode: 644 Total reward: 12.0 Training loss: [6.432914574361348e-07] Explore P: 0.3740\n",
      "Episode: 645 Total reward: 17.0 Training loss: [0.3335598409175873] Explore P: 0.3734\n",
      "Episode: 646 Total reward: 10.0 Training loss: [0.3412080407142639] Explore P: 0.3730\n",
      "Episode: 647 Total reward: 10.0 Training loss: [0.32605046033859253] Explore P: 0.3726\n",
      "Episode: 648 Total reward: 9.0 Training loss: [0.32205092906951904] Explore P: 0.3723\n",
      "Episode: 649 Total reward: 11.0 Training loss: [0.3455376923084259] Explore P: 0.3719\n",
      "Episode: 650 Total reward: 11.0 Training loss: [0.32532232999801636] Explore P: 0.3715\n",
      "Episode: 651 Total reward: 13.0 Training loss: [0.32743075489997864] Explore P: 0.3710\n",
      "Episode: 652 Total reward: 19.0 Training loss: [0.32586613297462463] Explore P: 0.3704\n",
      "Episode: 653 Total reward: 9.0 Training loss: [1.7564403833603137e-06] Explore P: 0.3700\n",
      "Episode: 654 Total reward: 11.0 Training loss: [0.32595717906951904] Explore P: 0.3696\n",
      "Episode: 655 Total reward: 16.0 Training loss: [0.3213501572608948] Explore P: 0.3691\n",
      "Episode: 656 Total reward: 13.0 Training loss: [0.3213448226451874] Explore P: 0.3686\n",
      "Episode: 657 Total reward: 13.0 Training loss: [0.3213963210582733] Explore P: 0.3681\n",
      "Episode: 658 Total reward: 10.0 Training loss: [0.32109174132347107] Explore P: 0.3678\n",
      "Episode: 659 Total reward: 13.0 Training loss: [0.32780948281288147] Explore P: 0.3673\n",
      "Episode: 660 Total reward: 9.0 Training loss: [0.3234160840511322] Explore P: 0.3670\n",
      "Episode: 661 Total reward: 18.0 Training loss: [0.3255552351474762] Explore P: 0.3663\n",
      "Episode: 662 Total reward: 14.0 Training loss: [0.3242359459400177] Explore P: 0.3658\n",
      "Episode: 663 Total reward: 15.0 Training loss: [0.3271041512489319] Explore P: 0.3653\n",
      "Episode: 664 Total reward: 10.0 Training loss: [1.5625834066668176e-06] Explore P: 0.3650\n",
      "Episode: 665 Total reward: 11.0 Training loss: [0.32387775182724] Explore P: 0.3646\n",
      "Episode: 666 Total reward: 10.0 Training loss: [0.3258480131626129] Explore P: 0.3642\n",
      "Episode: 667 Total reward: 11.0 Training loss: [0.32092413306236267] Explore P: 0.3638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 668 Total reward: 10.0 Training loss: [0.32372692227363586] Explore P: 0.3635\n",
      "Episode: 669 Total reward: 10.0 Training loss: [0.3205052316188812] Explore P: 0.3631\n",
      "Episode: 670 Total reward: 9.0 Training loss: [0.3362914025783539] Explore P: 0.3628\n",
      "Episode: 671 Total reward: 9.0 Training loss: [0.32577717304229736] Explore P: 0.3625\n",
      "Episode: 672 Total reward: 12.0 Training loss: [0.34477970004081726] Explore P: 0.3621\n",
      "Episode: 673 Total reward: 10.0 Training loss: [0.3204583525657654] Explore P: 0.3617\n",
      "Episode: 674 Total reward: 11.0 Training loss: [0.32689714431762695] Explore P: 0.3613\n",
      "Episode: 675 Total reward: 13.0 Training loss: [0.3455386757850647] Explore P: 0.3609\n",
      "Episode: 676 Total reward: 16.0 Training loss: [0.3218079209327698] Explore P: 0.3603\n",
      "Episode: 677 Total reward: 10.0 Training loss: [0.3443819284439087] Explore P: 0.3600\n",
      "Episode: 678 Total reward: 12.0 Training loss: [0.32524117827415466] Explore P: 0.3595\n",
      "Episode: 679 Total reward: 16.0 Training loss: [0.3224552571773529] Explore P: 0.3590\n",
      "Episode: 680 Total reward: 12.0 Training loss: [0.3444778025150299] Explore P: 0.3586\n",
      "Episode: 681 Total reward: 8.0 Training loss: [0.32110828161239624] Explore P: 0.3583\n",
      "Episode: 682 Total reward: 9.0 Training loss: [0.3217054307460785] Explore P: 0.3580\n",
      "Episode: 683 Total reward: 12.0 Training loss: [0.3276761770248413] Explore P: 0.3575\n",
      "Episode: 684 Total reward: 7.0 Training loss: [0.3236481845378876] Explore P: 0.3573\n",
      "Episode: 685 Total reward: 11.0 Training loss: [0.32524123787879944] Explore P: 0.3569\n",
      "Episode: 686 Total reward: 17.0 Training loss: [0.3399540185928345] Explore P: 0.3563\n",
      "Episode: 687 Total reward: 8.0 Training loss: [0.3219195604324341] Explore P: 0.3561\n",
      "Episode: 688 Total reward: 10.0 Training loss: [0.3215636610984802] Explore P: 0.3557\n",
      "Episode: 689 Total reward: 10.0 Training loss: [0.33555251359939575] Explore P: 0.3554\n",
      "Episode: 690 Total reward: 12.0 Training loss: [0.32613176107406616] Explore P: 0.3550\n",
      "Episode: 691 Total reward: 10.0 Training loss: [0.3257504105567932] Explore P: 0.3546\n",
      "Episode: 692 Total reward: 13.0 Training loss: [0.3335855007171631] Explore P: 0.3542\n",
      "Episode: 693 Total reward: 14.0 Training loss: [0.32552117109298706] Explore P: 0.3537\n",
      "Episode: 694 Total reward: 13.0 Training loss: [0.3222699463367462] Explore P: 0.3532\n",
      "Episode: 695 Total reward: 12.0 Training loss: [0.32409244775772095] Explore P: 0.3528\n",
      "Episode: 696 Total reward: 11.0 Training loss: [0.3225460648536682] Explore P: 0.3524\n",
      "Episode: 697 Total reward: 8.0 Training loss: [0.325161337852478] Explore P: 0.3522\n",
      "Episode: 698 Total reward: 12.0 Training loss: [0.3225741386413574] Explore P: 0.3518\n",
      "Episode: 699 Total reward: 11.0 Training loss: [0.32355260848999023] Explore P: 0.3514\n",
      "Episode: 700 Total reward: 10.0 Training loss: [0.3397609293460846] Explore P: 0.3510\n",
      "Episode: 701 Total reward: 17.0 Training loss: [0.3211604952812195] Explore P: 0.3505\n",
      "Episode: 702 Total reward: 10.0 Training loss: [3.878021743730642e-06] Explore P: 0.3501\n",
      "Episode: 703 Total reward: 11.0 Training loss: [0.32352763414382935] Explore P: 0.3497\n",
      "Episode: 704 Total reward: 8.0 Training loss: [0.32116302847862244] Explore P: 0.3495\n",
      "Episode: 705 Total reward: 11.0 Training loss: [0.32037392258644104] Explore P: 0.3491\n",
      "Episode: 706 Total reward: 11.0 Training loss: [0.32089918851852417] Explore P: 0.3487\n",
      "Episode: 707 Total reward: 9.0 Training loss: [0.325521320104599] Explore P: 0.3484\n",
      "Episode: 708 Total reward: 12.0 Training loss: [0.32006973028182983] Explore P: 0.3480\n",
      "Episode: 709 Total reward: 8.0 Training loss: [0.3207347095012665] Explore P: 0.3477\n",
      "Episode: 710 Total reward: 11.0 Training loss: [0.3206205666065216] Explore P: 0.3474\n",
      "Episode: 711 Total reward: 13.0 Training loss: [0.3217138946056366] Explore P: 0.3469\n",
      "Episode: 712 Total reward: 8.0 Training loss: [0.32499265670776367] Explore P: 0.3467\n",
      "Episode: 713 Total reward: 12.0 Training loss: [0.3221777379512787] Explore P: 0.3463\n",
      "Episode: 714 Total reward: 17.0 Training loss: [3.4518570828367956e-06] Explore P: 0.3457\n",
      "Episode: 715 Total reward: 14.0 Training loss: [9.852807124843821e-05] Explore P: 0.3452\n",
      "Episode: 716 Total reward: 13.0 Training loss: [0.33700868487358093] Explore P: 0.3448\n",
      "Episode: 717 Total reward: 12.0 Training loss: [0.34338656067848206] Explore P: 0.3444\n",
      "Episode: 718 Total reward: 12.0 Training loss: [0.32566413283348083] Explore P: 0.3440\n",
      "Episode: 719 Total reward: 11.0 Training loss: [0.32140296697616577] Explore P: 0.3436\n",
      "Episode: 720 Total reward: 8.0 Training loss: [0.33487457036972046] Explore P: 0.3434\n",
      "Episode: 721 Total reward: 11.0 Training loss: [0.32508572936058044] Explore P: 0.3430\n",
      "Episode: 722 Total reward: 11.0 Training loss: [0.33126235008239746] Explore P: 0.3426\n",
      "Episode: 723 Total reward: 11.0 Training loss: [0.32664087414741516] Explore P: 0.3423\n",
      "Episode: 724 Total reward: 10.0 Training loss: [0.3255479633808136] Explore P: 0.3419\n",
      "Episode: 725 Total reward: 10.0 Training loss: [0.3221246898174286] Explore P: 0.3416\n",
      "Episode: 726 Total reward: 9.0 Training loss: [0.32107213139533997] Explore P: 0.3413\n",
      "Episode: 727 Total reward: 14.0 Training loss: [0.3237631320953369] Explore P: 0.3408\n",
      "Episode: 728 Total reward: 10.0 Training loss: [0.3371264338493347] Explore P: 0.3405\n",
      "Episode: 729 Total reward: 11.0 Training loss: [0.32554155588150024] Explore P: 0.3401\n",
      "Episode: 730 Total reward: 15.0 Training loss: [0.34308311343193054] Explore P: 0.3396\n",
      "Episode: 731 Total reward: 13.0 Training loss: [0.3252585530281067] Explore P: 0.3392\n",
      "Episode: 732 Total reward: 10.0 Training loss: [0.32167986035346985] Explore P: 0.3389\n",
      "Episode: 733 Total reward: 8.0 Training loss: [0.32101190090179443] Explore P: 0.3386\n",
      "Episode: 734 Total reward: 14.0 Training loss: [0.32477936148643494] Explore P: 0.3382\n",
      "Episode: 735 Total reward: 14.0 Training loss: [0.3250027298927307] Explore P: 0.3377\n",
      "Episode: 736 Total reward: 8.0 Training loss: [1.3730372074860497e-06] Explore P: 0.3374\n",
      "Episode: 737 Total reward: 12.0 Training loss: [0.34387823939323425] Explore P: 0.3370\n",
      "Episode: 738 Total reward: 13.0 Training loss: [0.33927851915359497] Explore P: 0.3366\n",
      "Episode: 739 Total reward: 13.0 Training loss: [0.3249288499355316] Explore P: 0.3362\n",
      "Episode: 740 Total reward: 11.0 Training loss: [0.3213789761066437] Explore P: 0.3358\n",
      "Episode: 741 Total reward: 10.0 Training loss: [0.3254319131374359] Explore P: 0.3355\n",
      "Episode: 742 Total reward: 9.0 Training loss: [0.3234822452068329] Explore P: 0.3352\n",
      "Episode: 743 Total reward: 9.0 Training loss: [0.3235398530960083] Explore P: 0.3349\n",
      "Episode: 744 Total reward: 12.0 Training loss: [0.3207009732723236] Explore P: 0.3345\n",
      "Episode: 745 Total reward: 10.0 Training loss: [0.32092297077178955] Explore P: 0.3342\n",
      "Episode: 746 Total reward: 17.0 Training loss: [0.3362714946269989] Explore P: 0.3337\n",
      "Episode: 747 Total reward: 11.0 Training loss: [3.687207936309278e-06] Explore P: 0.3333\n",
      "Episode: 748 Total reward: 11.0 Training loss: [0.3209441602230072] Explore P: 0.3330\n",
      "Episode: 749 Total reward: 16.0 Training loss: [0.3262908458709717] Explore P: 0.3324\n",
      "Episode: 750 Total reward: 14.0 Training loss: [0.326276957988739] Explore P: 0.3320\n",
      "Episode: 751 Total reward: 16.0 Training loss: [0.32205215096473694] Explore P: 0.3315\n",
      "Episode: 752 Total reward: 12.0 Training loss: [0.32094866037368774] Explore P: 0.3311\n",
      "Episode: 753 Total reward: 9.0 Training loss: [2.997282308569993e-06] Explore P: 0.3308\n",
      "Episode: 754 Total reward: 9.0 Training loss: [0.324494332075119] Explore P: 0.3305\n",
      "Episode: 755 Total reward: 11.0 Training loss: [0.3223947286605835] Explore P: 0.3302\n",
      "Episode: 756 Total reward: 13.0 Training loss: [0.32093557715415955] Explore P: 0.3297\n",
      "Episode: 757 Total reward: 9.0 Training loss: [0.3309510350227356] Explore P: 0.3295\n",
      "Episode: 758 Total reward: 8.0 Training loss: [0.3209618330001831] Explore P: 0.3292\n",
      "Episode: 759 Total reward: 12.0 Training loss: [0.3222469389438629] Explore P: 0.3288\n",
      "Episode: 760 Total reward: 14.0 Training loss: [0.32514438033103943] Explore P: 0.3284\n",
      "Episode: 761 Total reward: 14.0 Training loss: [0.3219284117221832] Explore P: 0.3279\n",
      "Episode: 762 Total reward: 9.0 Training loss: [0.335622638463974] Explore P: 0.3276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 763 Total reward: 24.0 Training loss: [7.786057949488168e-07] Explore P: 0.3269\n",
      "Episode: 764 Total reward: 9.0 Training loss: [0.3260384798049927] Explore P: 0.3266\n",
      "Episode: 765 Total reward: 10.0 Training loss: [0.32506000995635986] Explore P: 0.3263\n",
      "Episode: 766 Total reward: 9.0 Training loss: [0.320679247379303] Explore P: 0.3260\n",
      "Episode: 767 Total reward: 11.0 Training loss: [0.32257843017578125] Explore P: 0.3256\n",
      "Episode: 768 Total reward: 19.0 Training loss: [0.32483866810798645] Explore P: 0.3250\n",
      "Episode: 769 Total reward: 11.0 Training loss: [0.32857775688171387] Explore P: 0.3247\n",
      "Episode: 770 Total reward: 16.0 Training loss: [0.3231072425842285] Explore P: 0.3242\n",
      "Episode: 771 Total reward: 9.0 Training loss: [0.32352548837661743] Explore P: 0.3239\n",
      "Episode: 772 Total reward: 22.0 Training loss: [0.3219069838523865] Explore P: 0.3232\n",
      "Episode: 773 Total reward: 13.0 Training loss: [0.32096022367477417] Explore P: 0.3228\n",
      "Episode: 774 Total reward: 12.0 Training loss: [0.325235515832901] Explore P: 0.3224\n",
      "Episode: 775 Total reward: 9.0 Training loss: [0.32386675477027893] Explore P: 0.3222\n",
      "Episode: 776 Total reward: 10.0 Training loss: [0.32282692193984985] Explore P: 0.3218\n",
      "Episode: 777 Total reward: 10.0 Training loss: [0.32459914684295654] Explore P: 0.3215\n",
      "Episode: 778 Total reward: 10.0 Training loss: [0.32115018367767334] Explore P: 0.3212\n",
      "Episode: 779 Total reward: 10.0 Training loss: [0.32557788491249084] Explore P: 0.3209\n",
      "Episode: 780 Total reward: 20.0 Training loss: [0.324503630399704] Explore P: 0.3203\n",
      "Episode: 781 Total reward: 11.0 Training loss: [0.3246673047542572] Explore P: 0.3199\n",
      "Episode: 782 Total reward: 12.0 Training loss: [0.32194995880126953] Explore P: 0.3196\n",
      "Episode: 783 Total reward: 11.0 Training loss: [0.325950026512146] Explore P: 0.3192\n",
      "Episode: 784 Total reward: 14.0 Training loss: [0.3249035179615021] Explore P: 0.3188\n",
      "Episode: 785 Total reward: 9.0 Training loss: [0.3332425057888031] Explore P: 0.3185\n",
      "Episode: 786 Total reward: 12.0 Training loss: [0.3357612192630768] Explore P: 0.3182\n",
      "Episode: 787 Total reward: 13.0 Training loss: [0.32144981622695923] Explore P: 0.3178\n",
      "Episode: 788 Total reward: 9.0 Training loss: [2.8232784643478226e-07] Explore P: 0.3175\n",
      "Episode: 789 Total reward: 10.0 Training loss: [0.3235480785369873] Explore P: 0.3172\n",
      "Episode: 790 Total reward: 13.0 Training loss: [0.32134127616882324] Explore P: 0.3168\n",
      "Episode: 791 Total reward: 12.0 Training loss: [0.3213077187538147] Explore P: 0.3164\n",
      "Episode: 792 Total reward: 10.0 Training loss: [0.33417466282844543] Explore P: 0.3161\n",
      "Episode: 793 Total reward: 14.0 Training loss: [0.3212401270866394] Explore P: 0.3157\n",
      "Episode: 794 Total reward: 15.0 Training loss: [0.3253868520259857] Explore P: 0.3152\n",
      "Episode: 795 Total reward: 11.0 Training loss: [0.3257156014442444] Explore P: 0.3149\n",
      "Episode: 796 Total reward: 10.0 Training loss: [0.32185935974121094] Explore P: 0.3146\n",
      "Episode: 797 Total reward: 10.0 Training loss: [0.32091423869132996] Explore P: 0.3143\n",
      "Episode: 798 Total reward: 11.0 Training loss: [0.33405449986457825] Explore P: 0.3139\n",
      "Episode: 799 Total reward: 10.0 Training loss: [0.3258644640445709] Explore P: 0.3136\n",
      "Episode: 800 Total reward: 14.0 Training loss: [0.3350376486778259] Explore P: 0.3132\n",
      "Episode: 801 Total reward: 11.0 Training loss: [0.3215799927711487] Explore P: 0.3129\n",
      "Episode: 802 Total reward: 10.0 Training loss: [0.34136104583740234] Explore P: 0.3126\n",
      "Episode: 803 Total reward: 7.0 Training loss: [0.3249453604221344] Explore P: 0.3124\n",
      "Episode: 804 Total reward: 10.0 Training loss: [0.3208146393299103] Explore P: 0.3121\n",
      "Episode: 805 Total reward: 7.0 Training loss: [0.3207775950431824] Explore P: 0.3118\n",
      "Episode: 806 Total reward: 12.0 Training loss: [0.32122814655303955] Explore P: 0.3115\n",
      "Episode: 807 Total reward: 12.0 Training loss: [0.3242112696170807] Explore P: 0.3111\n",
      "Episode: 808 Total reward: 20.0 Training loss: [0.3207651674747467] Explore P: 0.3105\n",
      "Episode: 809 Total reward: 15.0 Training loss: [0.32296207547187805] Explore P: 0.3101\n",
      "Episode: 810 Total reward: 9.0 Training loss: [2.3159921056503663e-06] Explore P: 0.3098\n",
      "Episode: 811 Total reward: 10.0 Training loss: [0.33337000012397766] Explore P: 0.3095\n",
      "Episode: 812 Total reward: 8.0 Training loss: [0.3218723237514496] Explore P: 0.3093\n",
      "Episode: 813 Total reward: 8.0 Training loss: [0.3256344795227051] Explore P: 0.3090\n",
      "Episode: 814 Total reward: 10.0 Training loss: [0.32530948519706726] Explore P: 0.3087\n",
      "Episode: 815 Total reward: 9.0 Training loss: [0.3243080973625183] Explore P: 0.3085\n",
      "Episode: 816 Total reward: 7.0 Training loss: [0.32588061690330505] Explore P: 0.3082\n",
      "Episode: 817 Total reward: 12.0 Training loss: [0.32337069511413574] Explore P: 0.3079\n",
      "Episode: 818 Total reward: 10.0 Training loss: [2.3107813831302337e-05] Explore P: 0.3076\n",
      "Episode: 819 Total reward: 12.0 Training loss: [0.32057252526283264] Explore P: 0.3072\n",
      "Episode: 820 Total reward: 10.0 Training loss: [0.3338082730770111] Explore P: 0.3069\n",
      "Episode: 821 Total reward: 13.0 Training loss: [0.32088568806648254] Explore P: 0.3065\n",
      "Episode: 822 Total reward: 11.0 Training loss: [0.32080474495887756] Explore P: 0.3062\n",
      "Episode: 823 Total reward: 17.0 Training loss: [0.33058270812034607] Explore P: 0.3057\n",
      "Episode: 824 Total reward: 27.0 Training loss: [0.3241197466850281] Explore P: 0.3049\n",
      "Episode: 825 Total reward: 13.0 Training loss: [0.3218674957752228] Explore P: 0.3045\n",
      "Episode: 826 Total reward: 13.0 Training loss: [7.62475451665523e-07] Explore P: 0.3042\n",
      "Episode: 827 Total reward: 10.0 Training loss: [9.734103514347225e-07] Explore P: 0.3039\n",
      "Episode: 828 Total reward: 11.0 Training loss: [2.6265342967235483e-05] Explore P: 0.3035\n",
      "Episode: 829 Total reward: 9.0 Training loss: [0.33717602491378784] Explore P: 0.3033\n",
      "Episode: 830 Total reward: 12.0 Training loss: [0.32400059700012207] Explore P: 0.3029\n",
      "Episode: 831 Total reward: 11.0 Training loss: [0.34096813201904297] Explore P: 0.3026\n",
      "Episode: 832 Total reward: 16.0 Training loss: [3.41779298196343e-07] Explore P: 0.3021\n",
      "Episode: 833 Total reward: 11.0 Training loss: [0.3415693938732147] Explore P: 0.3018\n",
      "Episode: 834 Total reward: 19.0 Training loss: [0.3251948058605194] Explore P: 0.3013\n",
      "Episode: 835 Total reward: 22.0 Training loss: [0.3257859945297241] Explore P: 0.3006\n",
      "Episode: 836 Total reward: 12.0 Training loss: [0.3216899037361145] Explore P: 0.3003\n",
      "Episode: 837 Total reward: 7.0 Training loss: [0.32522985339164734] Explore P: 0.3001\n",
      "Episode: 838 Total reward: 13.0 Training loss: [0.32568538188934326] Explore P: 0.2997\n",
      "Episode: 839 Total reward: 13.0 Training loss: [0.3239774703979492] Explore P: 0.2993\n",
      "Episode: 840 Total reward: 13.0 Training loss: [0.3205234110355377] Explore P: 0.2989\n",
      "Episode: 841 Total reward: 13.0 Training loss: [0.32368212938308716] Explore P: 0.2986\n",
      "Episode: 842 Total reward: 13.0 Training loss: [0.34083014726638794] Explore P: 0.2982\n",
      "Episode: 843 Total reward: 9.0 Training loss: [0.3257802128791809] Explore P: 0.2979\n",
      "Episode: 844 Total reward: 11.0 Training loss: [0.3208387494087219] Explore P: 0.2976\n",
      "Episode: 845 Total reward: 9.0 Training loss: [0.33687594532966614] Explore P: 0.2974\n",
      "Episode: 846 Total reward: 9.0 Training loss: [0.34265244007110596] Explore P: 0.2971\n",
      "Episode: 847 Total reward: 18.0 Training loss: [0.3251490890979767] Explore P: 0.2966\n",
      "Episode: 848 Total reward: 10.0 Training loss: [0.3239712417125702] Explore P: 0.2963\n",
      "Episode: 849 Total reward: 12.0 Training loss: [0.32176655530929565] Explore P: 0.2959\n",
      "Episode: 850 Total reward: 12.0 Training loss: [0.32116052508354187] Explore P: 0.2956\n",
      "Episode: 851 Total reward: 9.0 Training loss: [0.33243483304977417] Explore P: 0.2953\n",
      "Episode: 852 Total reward: 10.0 Training loss: [0.3239201009273529] Explore P: 0.2951\n",
      "Episode: 853 Total reward: 10.0 Training loss: [1.6893420706765028e-07] Explore P: 0.2948\n",
      "Episode: 854 Total reward: 9.0 Training loss: [6.83728899275593e-07] Explore P: 0.2945\n",
      "Episode: 855 Total reward: 8.0 Training loss: [0.32071489095687866] Explore P: 0.2943\n",
      "Episode: 856 Total reward: 19.0 Training loss: [0.32508108019828796] Explore P: 0.2938\n",
      "Episode: 857 Total reward: 13.0 Training loss: [0.3218386173248291] Explore P: 0.2934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 858 Total reward: 10.0 Training loss: [0.3227914571762085] Explore P: 0.2931\n",
      "Episode: 859 Total reward: 16.0 Training loss: [0.3408365547657013] Explore P: 0.2926\n",
      "Episode: 860 Total reward: 15.0 Training loss: [0.3250669538974762] Explore P: 0.2922\n",
      "Episode: 861 Total reward: 8.0 Training loss: [0.32116350531578064] Explore P: 0.2920\n",
      "Episode: 862 Total reward: 8.0 Training loss: [0.3216118812561035] Explore P: 0.2918\n",
      "Episode: 863 Total reward: 8.0 Training loss: [0.3225131928920746] Explore P: 0.2915\n",
      "Episode: 864 Total reward: 12.0 Training loss: [0.32264742255210876] Explore P: 0.2912\n",
      "Episode: 865 Total reward: 14.0 Training loss: [0.3217160105705261] Explore P: 0.2908\n",
      "Episode: 866 Total reward: 9.0 Training loss: [0.32475510239601135] Explore P: 0.2906\n",
      "Episode: 867 Total reward: 17.0 Training loss: [0.32390254735946655] Explore P: 0.2901\n",
      "Episode: 868 Total reward: 9.0 Training loss: [0.34008628129959106] Explore P: 0.2898\n",
      "Episode: 869 Total reward: 15.0 Training loss: [0.32083168625831604] Explore P: 0.2894\n",
      "Episode: 870 Total reward: 14.0 Training loss: [0.3216681182384491] Explore P: 0.2890\n",
      "Episode: 871 Total reward: 8.0 Training loss: [1.6887361198314466e-06] Explore P: 0.2888\n",
      "Episode: 872 Total reward: 10.0 Training loss: [0.3252914845943451] Explore P: 0.2885\n",
      "Episode: 873 Total reward: 12.0 Training loss: [0.32429805397987366] Explore P: 0.2882\n",
      "Episode: 874 Total reward: 10.0 Training loss: [0.3215506374835968] Explore P: 0.2879\n",
      "Episode: 875 Total reward: 12.0 Training loss: [0.3216237425804138] Explore P: 0.2876\n",
      "Episode: 876 Total reward: 7.0 Training loss: [0.32498985528945923] Explore P: 0.2874\n",
      "Episode: 877 Total reward: 15.0 Training loss: [0.32099223136901855] Explore P: 0.2870\n",
      "Episode: 878 Total reward: 11.0 Training loss: [2.6011940690295887e-07] Explore P: 0.2867\n",
      "Episode: 879 Total reward: 10.0 Training loss: [0.32105860114097595] Explore P: 0.2864\n",
      "Episode: 880 Total reward: 9.0 Training loss: [0.33991578221321106] Explore P: 0.2861\n",
      "Episode: 881 Total reward: 9.0 Training loss: [0.32248517870903015] Explore P: 0.2859\n",
      "Episode: 882 Total reward: 19.0 Training loss: [0.32063648104667664] Explore P: 0.2854\n",
      "Episode: 883 Total reward: 10.0 Training loss: [0.3317718207836151] Explore P: 0.2851\n",
      "Episode: 884 Total reward: 14.0 Training loss: [0.325196236371994] Explore P: 0.2847\n",
      "Episode: 885 Total reward: 10.0 Training loss: [0.33181697130203247] Explore P: 0.2844\n",
      "Episode: 886 Total reward: 12.0 Training loss: [2.5853114493656904e-05] Explore P: 0.2841\n",
      "Episode: 887 Total reward: 8.0 Training loss: [0.3209620416164398] Explore P: 0.2839\n",
      "Episode: 888 Total reward: 10.0 Training loss: [0.3209179937839508] Explore P: 0.2836\n",
      "Episode: 889 Total reward: 16.0 Training loss: [0.3233751058578491] Explore P: 0.2832\n",
      "Episode: 890 Total reward: 18.0 Training loss: [0.3236781060695648] Explore P: 0.2827\n",
      "Episode: 891 Total reward: 11.0 Training loss: [0.3211376667022705] Explore P: 0.2824\n",
      "Episode: 892 Total reward: 10.0 Training loss: [0.32462239265441895] Explore P: 0.2821\n",
      "Episode: 893 Total reward: 14.0 Training loss: [0.32530689239501953] Explore P: 0.2817\n",
      "Episode: 894 Total reward: 11.0 Training loss: [0.321645587682724] Explore P: 0.2814\n",
      "Episode: 895 Total reward: 9.0 Training loss: [0.32093706727027893] Explore P: 0.2812\n",
      "Episode: 896 Total reward: 11.0 Training loss: [0.3210326135158539] Explore P: 0.2809\n",
      "Episode: 897 Total reward: 16.0 Training loss: [0.32130736112594604] Explore P: 0.2805\n",
      "Episode: 898 Total reward: 9.0 Training loss: [0.31951767206192017] Explore P: 0.2802\n",
      "Episode: 899 Total reward: 12.0 Training loss: [1.1539853517206211e-07] Explore P: 0.2799\n",
      "Episode: 900 Total reward: 10.0 Training loss: [0.32100123167037964] Explore P: 0.2796\n",
      "Episode: 901 Total reward: 17.0 Training loss: [0.32494738698005676] Explore P: 0.2792\n",
      "Episode: 902 Total reward: 9.0 Training loss: [0.33364367485046387] Explore P: 0.2789\n",
      "Episode: 903 Total reward: 7.0 Training loss: [0.3401762545108795] Explore P: 0.2787\n",
      "Episode: 904 Total reward: 11.0 Training loss: [0.3204645812511444] Explore P: 0.2784\n",
      "Episode: 905 Total reward: 10.0 Training loss: [0.3215162456035614] Explore P: 0.2782\n",
      "Episode: 906 Total reward: 10.0 Training loss: [0.3225398063659668] Explore P: 0.2779\n",
      "Episode: 907 Total reward: 9.0 Training loss: [0.3224896192550659] Explore P: 0.2777\n",
      "Episode: 908 Total reward: 10.0 Training loss: [0.32098376750946045] Explore P: 0.2774\n",
      "Episode: 909 Total reward: 13.0 Training loss: [0.3215080499649048] Explore P: 0.2770\n",
      "Episode: 910 Total reward: 14.0 Training loss: [0.3404320776462555] Explore P: 0.2767\n",
      "Episode: 911 Total reward: 8.0 Training loss: [0.33532261848449707] Explore P: 0.2765\n",
      "Episode: 912 Total reward: 7.0 Training loss: [0.32239603996276855] Explore P: 0.2763\n",
      "Episode: 913 Total reward: 10.0 Training loss: [0.3391015827655792] Explore P: 0.2760\n",
      "Episode: 914 Total reward: 18.0 Training loss: [0.3214744031429291] Explore P: 0.2755\n",
      "Episode: 915 Total reward: 10.0 Training loss: [3.117675078101456e-07] Explore P: 0.2753\n",
      "Episode: 916 Total reward: 14.0 Training loss: [0.320291131734848] Explore P: 0.2749\n",
      "Episode: 917 Total reward: 9.0 Training loss: [0.3222452998161316] Explore P: 0.2746\n",
      "Episode: 918 Total reward: 13.0 Training loss: [0.3396672010421753] Explore P: 0.2743\n",
      "Episode: 919 Total reward: 15.0 Training loss: [0.3226481080055237] Explore P: 0.2739\n",
      "Episode: 920 Total reward: 14.0 Training loss: [0.32059013843536377] Explore P: 0.2735\n",
      "Episode: 921 Total reward: 9.0 Training loss: [0.3209783732891083] Explore P: 0.2733\n",
      "Episode: 922 Total reward: 7.0 Training loss: [0.32485735416412354] Explore P: 0.2731\n",
      "Episode: 923 Total reward: 15.0 Training loss: [0.32467034459114075] Explore P: 0.2727\n",
      "Episode: 924 Total reward: 11.0 Training loss: [0.3215120732784271] Explore P: 0.2724\n",
      "Episode: 925 Total reward: 10.0 Training loss: [0.3205466568470001] Explore P: 0.2722\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-282e5893a47a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     83\u001b[0m             \u001b[0mt\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m         \u001b[0mhist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magv3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\OneDrive\\projects\\udacity.mlnd\\mlnd\\projects\\RL-Quadcopter-2\\agents\\agent.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m    215\u001b[0m             \u001b[0mtarget_f\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m             \u001b[0mtarget_f\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m             \u001b[0mhist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    218\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhist\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    961\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    962\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 963\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m    964\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    965\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1705\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1706\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1235\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1236\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2478\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2479\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 905\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    906\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1135\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1137\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1138\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1353\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1355\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1356\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1357\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1359\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1360\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1361\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1362\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1363\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1338\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[1;32m-> 1340\u001b[1;33m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[0;32m   1341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1342\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from agents import agent\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "agv3 = agent.Agentv3 (4, 2)\n",
    "\n",
    "train_episodes = 1000\n",
    "max_steps = 200\n",
    "batch_size = 20\n",
    "\n",
    "# Make a bunch of random actions and store the experiences\n",
    "# Start new episode\n",
    "env.reset()\n",
    "# Take one random step to get the pole and cart moving\n",
    "state, reward, done, _ = env.step(env.action_space.sample())\n",
    "for ii in range(20):\n",
    "\n",
    "    # Make a random action\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "    if done:\n",
    "        # The simulation fails so no next state\n",
    "        next_state = np.zeros(state.shape)\n",
    "        # Add experience to memory\n",
    "        agv3.store ((state, action, reward, next_state))\n",
    "\n",
    "        # Start new episode\n",
    "        env.reset()\n",
    "        # Take one random step to get the pole and cart moving\n",
    "        state, reward, done, _ = env.step(env.action_space.sample())\n",
    "    else:\n",
    "        # Add experience to memory\n",
    "        agv3.store ((state, action, reward, next_state))\n",
    "        state = next_state\n",
    "\n",
    "# GPI with DQN\n",
    "env.reset ()\n",
    "state, reward, done, _ = env.step (env.action_space.sample())\n",
    "\n",
    "step = 0\n",
    "rewards_list = []\n",
    "for ep in range (1, train_episodes):\n",
    "    total_reward = 0\n",
    "    t = 0\n",
    "    while t < max_steps:\n",
    "        step += 1\n",
    "        # Uncomment this next line to watch the training\n",
    "        # env.render ()\n",
    "\n",
    "        # Explore or Exploit\n",
    "        action = agv3.act (np.reshape(state, [1, 4]), step)\n",
    "\n",
    "        # Take action, get new state and reward\n",
    "        next_state, reward, done, _ = env.step (action)\n",
    "\n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            # the episode ends so no next state\n",
    "            next_state = np.zeros (state.shape)\n",
    "            t = max_steps\n",
    "            \n",
    "            print('Episode: {}'.format(ep),\n",
    "                  'Total reward: {}'.format(total_reward),\n",
    "                  'Training loss: {}'.format(hist[0].history['loss']),\n",
    "                  'Explore P: {:.4f}'.format(agv3.explore_p))\n",
    "\n",
    "            rewards_list.append((ep, total_reward))\n",
    "\n",
    "            # Add experience to memory\n",
    "            agv3.store ((state, action, reward, next_state))\n",
    "\n",
    "            # Start new episode\n",
    "            env.reset ()\n",
    "            # Take one random step to get the pole and cart moving\n",
    "            state, reward, done, _ = env.step(env.action_space.sample())\n",
    "\n",
    "        else:\n",
    "            # Add experience to memory\n",
    "            agv3.store ((state, action, reward, next_state))\n",
    "            state = next_state\n",
    "            t += 1\n",
    "\n",
    "        hist = agv3.learn (batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
